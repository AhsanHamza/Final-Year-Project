P	Disassembly lines are widely used in the remanufacturing industry, as they are the most suitable layout for the disassembly of large products as well as small products received in large quantities (Güngör and Gupta, 2002).
N	For example, the disassembly can be partial or complete, depending on whether all components are separated; some tasks might fail because of poor conditions or missing components in the end-of-life product; and, most of all, the disassembly process can be operated in many ways because it is not necessary to ensure the functionality of the product, i.e., multiple processing alternatives exist (Güngör and Gupta, 2002).
O	There are multiple ways to represent the processing alternatives for disassembly, such as Bourjault’s tree, state diagram, AND/OR graph (AOG) and task precedence diagram (TPD) (Lambert 2006).
P	This problem was first introduced and formulated by Koc et al. (2009) and is the simplest version of the DLBP using an AOG representation on which many succeeding studies were based (Paksoy et al. 2013; Aydemir-Karadag and Turkbey 2013; Hezer and Kara 2015).
O	The ALBP has been intensively studied in the literature, whereas studies on the DLBP are relatively sparse. A recent literature review of the line balancing problem was provided by Battaïa and Dolgui (2013). Güngör and Gupta (1999) were among the first to consider the DLBP.
O	The first type uses various modified versions of the TPD, e.g., disassembly precedence matrix (Güngör and Gupta 2001), TPD with AND/OR relations (Kalaycılar et al. 2016), TPD with sequence-dependent cost (Kalayci et al. 2016), etc., to represent the precedence relations among tasks.
O	Therefore, the algorithms for one type cannot be directly applied to the other..Here, we focus on the second type, where this work belongs. Koc et al. (2009) first formulated the DLBP using an AOG representation.
O	Paksoy et al. (2013) studied a mixed-model DLBP with three fuzzy goals.
O	Hezer and Kara (2015) proposed a network-based shortest route model for the DLBP with multiple parallel lines.
O	Özceylan et al. (2014) formulated a model for simultaneously optimising a reverse supply network design problem and the DLBP. Moreover, some models and algorithms have been proposed to address the DLBP with stochastic task times.
O	Aydemir-Karadag and Turkbey (2013) studied a stochastic DLBP with parallel stations that requires the cycle time constraint at each station to be obeyed with a given probability.
O	Bentaha et al. (2014a, 2014b, 2015a, 2015b) also investigated a stochastic DLBP, but with alternative assumptions. 
O	Bentaha et al. (2014a, 2014b) assumed that the overloading beyond the cycle time at each station can be compensated at a fixed cost rate, whereas Bentaha et al. (2015a, 2015b) used joint probabilistic constraints.
O	The only research that addressed the SDLBP-1 after Koc et al. (2009) was by Mete et al. (2016).
N	Except for the benchmark instance generation scheme proposed by Koc et al. (2009), only a handful of general AOGs can be found in the literature, and they are all small sized in the sense that less than 10 tasks are required to completely disassemble a product.
O	Our choice of the BB&R algorithm was motivated by the fact that the BB&R algorithm proposed by Sewell and Jacobson (2012) is a state-of-the-art algorithm for the SALBP-1.
P	Section 3 analyses the properties of the SDLBP-1 and gives some preliminary results, including an improved version of the DP approach of Koc et al. (2009) and a proof that shows that a special case of SDLBP-1 is polynomially solvable.
O	Since Koc et al. (2009) already gave a detailed description of the SDLBP-1, only a brief introduction is presented here.
O	An AOG is a directed graph composed of two types of nodes, called artificial nodes and normal nodes. Figure 1 is an illustration from Bentaha et al. (2014a).
P	If the processing time is subassembly-independent, then the AOG can be reduced to one or more TPDs and the SDLBP-1 can be solved by algorithms designed for the SALBP-1 (Koc et al. 2009).
P	For details about a formal IP model for the SDLBP-1, please refer to Koc et al. (2009).
N	Moreover, lower bounds can be derived from equation (1), as shown in Section 4, but they cannot be derived from the recursion in Koc et al. (2009).
P	Although Bentaha et al. (2015b) proved that the DLBP using an AOG representation is NP-hard in general, some special cases of SDLBP-1 might be polynomially solvable.
O	Koc et al. (2009) proposed a scheme for randomly generating benchmark instances with no parallel tasks, i.e., every task in the AOG generates at most one subassembly, |S(Bj)|≤1 for every ask j=1,…,N.
P	Many studies (Koc et al. 2009; Aydemir-Karadag and Turkbey 2013; Hezer and Kara 2015; Mete et al. 2016) generated instances with this scheme to test the performance of their respective algorithms.
P	According to a recent survey by Pereira (2015) on the lower bounds for the SALBP-1, relaxing the balancing problem to a bin-packing problem or to a single-machine scheduling problem are the two most popular ways to get lower bounds.
P	Furthermore, the obtained bounds can be strengthened by using the concept of a dual feasible function (DFF) (Fekete and Schepers, 2001).
P	This procedure is a modified version of the recursive procedure first proposed by Fleszar and Hindi (2003).
O	Similar to the BB&R algorithm for the SALBP-1 proposed by Sewell and Jacobson (2012), the BB&R algorithm uses station-oriented branching and a CBFS search strategy.
P	For details about the BB&R algorithm and the CBFS search strategy, please refer to Sewell and Jacobson (2012).
P	The performance of proposed algorithms, including Procedure 1, the improved DP approach and the BB&R algorithm, are evaluated on two sets of instances: the first set is proposed by Mete et al. (2016) (referred to as Mete’s set), composed of instances with no parallel tasks, and the second set is composed of instances with parallel tasks.
O	The AOG of the instances in Mete’s set are generated by the scheme of Koc et al. (2009), which can be defined by a 3-tuple of (a, t’, K), where a∈{3, 4, 5, 10}, t’∈{2, 3, 5, 10}, and K∈{20, 50, 80} respectively represent the number of subassemblies at each level, the number of immediately successors after each subassembly, and the number of components in the product.
P	The largest instance has 80 components, 7720 tasks and 781 subassemblies, while the smallest instance has 20 components, 108 tasks and 55 subassemblies. For more details about the instances please refer to Mete et al. (2016).
P	Koc et.al (2009), the time limit for the CPLEX solver was set to 600 seconds.
O	Since BS heuristic runs 10 times for each instance as described in Mete et al. (2016), it is evaluated based on the minimal objective value among the 10 runs.
P	The rewritten BS program is validated by comparing its results with those reported in Mete et al. (2016).
O	It is assumed that each task takes apart the product or subassembly into exactly two new subassemblies or into one component and one new subassembly (Koc et al. 2009).
N	We improved the DP approach of Koc et al. (2009) and prove that the SDLBP-1 with no parallel tasks is polynomially solvable.
O	Each vertex of the graph then represents an atom, while an edge represents a molecular bond (Raymond and Willett, 2002).
P	Conte et al. (2004) presented a list of matching problems that basically fall into two main categories: exact isomorphism and error tolerant GM problems.
O	Solving this problem implies minimizing a dissimilarity measure that stands for the cost needed to transform one graph into another through a series of edit operations (Bunke and Allermann, 1983).
P	In the past years, the GED problem has gained more attention, mainly because it has been shown to generalize other GM problems such as maximum common subgraph, graph and subgraph isomorphism (Bunke, 1997; 1999).
O	Zeng et al. (2009) have shown that this particular case of the GED problem with unitary costs for deletion and insertion edit operations is NP-hard.
P	Starting with the exact ones, two Mixed Integer Linear Program (MILP) formulations are proposed by Lerouge et al. (2015) to solve the GED problem.
N	A very efficient MILP formulation (MILPJH) is introduced by Justice and Hero (2006), but it is restricted to the GEDEnA problem.
O	A quadratic formulation for the GED problem, referred to as Quadratic Assignment Problem (QAP), has been proposed by Bougleux et al. (2015).
P	Moving to the heuristic methods, an approach called Bipartite Graph Matching (BP) to solve the GED problem is introduced by Riesen et al. (2007).
P	Later on, many improvements have been proposed as in Riesen and Bunke (2009); Serratosa (2014) and Serratosa (2015), where mainly the cost matrix used is modified and more information about the neighborhood of vertices are considered.
N	Neuhaus et al. (2006) introduced another heuristic that relies on a beam-search method to solve the GED problem.
P	An interesting algorithm is presented by Ferrer et al. (2015) which is basically a combination of BP and beam-search approaches.
O	Bougleux et al. (2017) recently have proposed two algorithms called Integer Projected Fixed Point (IPFP) and Graduate Non Convexity and Concavity Procedure (GNCCP).
O	LocBra was originally introduced by Fischetti and Lodi (2003), as a general metaheuristic based on MILP formulations.
O	MILPJH is a mathematical formulation proposed in Justice and Hero (2006) that deals with the GEDEnA problem.
O	The details about the construction of the model can be found in Justice and Hero (2006) and only a short description is provided here.
N	This formulation was shown to be the most efficient for the GEDEnA problem among the other formulations found in the literature (Robles-Kelly et al., 2016).
O	BeamSearch is a heuristic for the GED problem presented by Neuhaus et al. (2006).
O	SBPBeam is a heuristic for the GED problem introduced by Ferrer et al. (2015).
P	The first one has been originally presented in Riesen et al. (2007) and consists in building a special cost matrix for vertices assignment.
O	IPFP is a heuristic that solves the GED problem and has been proposed by Bougleux et al. (2017).
N	It is based on a heuristic proposed in Leordeanu et al. (2009) to find a solution to the quadratic assignment problem (QAP).
O	GNCCP is introduced by Bougleux et al. (2017). It is also a heuristic for the GED problem that works similarly to IPFP by approximating the QAP problem.
O	More details about IPFP and GNCCP heuristics can be found in Bougleux et al. (2017).
N	This heuristic version follows the original version introduced by Fischetti and Lodi (2003), with improvements outlined when appropriate.
O	This assignment problem, of size max(|Ei|, |Ej|) × max(|Ei|, |Ej|), is solved by the Hungarian algorithm (Munkres, 1957) which requires O(max(|Ei|, |Ej|)3) time.
N	Preliminary experiments, not reported here, have shown that such a diversification significantly improves the local branching heuristic, better than the original one introduced by Fischetti and Lodi (2003), which was quite inefficient for escaping local optima.
P	To conduct the experiments, instances from MUTA (Abu-Aisheh et al., 2015) and PAH (Brun, 2016) databases are selected.
P	These two databases are chosen after reviewing the public datasets (Abu-Aisheh et al., 2015; Brun, 2016), which are usually used to evaluate the GED methods.
O	BeamSearch-α (Neuhaus et al., 2006), with α the beam size.
O	SBPBeam-α (Ferrer et al., 2015), with α the beam size.
O	IPFP-it (Bougleux et al., 2017), with it the maximum number of iterations.
O	GNCCP-d (Bougleux et al., 2017), with d the quantity to be deducted from the ζ variable at each iteration.
P	For BeamSearchα and SBPBeam-α, the heuristics values α are taken from the paper that originally presented the methods (Ferrer et al., 2015; Neuhaus et al., 2006).
P	The same holds for IPFP-it and GNCCPd heuristics, where the parameters values are extracted from Bougleux et al. (2017).
O	This work presents a local branching heuristic for the GEDEnA problem based on the MILPJH formulation of Justice and Hero (Justice and Hero, 2006).
O	Next, the heuristic is evaluated on two databases called MUTA (Abu-Aisheh et al., 2015) and PAH (Brun, 2016).
O	The underlying optimization problem is known as the container relocation problem (CRP) (Forster and Bortfeldt, 2012) or the block(s) relocation problem (BRP) (Caserta et al., 2011) in the literature.
P	It is often referred to as the restricted container (or block) relocation problem (R-BRP) (see, e.g., Galle et al., 2018).
N	More recently, Exp´osito-Izquierdo et al. (2015) have proposed a mathematical model for the R-BRP and a branch-and-bound algorithm which can be truncated and thus used as a heuristic.
P	Eskandari and Azari (2015) propose corrections and improvements to one of the formulations proposed in Caserta et al. (2012) and present improved results for the R-BRP and Ku and Arthanari (2016b) develop an exact method which relies on clever ways to reduce the search space as well as on a bi-directional search scheme.
O	Very recently, Galle et al. (2018) have propose a new mathematical formulation relying on a binary encoding of different configurations.
P	For the unrestricted BRP studied in this paper, Jin et al. (2015) propose a look-ahead heuristic, in which a tree search algorithm is called so as to anticipate the impact of the next relocation.
N	They are able to improve on the results of Forster and Bortfeldt (2012) and Zhuet al. (2012).
P	Tricoire et al. (2018) design a branch-and-bound algorithm and several heuristics for the U-BRP, relying on the notion of safe moves and decreasing sequences, and incorporate them into metaheuristic search schemes which they call rake search and pilot method.
O	Also Azari et al. (2017) rely on a branch-and-bound based heuristic algorithm. In contrast to other studies, they aim at minimizing the total working time of the crane.
O	New lower bounds for the U-BRP have been proposed and used in an A* type algorithm by Quispe et al. (2018).
P	New unifying problem formulations of the premarshalling problem, the R-BRP and the U-BRP have recently been developed by de Melo da Silva et al. (2018).
O	The pre-marshalling problem is also subject to investigation in Jovanovic et al. (2017).
P	In its basic version, first introduced by Kim and Hong (2006), unproductive movements may only concern those containers which are on top of the container which needs to be retrieved next.
P	For Literature review and a classification scheme considering all kinds of problems arising in situations where containers are organized in stacks, we refer to Lehnfeld and Knust (2014).
P	Among the more recent works on the restricted BRP is the one of Jovanovic and Voß (2014).
P	The branchand-bound algorithm relies on a new lower bound, improving on the one of Forster and ortfeldt (2012), and it employs the loop idea of Tanaka and Mizuno (2015), in which the lower bound is iteratively increased by one.
P	Very recently, Tanaka and Mizuno (2018) have designed a branch-and-bound algorithm for the unrestricted BRP.
P	Prandtstetter (2013) developed a dynamic programming based branch-and-bound algorithm for the pre-marshalling problem. 
P	Very recently Tanaka and Tierney (2018) have proposed a new branch-and-bound algorithm for the pre-marshalling problem solving a number of previously unsolved instances to optimality. 
P	Parre˜no Torres and Alvarez-Valdes (2019) develop new integer programming models as well as an iterative solution method which, in each iteration, increases the lower bound on the maximum number of moves needed to rearrange the bays by one until a feasible and thus optimal solution has been found.
O	Generalizations of the BRP involve, e.g., the work of Hakan Aky¨uz and Lee (2014), who introduce the dynamic container relocation problem in which containers are received and retrieved from a single yard-bay.
P	Ku and Arthanari (2016a) propose to incorporate departure time windows and they develop a stochastic dynamic programming model, since the retrieval sequence of containers belonging to the same departure time window is unknown.
O	Two of which have been proposed by other authors (Caserta et al. (2011) and Tricoire et al. (2018)) while the third set is new.
O	The small instances of Caserta et al. (2011) and the large instances proposed by Tricoire et al. (2018) follow the same scheme.
O	Let H donate the maximum height of a stack in an initial configuration, the size of the Caserta et al. instances ranges from (H, W) = (3, 3) until (H, W) = (10, 10), the size of the Tricoire et al. instances ranges from (H, W) = (10, 10) until (H, W) = (100, 100), where we recall that W is the number of stacks.
O	For the small Caserta instances, we use the rake search and pilot method of Tricoire et al. (2018) as well as two versions of the greedy look ahead heuristic (GLAH) of Jin et al. (2015).
O	JZW: evaluation subroutine of the greedy look ahead heuristic of Jin et al. (2015).
O	LA-S-1: extended look-ahead heuristic of Petering and Hussein (2013), where “S-1” defines the look ahead in terms of containers and here it is equal to W − 1.
O	SM-2: heuristic incorporating best safe 1-relocate moves and best safe 2-relocate moves (Tricoire et al., 2018).
O	SmSEQ-2: combination of decreasing sequence relocate (sequence length ≥ 1) and SM-2 (Tricoire et al., 2018).
P	For all construction methods, we use the implementation from Tricoire et al. (2018).
N	Many of the small instances from Caserta et al. (2011) are solved optimally with constructive methods and the necessity of developing complex local search operators is not obvious for these instances.
P	More interesting results are obtained with larger instances like the ones introduced in Tricoire et al. (2018).
N	For the instances of Tricoire et al. (2018) an obvious trend can be observed: the larger the instance, the larger the improvement by our local search algorithm.
N	Average percentage improvement compared to starting solution for benchmark instances of Tricoire et al. (2018) with Hmax = H + 2 and H ∈ {10, 20, 30, 40, 50, 60, 70, 80, 90, 100}.
P	Worst percentage improvement compared to best constructive solution across all methods for benchmark instances of Tricoire et al. (2018).
N	It illustrates that, in the case of the benchmark instances of Tricoire et al. (2018) except for the smallest instances, the worst solution obtained by LS is usually better than the best solution identified with the best performing constructive method.
O	Information on the required CPU time in seconds for the instances of Tricoire et al. (2018) can be found in Tables 2 and 3.
P	Personnel scheduling has been studied extensively over last five decades, with the most common applications associated with airline and airport operations (Kohl and Karisch, 2004), healthcare (Bard et al., 2017; Liang and Turkan, 2016), call centers (Mehrotra and Grossman, 2009), and postal services (Bard et al., 2007; Brunner and Bard, 2013).
O	The range of problems investigated includes workforce sizing and composition in the long term, days off and shift scheduling in the mid-term, task assignments, downgrading and the use of overtime in the short term, and reactive planning to disruptions in real time (e.g., Clausen et al., 2010; see Ernst et al., 2004; Hu et al., 2016).
N	For airport ground handlers at the Munich airport, taking advantage of this option was shown by Hur et al. (2018) to reduce the total number of uncovered periods in a day by up to 94%.
P	Recently, several papers have focused on the impact of break flexibility and different break regulations on shift scheduling (Rekik et al., 2010) and tour scheduling (Kiermaier et al., 2017) given a deterministic workforce demand.
O	Thompson and Pullman (2007) analyzed the impact of scheduling breaks in advance (e.g., prior to the day of operation versus scheduling breaks in real time).
P	Hur et al. (2018) extended their approach to account for randomized demand by developing a rolling horizon procedure that allowed the scheduler to adapt to demand disruptions throughout the day.
O	Nominally starting with the set-covering formulation of the shift scheduling problem by Dantzig (1954), there has been anearly endless stream of research on personnel scheduling.
P	A good summary of problem types and models is given by Ernst et al. (2004).
O	Bechtold and Jacobs (1990) were the first to develop an implicit model for including a break in a shift.
O	For related work, see Bard et al. (2003), Topaloglu and Ozkarahan (2003). 
P	Rekik et al. (2010) proposed a new model that extended the ideas of Bechtold and Jacobs to include several sub-breaks in a shift, rather than a single break. 
O	Based on the formulation of Rekik et al. (2010), Kiermaier et al. (2017) investigated the weekly tour scheduling problem for airport ground handlers in which various break types are considered.
O	Defraeye and Nieuwenhuyse (2016) proposed a branch and bound approach to estimate optimal shift schedules with nonstationary stochastic demand for general personnel scheduling problems.
O	Parisio and Jones (2015) developed a two-stage stochastic optimization model for retail outlets.
O	Maass et al. (2017) proposed a stochastic programming formulation for nurse staffing in the face of uncertain patient demand and nurse absenteeism.
O	Pacqueau and Soumis (2014) modeled a shift scheduling problem with uncertain demand using two-stage stochastic integer programming.
N	Based on the application addressed by Kiermaier et al. (2017), Hur et al. (2018) solved the one-day break assignment problem with five different break types for both deterministic and randomized demand.
P	Instead of using a rolling horizon algorithm, we develop a multi-stage stochastic optimization model and introduce daily scenarios in the same vein as Maass et al. (2017).
P	Rifai et al. (2015) solved a stochastic optimization model for shift scheduling in hospital emergency departments with the objective of minimizing the total expected time that patients wait for service.
O	Kiermaier et al. (2016) studied a flexible cyclic rostering problem to schedule a workforce with uncertain demand.
P	Escudero et al. (2013) used Lagrangian duality to get a lower bound on a two-stage stochastic programming problem.
O	Bard and Purnomo (2007) studied the cyclic nurse scheduling problem and used Lagrangian relaxation to decompose it into independent subproblems – one for each nurse type.
N	Then, to measure the value of stochastic solutions, we calculate the expected value of perfect information (EVPI) and the value of the stochastic solution (VSS) by using the wait-and-see (WS) model and expected value (EV) model (e.g., see Bard et al., 2007; Birge and Louveaux, 1997; Marufuzzaman et al., 2014).
O	Shifts are characterized by variable length, multiple starting times, and flexible break assignments, i.e., the shift scheduling problem incorporates the break assignment subproblem (BAP), which we define with the help of the 3-field classification scheme developed by Kiermaier et al. (2017): [S, M, F | X, V | T, W].
O	Using the definitions provided by Rekik et al. (2010), a shift is a combination of start time and length, a break profile is a number of sub-breaks and their lengths, and a break is a combination of a break profile, start time, and length.
N	In our implicit modeling approach to break scheduling (see Bechtold and Jacobs, 1990; Rekik et al., 2008,2010), the numbers and types of shifts and breaks are determined simultaneously, but the actual break assignments for each shift are determined in a post-processing step.
O	Constraints (1d)–(1f) are the extended forward, backward and equality constraints used to implicitly model breaks (Rekik et al., 2010).
P	They ensure that each shift receives the appropriate number of sub-breaks for the scenario being considered (for more detail, see Hur et al., 2018).
O	We therefore seek a less complex formulation and convert (1a)–(1j) to a two-stage stochastic optimization model (e.g., see Kiermaier et al., 2016 and Maass et al., 2017).
N	Experience has shown that it is best to avoid the use of non-signed Lagrangian multipliers in the dualized model, so we propose to replace (3a) with (3b) and (3c) (e.g.,see Escudero et al., 2013).
P	To find solutions we implemented a standard subgradient procedure (Wolsey, 1998). T
P	In this section, we present a number of ways (e.g., see Birge and Louveaux, 1997; Bard et al., 2007) of measuring the quality of stochastic programming solutions.
P	For the interested reader, Table 1 in Hur et al. (2018) shows all possible break profiles for each break type.
O	More details can be found in Hur et al. (2018).
P	Hur et al. (2018) developed a rolling horizon (RH) approach to the shift and break scheduling problem with random demand.
O	The details can be found in Section 5 in Hur et al. (2018).
O	As mentioned by Hur et al., 2018, the RH results for the flexible shift model are almost identical to those for the fixed shift model because all shift lengths are fixed in advance in both cases.
N	In fact, the last mile is often considered to be one of the most costly and least efficient stages of the whole supply chain (Gevaers et al., 2009).
N	While perishable products can be found in many areas, the food sector presents an important example of an environment in which quality and safety aspects play an important role (Akkerman et al., 2010), and where high perishability leads to considerable losses and wastage (Yu and Nagurney, 2013).
P	Problems concerned with the optimal routing of vehicles, to improve delivery operations, have been extensively studied for decades (Cordeau et al., 2007; Laporte, 2009).
O	Over the course of time, several variants of the basic vehicle routing problem (VRP) have incorporated other aspects and more specific requirements related to decision making in the supply chain context (Schmid et al., 2013).
P	Thus, (Tarantilis and Kiranoudis, 2001) developed a metaheuristic for the vehicle routing related to the distribution of fresh milk with a fleet of heterogenous vehicles.
N	In the context of fresh vegetable distribution, (Osvald and Stirn, 2008) included perishability into the vehicle routing problem with time windows and time-dependent travel times.
P	Amorim and Almada-Lobo (2014) developed a multi-objective model for the vehicle routing problem with time windows to investigate different distribution scenarios and the trade-off between cost and product quality.
P	Rabbani et al. (2015) proposed a multi-objective VRP with time windows and customer selection, assuming a heterogenous fleet of vehicles and considering multiple deteriorating products.
P	Wang et al. (2016) solved a multi-objective VRP with time windows and perishability considerations using a two-phase heuristic method based on a variable neighbourhood search and a genetic algorithm.
O	Rabbani et al. (2016) considered the use of multiple middle depots and incorporated several aspects, such as product freshness and profit maximisation into the objective function.
P	Considering perishability in a site-dependent vehicle routing problem with time windows and a heterogeneous fleet of vehicles, (Amorim et al., 2014) developed a neighbourhood search algorithm and applied it to a real-life case study arising in a Portuguese food distribution company.
N	Hsu et al. (2007) extended the vehicle routing problem with time windows by adding a stochastic cost component related to the perishability of products.
P	Song and Ko (2016) proposed a non-linear model with the objective of maximising customer satisfaction related to the delivery of multi-commodity perishable products with refrigerated and non-refrigerated vehicles.
P	While the focus in the following will be on the latter aspect, examples related to other features can be found in Farahani et al. (2012); Govindan et al. (2014) and the review of Amorim et al. (2013).
O	Nahmias (2011) and Karaesmen et al. (2011) provide reviews related to the management and modelling of perishable inventory systems.
P	For a more general and extensive overview of the field of inventory-routing problems (IRP), its variants and associated solution approaches we refer to the reviews of Andersson et al. (2010); Bertazzi et al. (2008) and Coelho et al. (2013).
P	In the context of perishable products, (Hiassat and Diabat, 2011) proposed an integrated model for a location-inventoryrouting problem considering products with a limited life-span. Le et al. (2013) developed an algorithm for an IRP based on column generation and cutting planes, the problem is extended in Hiassat et al. (2017), integrating location decisions into the model, and solved using a genetic algorithm.
P	Coelho and Laporte (2014) applied branch-and-cut to optimally solve the perishable inventory-routing problem (PIRP) under general assumptions and consideration of two different selling policies.
P	Jia et al. (2014) solved an IRP for perishable products with multiple time windows and loading costs, solving the problem using a twophase solution algorithm.
P	Mirzaei and Seifi (2015) considered the impact of lost sales in their inventory-routing problem.
P	Kande et al. (2015) proposed a tabu search metaheuristic for a routing problem with inventory and lot-sizing decisions as well as multiple source nodes.
O	Dealing with uncertain demand in a multi-period IRP model, (Soysal et al., 2015) further included environmental aspects in the form of greenhouse gas emissions and fuel consumption.
P	Rahimi et al. (2017) developed a multi-objective model for the IRP of perishable products, allowing for a choice of different vehicles.
P	Diabat et al. (2016) proposed a new arc-based formulation and a tabu search algorithm for the inventory-routing problem for perishable products.
P	Azadeh et al. (2016) considered an inventory-routing problem with transshipments for a perishable product and applied a genetic algorithm to solve the problem.
P	Li et al. (2016) developed a mixed integer linear programming model for perishable supply chains, incorporating production decisions in the inventory-routing problem and maximising profit.
N	In addition, (Zhao et al., 2008) proposed a similar structured twoechelon inventory routing problem without perishability considerations.
P	iven this focus, it is reasonable to assume a multi-level system, resulting in a two-echelon routing problem (Hemmelmayr et al., 2012).
O	A survey of two-echelon routing problems can be found in Cuda et al. (2015).
P	These delivery day combinations are represented for each location as a list of combinations of daily time windows, during which deliveries can be made, as it is the case in the periodic vehicle routing problem (PVRP) presented by Cordeau et al. (1997).
O	The first type is associated with an expiry date, meaning that the products are suitable for consumption up until a certain point in time, after which they are discarded (Nahmias, 2011).
N	The second type relates to a gradual decrease in product quality and can be for example observed for salads, fruits and bread (Rong et al., 2011).
P	We therefore propose a two-stage matheuristic, i.e. a “heuristic algorithm[ ] made by the interoperation of metaheuristics and mathematical programming techniques” (Boschetti et al., 2009), combining an adaptive large neighbourhood search (ALNS) with the solution of a MILP formulation, in order to solve the problem for more realistic instances.
O	This operator is based on the related customer removal operators used by Shaw (1998) and Azi et al. (2014).
O	However, while Azi et al. (2014) build on Shaw (1998) by defining a proximity measure based on spatial and temporal distance, we apply two variants of the operator.
O	The acceptance criteria for candidates is based on a simulated annealing rule, as in Ropke and Pisinger (2006).
O	Based on the instances of Song and Ko (2016), the customer demands are volume based and range between 0.3 m3 and 1.8 m3, the capacity of the vehicles used for the customer routing is set at 12 m3.
O	All other parameter values were initially set equal to those of Ropke and Pisinger (2006) and then sequentially altered in the tuning phase.
O	The current body of research on this challenging problem is rich and diverse and has resulted in various problem formulations with extensions to robust time/cost optimization (Hazır et al., 2011; 2010a), stochastic durations (Kang and Choi, 2015; Klerides and Hadjiconstantinou, 2010), budget uncertainty (Yang, 2005), and well as perception about problem complexity and uncertainty (Wauters and Vanhoucke, 2016).
P	The time/cost relation of the activities is modelled as a (piecewise) linear function, convex or concave function, but the most well-known variant is the discrete time/cost trade-off problem (De et al., 1995; 1997) in which only predefined discrete time/cost points can be chosen for each project activity.
N	This NP-hard problem has been solved using exact and meta-heuristic search algorithms, for which a summary is given in Vanhoucke and Debels (2007).
O	Recent summaries of the latest developments in research can be found in papers by Hazır et al. (2010b) and Vanhoucke (2015).
P	Despite this overwhelming amount of problem formulations, only one paper has - to the best of our knowledge - explicitly taken the relation between the project owner and contractor into account (Szmerekovsky and Venkateshan, 2012).
P	Scheduling projects to maximize the net present value was originally introduced in Russell (1970) and has resulted in various exact and heuristic algorithms under different payment models (see e.g. Icmeli et al., 1993 and Vanhoucke et al., 2001).
O	Simultaneously determining both the timing of payments and the completion times of activities has resulted in the socalled payment scheduling problem (Grinold, 1972).
O	However, Szmerekovsky (2005) argues that the project baseline schedule is typically determined by the contractor rather than the owner, and proposes a model in which the owner selects the payment activities and the contractor selects the activity schedule, each to maximize their own net present value.
P	The research builds further on the owner/contractor negotiation problem for the payment scheduling problem for which an overview is given in Dayanand and Padman (1999).
O	Dayanand and Padman (2001) consider the problem of simultaneously determining the amount, location, and timing of progress payments in projects from an owner’s perspective, and present three mixed-integer linear programming models, based on some practical methods of determining payment schedules from different types of project contracts.
P	Further efforts to model the bargaining process between project owner and contractor in order to reach a net present value solution which is fair and impartial for both parties have been proposed by Ulusoy and Cebelli (2000), Kavlak et al. (2009) and Bahrami and Moslehi (2013).
P	Choi et al. (2011) argue that incentive/disincentive and cost-plus-time are the two most widely used alternative contracting strategies to implement public transportation infrastructure projects in the United States.
P	In their paper, Kettunen and Kwak (2018) develop an optimization framework to solve the problem of scheduling procurement contracting, and show that optimization of the project schedule can contribute to the design of the project contract.
P	In Tran et al. (2018), the authors advance the understanding of the impact of the competitive guaranteed maximum price (GMP) contracting strategy on project costs, and provide through several case studies guidance for owners and practitioners to better understand the cost implications of competitive GMP projects throughout the project life.
P	A quantitative framework and extensive summary of the literature for incentive contract design for projects has been recently proposed by Kerkhove and Vanhoucke (2016).
O	However, the authors extend this time/cost trade-off to the scope dimension to model the well-known time/cost/scope trade-off of the project’s iron triangle (Marques et al., 2011).
P	To the best of our knowledge, the only study similar to the current problem formulation is written by Kerkhove and Vanhoucke (2017) who integrate the well-known multi-mode project scheduling problem with incentive contracting.
P	Moreover, Choi and Kwak (2012) are the only authors who present a decision support model for incentives and disincentives to include time/cost trade-offs and propose a model to assist decision makers in estimating better incentive amounts.
N	In the first step, the project is optimized as a joint enterprise between the owner and contractor and consists of the classic time/cost trade-off problem known in the literature (De et al., 1995).
N	It has been mentioned earlier that the problem formulations show some similarities with the work by Kerkhove and Vanhoucke (2016, 2017), but it is crucial to highlight some important differences in some essential aspects of our study.
O	Similar to Kerkhove and Vanhoucke (2016), the contract design will be limited to linear incentive contracting, i.e. time and cost are linearly related to each other, and incentives are used to align the project objectives to the objectives of both owner and contractor. 
N	Unlike Kerkhove and Vanhoucke (2017), we have extended the project scheduling problem to incorporate effort under risk, which is a central theme in our study.
P	Such a translation of project time into a monetary value has been used by Kerkhove and Vanhoucke (2016) through a so-called valuation parameter v, not only to translate the project time (parameter vd), but also the scope dimension of the project (parameter vs), into monetary units.
O	Hence, the contractor may make decisions that conflict with the owner’s objective, which is known in the principal-agent literature as the moral hazard problem (Bolton and Dewatripont, 2005; Laffont and Martimort, 2002; Rowell and Connelly, 2012).
O	An option to deal with this problem is to contract based on the project’s actual outcome (Hölmstrom, 1979; Holmstrom and Milgrom, 1987).
O	The certainty equivalent has the form of a mean-variance utility, see Gibbons (2005) and Lal and Srinivasan (1993).
P	This contract is called the second-best contract in principal-agent theory because it is optimal given information asymmetry between the contractor and the owner, (Bolton and Dewatripont, 2005; Laffont and Martimort, 2002).
P	In fact, problem CP reduces to the discrete time/cost tradeoff problem, which is known to be NP-hard (De et al., 1997).
P	In fact, CP can be solved using exact methods such as branch and bound (Demeulemeester et al., 1998), Benders decomposition (Erengüç et al., 1993) or meta heuristics (Vanhoucke and Debels, 2007).
N	This is in line with principal-agent theory, where it is known that when the agent’s (contractor) action is observable, it is optimal for the principal (owner) to make a fixed payment to the agent, see for example Laffont and Martimort (2002) and Bolton and Dewatripont (2005); we also prove this in Section 5.2 for a stylized model.
O	These restrictive assumptions are commonly made in the principal-agent literature, see for example Prendergast (1999), Gibbons (2005) and Bolton and Dewatripont (2005).
P	This analysis follows principal-agent literature, see for example Gibbons (2005) and Bolton and Dewatripont (2005).
O	The network is taken from Demeulemeester et al. (1998) and will be used for our numerical example.
O	More precisely, we have used the random project network generator RanGen2 (Vanhoucke et al., 2008) and controlled the topological structure using the Serial/Parallel (SP) indicator that measures the closeness of a project network to a completely serial (SP = 1) or parallel network (SP = 0).
P	The problems have been solved by iterating over a discrete set of values for the bonus coefficient s (from 0 to 1 in steps of 0.05), each time solving the contractor’s problem using the branch and bound of Demeulemeester et al. (1998).
P	Triggered by the seminal work by Newman and Girvan (2004) in the literature of the community detection, maximizing the modularity function has extensively been studied.
N	Modularity maximization is now one of the central subjects in this field, but has also received serious criticism from mainly two viewpoints: degeneracy (Good et al., 2010) and resolution limit (Fortunato and Barthélemy, 2007).
P	Even in a schematic case where a graph consists of multiple replicas of an identical clique which are connected by a single edge, Fortunato and Barthélemy (2007) showed that maximizing the criterion results in regarding two or more cliques connected as a community when the number of cliques in the graph is larger than the square root of the number of edges.
P	To avoid the resolution limit issue, Li et al. (2008)1 proposed a new function, called modularity density, and their theoretical analysis with respect to maximizing the function leads to detecting communities with different scales.
P	In fact, Li et al. (2008) fixed the number of communities and solved the continuous relaxation problem. 
P	Although Karimi-Majd et al. (2015) presented an improved formulation that does not require the number of communities to be known, it is still a binary nonlinear fractional programming problem.
P	To date, several metaheuristic approaches have been developed: ones based on a genetic algorithm by Liu and Zeng (2010), a memetic algorithm with simulated annealing in its local search phase by Gong et al. (2012) and biological operations by KarimiMajd et al. (2015).
P	Costa et al. (2016) proposed hierarchical divisive heuristics based on repetitive resolutions of an integer linear programming (ILP) problem or a mixed integer linear programming (MILP) problem to split a community into two.
O	Santiago and Lamb (2017) presented seven scalable heuristic methods, and compared them with the metaheuristic algorithms mentioned above as well as the heuristics by Costa et al. (2016).
O	Izunaga et al. (2016) formulated the problem as a variant of a semidefinite programming problem called 0-1SDP.
N	The exact formulation proposed by Li et al. (2008), Karimi-Majd et al. (2015) or Izunaga et al. (2016) has not yet been solved to optimality due to its nonlinearity.
P	Costa (2015) presented several MILP reformulations, which enables an application of general-purpose optimization solvers to the problem.
O	Costa et al. (2017) discussed MILP reformulations of the upper bound calculation, providing the whole modularity density maximization process completely expressed as MILP problems.
O	Izunaga et al. (2016) calculated the upper bound in their numerical experiments for comparison by the parametric algorithm by Dinkelbach (1967) in which a series of ILPs was solved.
O	Very recently, and independently of our work, de Santiago and Lamb (2017) have considered the clustering problem as the setpartitioning problem and have presented its ILP formulation (refer, for instance, to Nemhauser and Wolsey, 1999 on the setpartitioning and related problems as well as their ILP formulations).
P	They have solved the problem by column generation (refer, for instance, to Desrosiers and Lübbecke, 2005 on column generation), in which framework an initial set of columns is given by heuristics that has stochastic behavior.
P	In this paper, independently of the work by de Santiago and Lamb (2017), we regard the modularity density maximization as the set-partitioning problem and present its ILP formulation, which enables us to devise an efficient algorithm to provide an optimal solution for the modularity density maximization.
N	To be specific, we develop an algorithm based on a branch-and-price framework, i.e., column generation in a branch-and-bound framework, to truly, exactly optimize the modularity density function value (refer to Barnhart et al., 1998 as well as Desrosiers and Lübbecke, 2005 on branch-and-price).
P	We also incorporate two existing techniques into the algorithm: the set-packing relaxation proposed by Sato and Fukumura (2012), which was originally applied to a setpartitioning-based scheduling problem, and the multiple-cuttingplanes-at-a-time by Izunaga and Yamamoto (2017), which was originally done to the modularity maximization, to accelerate the column generation process within the algorithm.
N	We present the set-partitioning formulation of the modularity density maximization and column generation for that, referring to the recently proposed IQP formulation of the column generation subproblem by de Santiago and Lamb (2017).
P	Hence, as it was done to the modularity maximization by Aloise et al. (2010), we can regard the modularity density maximization as the set-partitioning problem.
N	Here let us introduce what we call column generation “at the root node,” which has also been presented by de Santiago and Lamb (2017) quite recently.
O	Note that de Santiago and Lamb (2017) have generated 30 columns to form the initial column set C(1) by heuristics that has stochastic behavior.
P	To find a solution to this problem, de Santiago and Lamb (2017) have presented different stochastic heuristics.
O	Although its objective function is nonconvex, it can be cast as an equivalent convex programming problem (refer, for instance, to Billionnet and Elloumi (2007).
P	For each of the instances which have been solved by de Santiago and Lamb (2017), the solution is integral, thereby indicating that it is an optimal solution to the modularity density maximization.
O	There are three major differences between our approach presented in the next section and that by de Santiago and Lamb (2017):
P	We follow the standard “identical restrictions on subsets” branching rule for the set-partitioning problem by Barnhart et al. (1998), which dates back to Ryan and Foster (1981).
N	Straightforward column generation applied to the setpartitioning problem unfortunately requires much computation time for large instances due to degeneracy (in the LP context), as Lübbecke and Desrosiers (2005) pointed out.
P	For a set-partitioningbased minimization problem in the field of scheduling, Sato and Fukumura (2012) proposed the set-covering relaxation to overcome the disadvantage.
P	Although this approach is much simpler than stabilized column generation proposed by du Merle et al. (1999), it contributed to enough computation time reduction for their scheduling problem instances.
O	We follow the standard “identical restrictions on subsets” branching rule for the set-partitioning problem by Barnhart et al. (1998), which dates back to Ryan and Foster (1981).
O	Straightforward column generation applied to the setpartitioning problem unfortunately requires much computation time for large instances due to degeneracy (in the LP context), as Lübbecke and Desrosiers (2005) pointed out.
O	For a set-partitioningbased minimization problem in the field of scheduling, Sato and Fukumura (2012) proposed the set-covering relaxation to overcome the disadvantage.
O	Although this approach is much simpler than stabilized column generation proposed by du Merle et al. (1999), it contributed to enough computation time reduction for their scheduling problem instances.
O	Such techniques are also reviewed in Lübbecke and Desrosiers (2005).
P	Izunaga and Yamamoto (2017) introduced the multiple-cutting-planes-at-a-time technique for its column generation subproblem of the modularity maximization.
O	Note that an interior point method is applied to this problem according to an indication by Vanderbeck (2005) that fewer iterations are required for column generation to terminate if an analytic center of the optimal face is provided as the solution.
N	For comparison, we also solve them by the best MILP formulation called MDB by Costa (2015), and by our branch-and-price algorithm in which the column generation subproblem is (SIQP () ) modeled by de Santiago and Lamb (2017) with the branching constraints (7) and (8).
O	We calculate the upper bound value of the contribution of a community required as input of MDB by the parametric algorithm by Dinkelbach (1967), as Izunaga et al. (2016) did.
O	They are from Costa (2015) (IDs 01–10), Costa et al. (2016) (IDs 11, 13–15), de Santiago and Lamb (2017) (IDs 12, 16) and Santiago and Lamb (2017) (IDs 17, 18), respectively.
O	The programs are implemented in Python 3.5.2, calling the Python API of Gurobi Optimizer 7.0.2 (developed by Gurobi Optimization, 2017) to solve the LP, ILP, MILP and IQP problems.
N	We have also added the heuristic solution obtained by the best heuristics reported by Santiago and Lamb (2017) (MCN for ID 17 and CM+LNM for ID 18) to the initial set of columns at Operation 2 of Procedure 1 in our algorithm, which has brought no improvement.
O	The symbol ‘MDB’ means the best formulation by Costa (2015), ‘BP-(SIQP () )’ our branch-and-price approach combined with the column generation subproblem formulation by de Santiago and Lamb (2017) and ‘BP-(SMILP (b,,q) )’ our approach with the MILP subproblem formulation.
O	The multiple-cutting-planes-at-a-time technique applied to the standard set-partitioning column generation process has been shown to be quite effective, as it was shown on the modularity maximization by Izunaga and Yamamoto (2017). 
O	Table 5 compares lower bounds of the objective value obtained in this manner with those obtained by the best heuristics reported by Santiago and Lamb (2017) (MCN for ID 17 and CM+LNM for ID 18).
P	One way is to search for a solution with a positive objective value by heuristics as it was done by de Santiago and Lamb (2017).
O	Several versions of this classical AP model have been studied, including but not limited to the k-cardinality assignment, the bottleneck assignment (BAP), the balanced assignment, the minimum deviation assignment, the lexicographic bottleneck, the semi-assignment, and the categorized assignment models (see Pentico, 2007, and the related references therein).
O	The resulting problem is referred to as generalized assignment problem and a survey by Cattrysse and Van Wassenhove (1992) discusses a variety of its applications.
O	If the agent capacities are determined with respect to multiple resources the problem becomes a multiple resource generalized assignment problem (Karsu and Azizoglu, ˘ 2012, 2014).
O	There are two versions of these bottleneck models: task bottleneck and agent bottleneck, in which the maximum cost over all tasks and the maximum cost over all agents are minimized, respectively (Mazzola and Neebe, 1988; Karsu and Azizoglu, ˘ 2012). 
P	The quadratic assignment problem (QAP), is another important variant of the AP that uses a quadratic objective function of the following form (Lawler, 1963):
N	Loiola et al. (2007) state that the QAP is one of the hardest NP-hard problems.
P	Integer linear programming, mixed integer linear programming, permutation-based, trace and graph formulations of the QAP have been developed and exact solution methods such as branch and bound, branch and cut, Bender’s decomposition, and dynamic programming have been proposed (Loiola et al., 2007; Burkard, 2013; Drezner, 2015).
O	Unlike the QAP, the number of objects is not necessarily equal to the number of locations, i.e., the assignment constraints are replaced by semiassignment constraints (Pitsoulis, 2009).
P	The QSAP is introduced by Greenberg (1969) and shown to be strongly NP-hard by Sahni and Gonzalez (1976).
O	The suggested solution approaches include integer programming and column generation methods (Ernst et al., (2006)), and mathematical programming and branch-and-bound algorithms (Billionnet et al., 1992; Magirou and Milis, 1989; Sinclair, 1987; Stone, 1977).
O	Milis and Magirou (1995) define lower bounds for the QAP using the task allocation problem. 
P	Malucelli and Pretolani (1995) develop a class of lower bounds for the QSAP and Malucelli (1996) defines some polynomially solvable cases.
O	Saito et al. (2009) discuss the theoretical aspects of the quadratic semi-assignment polytope.
O	Drwal (2014) studies a special case of the QSAP, which assigns n tasks to m server machines while minimizing the sum of worst-case processing times.
P	Several metaheuristic algorithms are proposed for the QSAP (Wang and Punnen, 2017) and for some of its special cases (Punnen and Wang, 2016).
O	Ensuring a balanced (fair) workload allocation is an important concern in task assignment problems, hence various ways of incorporating fairness into assignment decisions have been discussed in a number of papers (see Karsu and Morton, 2015 and the references therein).
O	One of the methods used to ensure a balanced allocation of a good (bad) is maximizing (minimizing) a specific Schur-concave (Schur-convex) function that aggregates the outcomes (Karsu and Morton, 2015).
O	One could also use other Schur-convex function forms, but the sum of squares functions (and in general convex functions of the form Sigma i yα i : α ≥ 1) are considered appropriate for ensuring fairness in many applications (see for example Martin et al. (2013) for scheduling and Lulli and Odoni (2007) for air traffic flow management applications).
O	This follows that if all pijvalues are integer, once the integrality constraints on the xijvalues are relaxed, the optimal xijvalues are either 0 or 1 (we refer the reader to Ahuja et al. (1993), for network flow models and in-depth treatment of the total unimodularity property).
O	One may solve the assignment problem at any node and benefit from the dual variables to eliminate the nonpromising descendant nodes without solving the corresponding dual problem (seeRinnooy Kan et al., 1975, for the minimum total cost single machine scheduling problem).
O	An alternative option is to solve the assignment problem only at the root node and use the dual variables of the root node solution to find lower bounds at all other nodes (see Azizoglu and Kirca, 1999, for the minimum total cost unrelated parallel machine scheduling problem).
P	Future research can also be performed on extending the models to solve other practical problems such as the ergonomic job rotation problem (Otto and Battaïa, 2017), which aims at ensuring a balanced distribution of risks among individual work assignments.
P	An important class of bi-objective problems on graphs is modeled by means of two performance measurements, a cost function and a bottleneck function (e.g., see Berman et al. (1990) and Hansen (1980)).
O	Network densification is identified as one of the key strategies to evolve the communication infrastructure, mainly in the context of wireless devices, according to Bhushan et al. (2014).
N	However, load balancing strategies are needed to improve performance, and make effective use of the network capillarity, as described by Pham and Perreau (2004).
O	Thus, they create heuristics to efficiently solve the problem of jointly optimizing the network bottleneck (i.e., load balancing) and the flow path length, such as in the case of G´alvez and Ruiz (2013), Liu et al. (2012), and de Mello et al. (2016).
P	Exact and polynomial algorithms for solving bi-objective problems with a cost function and a bottleneck function have been created by Berman et al. (1990), Hansen (1980), Cl´ımaco and Martins (1982) and Gadegaard et al. (2016), for example. Bornstein et al. (2012) designed an exact and polynomial algorithm to solve problems with a cost function and multiple bottleneck functions.
O	The algorithm is based on the -constraint method (e.g., see Ehrgott (2005)) and operates iteratively, by solving a single-objective linear integer programming subproblem in each iteration.
O	In a general multi-objective integer linear programming problem, the objective functions are linear and, naturally, the problem is NP-hard because the integer linear programming problem is NP-hard (e.g., see Schrijver (1986)).
O	There have been a lot of papers about multi-objective integer programming in the field of mathematical optimization (e.g., see Chalmet et al. (1986), Ehrgott (2006).
O	Klein and Hannan (1982), Lokman and K¨oksalan (2013), Ozlen et al. (2014), and Boland et al. (2016)).
O	In general, multi-objective optimization problems aim at finding a minimal complete set of Pareto-optimal solutions (Hansen (1980)).
O	Since the matrices A1, A2, . . . , Ar, and Im are totally unimodular, A is also totally unimodular (see Schrijver (1986)).
O	In addition, the right-hand side vector of (P) only has integer values and so (P) can be solved in polynomial time (see Maurras et al. (1981)).
P	The proof of the algorithm’s optimality is similar to that presented by Bornstein et al. (2012).
O	The random topology was generated by the Barab´asi-Albert model, see Barab´asi and Albert (1999), which is described as follows.
O	The second setting often appears in wireless sensor networks and some types of wireless mesh networks, de Mello et al. (2016).
O	Additionally, the random topology results in smaller sums of the path lengths caused by their topological properties, see Barab´asi and Albert (1999), such as the presence of vertices that are highly connected, i.e., high degree vertices.
O	Our exact approach might be regarded as computationally expensive in some scenarios, e.g., in G´alvez and Ruiz (2013); de Mello et al. (2016), where the traffic flow dynamics and the demand for fast decisions require heuristic approaches.
O	The permutation flowshop scheduling problem with limited buffers (PFSP-LB) is a generalization of the regular permutation flowshop scheduling problem (PFSP) (Fernandez-Viagas et al., 2017) by considering inter-machine buffers with restricted capacity.
P	As reviewed in the literature (Hall and Sriskandarajsh, 1996), the prototype of blocking and no-wait flowshop can be found in many modern manufacturing industries such as plastic molding, steel production, pharmaceutical processing, and information service and so on.
O	The PFSP-LB is known to be NPhard in the strong sense when deploying more than two machines (Papadimitriou and Kanellakis, 1978).
O	Consequently, the approximation methods are invested a large amount of efforts such as the heuristics, meta-heuristics and their hybridization methods (Leisten, 1990; Smutnicki, 1998; Nowicki, 1999; Wang et al., 2006; Liu et al., 2008; Pan et al., 2011a,b; Moslehi and Khorasanian, 2014; Qian et al., 2009; Grabowski and Pempera, 2007; Wang and Liu, 2013; Gourgand et al., 2005; Pranzo, 2004; Wardono and Fathi, 2004).
O	Nowadays, the realistic problems of concurrent and globalized production have necessitated distributed or multi-factory manufacturing, since such production pattern enables enterprises to attain higher product quality, better corporate reputation, as well as reduce manufacturing and delivery costs and period (Behnamian and Fatemi Ghomi, 2016; Kahn et al., 2004).
O	As a result, in the last years researchers are paying special and increasing attention to different distributed scheduling problems (DSPs) (FernandezViagas et al., 2018; Zhang et al., 2018a,b; Zhang and Xing, 2018).
O	Behnamian (2014) used a decomposition-based VNS-TS algorithm for tackling a distributed parallel factories scheduling problem.
P	De Giovanni and Pezzella (2010) proposed an improved GA technique to handle a distributed and flexible jobshop scheduling problem.
O	Jia et al. (2003) handled a distributed scheduling problem by using a modified GA, and further Jia et al. (2007) incorporated Gantt chart into GA algorithm to figure out the distributed jobshop scheduling problem.
O	Chan et al. (2005) used an adaptive GA merged dominant genes (GADG) to deal with the large-scale distributed jobshops, and later Chan et al. (2006) applied GADG to address a distributed flexible manufacturing system.
O	Xiong et al. (2014) presented three metaheuristics for scheduling a distributed two-stage assembly flowshop system, and then for such a problem Deng et al. (2017) proposed a competitive memetic algorithm.
P	More DSPs with assembly operations can be found in the recent review of Framinan et al. (2019), which presents the most general research framework for assembly-type scheduling problems.
O	A typical DSP is referred to as the distributed permutation flowshop scheduling problem (DPFSP) (Naderi and Ruiz, 2010), in which there are two or more factories and each one is set a PFSP layout.
O	For instance, there are genetic algorithm (Gao and Chen, 2011), tabu search algorithm (Gao et al., 2013), estimation of distributed algorithm (Wang et al., 2013), iterated greedy algorithm (Fernandez-Viagas and Framinan, 2015), immune algorithm (Xu et al., 2014), and scatter search algorithm (Naderi and Ruiz, 2014).
P	More recently, a few new contributions (Fernandez-Viagas et al., 2018; Companys and Ribas, 2015; Ying and Lin, 2017; Ribas et al., 2017) have been proposed to tackle different DPFSP variants such as those without buffers or with zerocapacity buffers.
P	The heuristic of shortest processing time (SPT) is a simple yet effective method for solving the PFSP (Vollmann et al., 2011).
P	Nawaz et al. (1983) proposed NEH heuristic for the first time Ruiz and Maroto (2005) pointed out that its performance performed the best among the heuristics known for solving the PFSP.
O	Basic DE is the population-based and stochastic metaheuristics (Storn and Price, 1997; Das and Suganthan, 2011), which is originally developed based on the floating-point representation so that its iteration can be performed only in a continuous search space.
O	To explain the above DDE mutation operator, consider Ya = (Papadimitriou and Kanellakis, 1978; Fernandez-Viagas et al., 2017; Hall and Sriskandarajsh, 1996; Smutnicki, 1998; Leisten, 1990), Yb = (Hall and Sriskandarajsh, 1996; Fernandez-Viagas et al., 2017; Papadimitriou and Kanellakis, 1978; Leisten, 1990; Smutnicki, 1998), π = [J5, J2, J4, J1, J3], and κ = 0.4.
O	Applying formula (7.2) for Ya and Yb, we can obtain  = [(Papadimitriou and Kanellakis, 1978), 0, (Hall and Sriskandarajsh, 1996; Smutnicki, 1998; Leisten, 1990)].
O	To interpret the DDE crossover operator, consider π = [J5, J2, J3, J1, J4], V = [J1, J3, J5, J4, J2] and suppose that  = (Fernandez-Viagas et al., 2017; Papadimitriou and Kanellakis, 1978; Smutnicki, 1998).
O	Combining the instance generation technique (Naderi and Ruiz, 2010; Taillard, 1993) and the considered buffer constraints, 2160 testing instances are constructed in this experiment.
O	A non-parametric Wilcoxon signed-rank test (García et al., 2009) is conducted based on the data in Table 1.
P	Naderi and Ruiz (2014) pointed out that their scatter search (SS) algorithm was the most competitive metaheuristic for solving the DPFSP.
P	Up to present, there are only two literatures (Companys and Ribas, 2015; Ying and Lin, 2017) for the blocking-type DPFSP, and the authors in (Ying and Lin, 2017) have proved that their HIG1 algorithm was the most efficient among all the related approaches.
O	 Using identical machines connected in various configurations is common in the semiconductor industry (Bureau et al. 2006).
O	While the single row facility layout problem (SRFLP) concerns the arrangement of facilities on a single row as in Heragu and Kusiak (1991), Amaral (2006, 2008), the DRLP considers two rows of machines, an upper and a lower.
O	It has widespread application in industry, including semiconductor manufacturing as studied in Zuo et al. (2016a), because determining the exact location for each machine may result in a layout with less material flow cost than only choosing the sequence separated by standard clearances.
O	However, the production area may also be an important factor, such as in high value manufacturing environments where material flow can be complex and construction costs (e.g., clean room) are approximately $3,500 per square foot or higher (Turley 2002).
P	Thus, Murray et al. (2012) proposed an extended double row layout problem (EDRLP) with non-zero aisle width and the optimization objectives of both cost and layout area were linearly combined to form a single objective.
P	A multi-objective heuristic, suitable for larger problems, was proposed in Zuo et al. (2014).
P	Although the research on DRLP goes back to Heragu and Kusiak (1988), the first mixed integer formulation of the problem was developed by Chung and Tanchoco (2010).
O	Expressing machine location in dual-parameters is a common practice, and it dates back to another work of Heragu and Kusiak (1991) where a binary-continuous variable combination is used in the SRFLP formulation.
O	An alternative to the Chung and Tanchoco (2010) formulation was proposed by Amaral (2013b) with a fewer number of variables and constraints.
P	Over several test cases, Amaral (2013b) showed that his approach performs faster than the formulation proposed by Chung and Tanchoco (2010).
P	The performance of Amaral (2013b) is further improved in a modified formulation of Secchin and Amaral (2018).
P	Recently, Amaral (2018) re-formulates DRLP so that it can be more intuitive in handling qualitative input.
O	Several papers have appeared to address the “double row layout” problem, although the problem they study is different from that defined by Chung and Tanchoco (2010).
O	In the CAP formulation by Amaral (2012) there is no gap between the wall and the leftmost facilities.
O	PROP can be considered as an extension of SRFLP, as facilities are restricted to specific rows (Amaral 2013a).
O	This category of combinatorial problems also includes minimum duplex arrangement problems where n facilities of equal length are assigned to n locations (Amaral 2011).
O	This paper builds upon the existing double-row layout problem (DRLP) literature, previous examples of which are provided by Heragu and Kusiak (1988), Chung and Tanchoco (2010), Zhang and Murray (2012), and Murray et al. (2013).
O	The DLRP as formulated in this research is based on the mixed integer formulation of Chung and Tanchoco (2010).
O	This original work was modified and expanded by Zhang and Murray (2012), Murray et al. (2013), and Zuo et al. (2016b).
O	In the Chung and Tanchoco (2010) formulation, the integer variables determine the ordering of machines in the upper and lower rows of the layout.
O	Unlike our approach, the Chung and Tanchoco (2010) formulation does not include machine replicates, re-entrant flows or positive aisle width; it can be solved as a mixed integer program.
O	Within the context of the DRLP, Castillo and Peters (2004) used a two-stage solution approach for a multi-bay manufacturing facility with replicate machines.
P	Another two stage approach is proposed by Tubaileh and Siam (2017) within the context of a multi-row layout problem.
P	Perhaps most related to our problem is the work presented by Solimanpur and Jafari (2008).
P	Solimanpur and Kamran (2010) propose a genetic algorithm to solve a simplified version of the model described in Solimanpur and Jafari (2008).
P	Taghavi and Murat (2011) propose a heuristic to solve the exact model formulated by Solimanpur and Jafari (2008).
O	A similar approach is used by Wang et al. (2015) in solving dynamic double row layout problems.
O	Ioannou (2007) restricts the maximum amount of flow on a link.
P	Zhang et al. (2011) propose a flow assignment model with congestion to design layouts and flow routing decisions.
P	For example, Urban et al. (2000) proposed the integrated machine allocation and layout problem (IMALP).
O	A similar approach was used by Amaral (2011), where mixed integer linear optimization is used for identically-sized departments.
O	Smilarly, Jaramillo and McKendall, Jr. (2010) present a generalized QAP formulation with multi-product flows.
O	More recently, Zhao and Wallace (2013) formulate the QAP with machine types and duplicate machines in each machine type category.
N	Castillo and Peters (2003) consider a layout problem in which the department shapes are not assigned a priori.
N	Although we are not aware of any application of decomposition approaches for the DRLP, such approaches have been successfully utilized in network flow problems, such as Campbell and Savelsbergh (2004), Romero and Monticelli (1994), Harjunkoski and Grossmann (2001), Yao et al. (2013).
O	Gupte et al. (2016) describe the pooling problem with alternate formulations and discuss discretization methods to approximate the BLP as an MILP. Metaheuristics have also been used to solve pooling problems.
P	Erbeyoglu and Bilge (2016) propose simulated annealing (SA) and particle swarm optimization (PSO) to solve larger instances of the problem.
P	Kosucuoglu and Bilge (2012) propose a GA-LP heuristic to decompose the problem into two subproblems.
O	Nahapetyan and Pardalos (2007) studied a network flow problem with a piecewise cost function.
O	Such solution approaches to quadratic problems are discussed in Audet et al. (2004).
O	Once the relative position of each machine is determined, the problem is formulated and solved as BLP in the lower level by following the methodology described in Audet et al. (2004).
O	Similar approaches were previously used in solving various facility layout problems such as in Montreuil et al. (2004) and Kulturel-Konak (2012).
O	A solution approach defined in Audet et al. (2004) uses an iterative heuristic that divides decision variables into two subsets. 
O	In the area of facility planning, TS is used by McKendall and Jaramillo (2006), McKendall (2008), Samarghandi and Eshghi (2010), and da Silva et al. (2012), for example. In our implementation, the TS starts with a randomly-generated single layout.
P	This solution approach is well established in the dynamic vehicle routing literature (c.f. Psaraftis (1980), Savelsbergh and Sol (1998), Montemanni et al. (2005), Chen and Xu (2006)).
O	A detailed discussion on the TSP can be found in surveys by Golden et al. (2008) and Eksioglu et al. (2009).
O	Dumitrescu et al. (2010) discuss a TSP variant with pickups and deliveries (TSPPD), where each delivery request corresponds to exactly one pickup.
O	A further extension of the TSP known as the multiple traveling salesman problem (mTSP) considers scenarios with multiple vehicles (c.f. Bektas (2006) and Kergosien et al. (2009)).
O	Similarly, both Gelareh et al. (2013) and Paolo and Vigo (2014) provide an overview of the VRP and its many extensions.
O	A survey of the dynamic VRP (DVRP) is provided by Psaraftis (1995).
P	A model for event-driven optimization of a DVRP is presented by Pillac et al. (2012), in which a problem is re-optimized upon the arrival of a new request.
O	More specific considerations such as capacitated vehicles, waiting strategies in dynamic environments, and solution approaches for dynamic problems can be found in Pillac et al. (2013).
O	Stochastic DVRP models and models which implement rolling horizons can be found in Bektas et al. (2014).
N	Additional problem features such as heterogeneous vehicle fleets, time windows, and traffic networks have also been explored (c.f. Fleischmann et al. (2004), van Woensel et al. (2008), Schyns (2015)).
O	Overviews of the staticdeterministic PDP are provided by Berbeglia et al. (2007) and Parragh et al. (2008).
O	Crainic et al. (2016) consider various classifications of both travel and demand types. 
O	In the dynamic PDP (DPDP), pickup and delivery requests are handled in real time (c.f. Berbeglia et al. (2010)).
O	Savelsbergh and Sol (1995) allow for multiple pickups per delivery request, while Liu et al. (2013) consider various types and combinations of pickups and deliveries.
O	In Savelsbergh and Sol (1998), time windows constraints are applied to pickups, deliveries, and vehicle availability.
P	Like the DVRP, the DPDP has been widely studied with respect to various problem features (Goel and Gruhn (2008), Lin (2011)) and solution techniques (Benavent et al. (2015), Cherkesly et al. (2015), Cherkesly et al. (2016)).
P	As described in Battarra et al. (2014), PDPs are traditionally classified as either one-to-one, one-to-many-to-one, or many-to-many.
P	For a deeper discussion on the classical DARP, the interested reader is referred to the review papers by Cordeau and Laporte (2007) and Berbeglia et al. (2010).
P	Parragh et al. (2009) provide a multi-objective model, which additionally seeks to minimize the average ride time for a customer. 
O	The modeling of customer-experience considerations is also discussed in Doerner and Salazar Gonz´alez (2014).
O	Once again, heterogeneous vehicle fleets and stochastic system conditions have been studied in relation to the DARP (c.f. Jain and Van Hentenryck (2011) and Xiang et al. (2008), respectively). 
P	Various heuristic techniques have also been applied to the dynamic DARP, including parallel tabu search, fuzzy logic, and hyperheuristics (Attanasio et al. 2004, Maalouf et al. 2014, Urra et al. 2015).
P	The school bus routing problem (SBRP), first defined in Newton and Thomas (1969), shares strong ties to the DARP.
O	In Doerner and Salazar Gonz´alez (2014), a version of the SBRP which forces clusters of students to be picked up within specific time windows is considered.
P	B¨ogl et al. (2015) provides a further extension of the SBRP which allows students to transfer buses en route to their final destination.
O	Another extension of the SBRP which considers stochastic demand can be found in Caceres et al. (2017).
O	An exact solution approach to the meal delivery routing problem (MDRP) is presented in Yildiz and Savelsbergh (2017).
P	In Reyes et al. (2018), the authors introduce a dynamic-deterministic model for the MDRP, which they solve using a rolling horizon approach.
O	Similar to the MDRP, the restaurant meal delivery problem (RMDP) is defined by Ulmer et al. (2017).
N	In our system, travel times are queried directly from the MapQuest Developer Network API (MapQuest, 2018) each time a solve is triggered, and thus accurately reflect the road networks over which problems were tested.
O	In their work on the DARP, Diana and Dessouky (2004) define a measure of decentralization, or dispersion, which ensures that vehicles are spread out.
O	Similarly, Ritzinger et al. (2016) define spatial and temporal utilization measurements for the stochastic VRP.
O	Batta et al. (2014) define a dispersion metric for a facility location problem which requires at least a certain distance exists between each pair of facilities.
O	Further, rental and return demands are uncertain and subject to heterogeneous spatial-temporal patterns (Büttner et al., 2011).
P	The success of BSSs strongly depends on two factors: density of stations within the city and a reliable availability of bikes and free bike racks any time and at any station (Gauthier et al., 2013).
O	Data analysis provides insights into demand patterns (Borgnat et al., 2011; O’Brien et al., 2014; Vogel et al., 2011).
O	Further, detailed data of user bike trips is aggregated into typical bike flows (Vogel et al., 2017).
O	Here, flows are incorporated in order to determine appropriate distributions of bikes as starting point for the next morning (we refer to Espegren et al., 2016 for a survey).
O	Therefore, periodic deterministic models aim at optimizing relocation activities over the course of the day (Brinkmann et al., 2016).
O	The problem at hand is an inventory routing problem (IRP) and contains both stochasticity and dynamism (Coelho et al., 2014b).
O	As a result, we consider a stochasticdynamic inventory routing problem (SDIRP) for bike sharing systems (Brinkmann et al., 2015).
O	Short horizons of one or two hours may not be able to capture many important future developments while long horizons over several hours may lead to a significant discrepancy between simulated and actually realized outcome (Ghiani et al., 2009; Voccia et al., 2017).
O	Ulmer (2017) shows that more predictable demand patterns require different horizons than demand patterns blurred by statistical noise.
O	The horizons per hour are determined a priori by means of value function approximation (VFA), a method of approximate dynamic programming (ADP, Powell, 2011).
O	In comprehensive computational studies based on real-world data of the BSS in Minneapolis (Minnesota, USA, MN, 2016), we show that the VFA-parametrized DLA significantly outperforms LAs with static horizons and manually parametrized DLAs as well as benchmark policies from the literature.
O	The SDIRP is stochastic and dynamic as defined by Kall and Wallace (1994).
O	Examples are works by Chemla et al. (2013), Raviv et al. (2013), Erdogan ˇ et al. (2014), Erdogan ˇ et al. (2015), Kloimüllner et al. (2015), Espegren et al. (2016) and Schuijbroek et al. (2017).
P	Other static and deterministic works develop master tours to meet typical daily user demand, for example, Contardo et al. (2012), Kloimüllner et al. (2014), Neumann Saavedra et al. (2015) and Neumann Saavedra et al. (2016).
O	Contardo et al. (2012) use mixedinteger programming to determine suitable inventory and routing decisions.
O	Kloimüllner et al. (2014) apply metaheuristics to find a suitable routing and inventory solution avoiding as much failed demand as possible.
P	Vogel et al. (2014) and Neumann Saavedra et al. (2016, 2015) relax the routing problem into a transport problem and solve this problem by means of matheuristics.
O	There are a few papers considering stochastic demand in a static context (Fricker and Gast, 2016; Ghosh et al., 2017; Lu, 2016; Yan et al., 2017).
O	Fricker and Gast (2016) consider a stochastic system in a steady state and calculate the percentage of critical stations where demand may not be fulfilled.
O	Lu (2016) address a problem with stochastic demands.
O	Yan et al. (2017) also use a time-space network and address the stochasticity of the problem with a threshold-based heuristic.
P	The two works on BSSs closest to the work presented in this article are presented by Brinkmann et al. (2015) and Ghosh et al. (2017).
P	Brinkmann et al. (2015) present a policy function approximation based on safety buffers.
P	Ghosh et al. (2017) reduce their problem by means of aggregation.
O	Godfrey and Powell (2002) address a stochastic and dynamic resource allocation problem where stochastic demand is revealed every period.
P	Adelman (2004) presents a problem where a set of customers needs to be served over a set of days.
O	A VFA is also applied by Toriello et al. (2010) and Papageorgiou et al. (2014) for deterministic inventory routing problems.
O	Bertazzi et al. (2013) apply a rollout algorithm (RA) to a similar stochastic dynamic inventory routing problem.
O	While RAs often draw on simulation for the lookahead, Bertazzi et al. (2013) use the solution of a mixed-integer program based on average values.
O	Coelho et al. (2014a) address a stochastic and dynamic inventory routing problems where a route through a set of customers needs to be determined every day.
O	Containers are used in maritime (Erera et al., 2009) as well as road-based transport (Shintani et al., 2007; Song and Carter, 2009) to wrap commodities.
O	In this section, we define the SDIRP (Brinkmann et al., 2015).
P	The SDIRP can be modeled as a Markov decision process (MDP, Puterman, 2014).
O	According to the Bellman equation (Bellman, 1957), in every decision state sk, π∗ returns the optimal decision π∗(sk ) ∈ Xsk .
O	Gathering this amount of information is hardly possible due to the three curses of dimensionality (Powell, 2011):
O	In the case of failed demand, the demand at a station is subject to the fill levels of neighboring stations (Rudloff and Lackner, 2014).
O	There are dynamic vehicle routing methods extending the decision space to a set of tentative route plans (Ulmer et al., 2017b).
O	VFA approximates the value function by means of offline simulations (Powell, 2011).
N	To avoid local optima, we draw on Boltzmann exploration, balancing exploitation and exploration by selecting potentially inferior horizons with a certain probability (Powell, 2011; Rothlauf, 2011).
O	We draw on real-world data offered by Minneapolis’ (Minnesota, USA) bike sharing system “Nice Ride MN” (MN, 2016).
P	We follow the preprocessing steps proposed by Vogel et al. (2011).
P	To analyze the impact of incorporating historical data by means of lookaheads, we define a conventional short-term relocation policy (STR) based on safety buffers as suggested in Coelho et al. (2014a) and Brinkmann et al. (2015).
P	This confirms the observation of Voccia et al. (2017) that long horizons may lead to inferior decision making.
O	To this end, we draw on the concept of a post-decision state rollout algorithm (for an overview of rollout algorithms, we refer to Goodson et al., 2017).
P	Rollout algorithms for inventory routing are, for example, presented by Bertazzi et al. (2013).
P	As Ulmer et al. (2017a) show, the rollout improves the base policy given that the expected value of the base policy is known or given a sufficient number of simulation runs.
O	The CNP is known to be NP-complete (Arulselvan et al., 2009) on general graphs.
O	There are a number of heuristic approaches using local search (LS) for the CNP (Aringhieri et al., 2016a; Purevsuren et al., 2017; Zhou et al., 2018; Ventresca, 2012; Zhou and Hao, 2017) in which LS plays a main role.
O	To evaluate the solution of the CNP, it is necessary to find the connected components which require O(max{|V|, |E|}) computational time (Hopcroft and Tarjan, 1973), where |V| is the number of nodes and |E| is the number of edges of the graph.
O	There are a few greedy-based heuristic approaches (Arulselvan et al., 2009; Addis et al., 2016; Pullan, 2015).
O	The approach described in Addis et al. (2016) is designed for larger networks.
O	There are a small number of population-based heuristics (Aringhieri et al., 2016b; Zhou et al., 2018).
P	The algorithm proposed in Zhou et al. (2018) is a population-based heuristic which uses local search.
P	A bi-objective version of the CNP is introduced in Ventresca et al. (2018).
P	For a more complete discussion of these approaches, the interested reader is referred to Lalou et al. (2018).
O	Then the objective function F (S) ≥ n ∗ (|V|−k n )∗(|V|−k n −1) 2 with equality holding if and only if ci = cj, ∀i, j ∈ C where ci is the size of the ith component of C. (Arulselvan et al., 2009).
N	If ci = cj, ∀i, j ∈ C1, then we obtain a better objective function value by deleting the set S1. (Arulselvan et al., 2009).
P	Note that shrinking a subgraph of a planar graph to a single node does not affect planarity (Lipton and Tarjan, 1979).
O	Moreover, the vertex cover problem is NP-Complete on planar graphs, even when there is restriction on the degree of graphs (Garey and Jonhson, 1976).
P	We use a modified version of the fundamental cycle separator (FCS) algorithm proposed in Holzer et al. (2009).
P	To determine the root of the BFS tree, the function calls the height maximization (maximizing the number of levels in the tree) method proposed in Holzer et al. (2009).
P	For more information about the fundamental cycle separator, the interested reader is referred to Holzer et al. (2009) and Lipton and Tarjan (1979).
N	Although road networks seem planar, some studies have found that they are quite non-planar (Eppstein and Goodrich, 2008; Eppstein and Gupta, 2017).
P	For further information about Delaunay triangulation and the function in Mathematica, the interested readers are referred to Devadoss and O’Rourke (2011) and Weisstein (2018).
P	Five recently proposed algorithms Greedy3d (Addis et al., 2016), Greedy4d (Addis et al., 2016), GRASP_ePR2 (Purevsuren et al., 2017), GA (Aringhieri et al., 2016b), and MACNP (Zhou et al., 2018), are chosen for the performance evaluation of the proposed algorithms.
P	According to Table 9 in Lalou et al. (2018), GRASP_ePR2 is one of the best LS-based approaches.
P	Two recently proposed algorithms, GA (Aringhieri et al., 2016b), MACC-CNP (Zhou et al., 2018), are chosen for the performance evaluation of the proposed planar_3cnp.
O	The work of Dalaijargal Purevsuren was supported by China Scholarship Council grant no. 2012496047 (2012–2017).
O	A more formal introduction of the VRP is provided by Laporte (2007).
P	Due to the complexity of this combinatorial optimization problem, usually only smaller instances of about 200 customers can be solved reliably to optimality by exact methods (Pecin et al., 2017), even though some instances with a specific structure and with up to 600 customers have been solved in Uchoa et al. (2017), given several hours of computational time.
O	The often used instances by Golden et al. (1998) contain up to 483 customers, and the largest problem in the more recent benchmark set by Uchoa et al. (2017) involves 1000 customers.
O	Waste collection is typically modeled as an arc routing problem (Toth and Vigo, 2014; Wøhlk and Laporte, 2018) when the density of customers along a street segment is sufficiently high to consider the street itself as service entity (Assad and Golden, 1995).
O	This condition is usually fulfilled in residential waste collection, whereas in commercial waste collection the customers can be more scattered and it is beneficial to consider the individual nodes (Buhrkal et al., 2012).
P	Another situation where decision-makers would prefer to model waste collection as node routing problem is described in Kyt¨ojoki et al. (2007).
P	As another example, vehicle routing can play a key role in improving the movement of goods in cities as pointed out by Cattaruzza et al. (2017).
O	The authors found, based on the French Surveys on Urban Goods Movement database (Patier and Routhier, 2009), that a large number of deliveries in cities concerns the transportation of small parcels from third-parties on longer delivery routes with 31 and more deliveries per route.
O	This approach allows to account for the familiarity of the driver with a territory (Zhong et al., 2007), and to keep routing solutions flexible (Janssens et al., 2015). 
P	A first, and so far only, step in this direction has been taken a decade ago in Kyt¨ojoki et al. (2007), who develop a variable neighborhood search algorithm that is able to solve instances with up to 20,000 customers.
O	The literature on the TSP, for example, has covered very large instances since several decades, solving instances with 85,900 nodes and more (Reinelt, 1991).
O	For instance, the famous heuristic by Lin and Kernighan (1973) only considers local search moves that connect relatively close nodes.
O	Helsgam (2000) bases the decision of whether to consider a certain edge in a TSP solution on its α-nearness, defined as the cost difference between the optimal 1-tree of all nodes and the minimum 1-tree that contains this edge.
O	Helsgaun (2000) only stores distance entries of candidate edges computed with the approach above, and all other requested distance values are computed and cached during runtime.
P	Applegate et al. (2003) minimize the representation of large TSPs by building a neighbor graph.
P	Rather than limiting the amount of stored information, it is possible to store information in a highly compressed format as proposed by Kyt¨ojoki et al. (2007).
O	In previous work, we have demonstrated that a well-implemented and well-configured local search is sufficient to compute highquality solution for VRPs with up to 1000 customers (Arnold and S¨orensen, 2019), and the question arising is whether similar ideas can be used to tackle instances of larger magnitudes.
P	Local search has been shown to be one of the few approaches that can successfully tackle a wide range of combinatorial optimization problems (Johnson et al., 1988).
P	It has been particularly successful to solve the TSP (Lin and Kernighan, 1973) and is a major ingredient of many successful heuristics for the VRP (Mester and Br¨aysy, 2007; Subramanian et al., 2013; Vidal et al., 2012).
N	In Arnold and S¨orensen (2019), we investigated this trade-off, and found that larger neighborhoods can yield a good performance if the underlying local search operators are complementary, well-implemented, and effectively pruned.
P	At first, an initial solution is constructed with the heuristic by Clarke and Wright (1964) (CW), and the individual routes are then improved with the heuristic by Lin and Kernighan (1973) (LK).
O	For the optimization between two routes the CROSS-exchange operator (CE) (Taillard et al., 1997) is used, and for the optimization of more than two routes a relocation chain (RC) is proposed, which is based on the concept of the ejection chain introduced in Glover (1996).
O	Rather than generating and evaluating all possible moves, we determine promising moves with the concepts sequential search (Irnich et al., 2006) and pruning (Toth and Vigo, 2003).
O	A common idea is to only consider those moves that involve short edges (Toth and Vigo, 2003), and we applied this type of pruning to all three operators.
O	Additionally, we found in a data-mining study that the penalization of wide edges can result in performance gains (Arnold and S¨orensen, 2018).
O	In Figure 3 we plot the computational times of two state-of-the-art heuristics, the hybrid genetic heuristic (HGSADC) by Vidal et al. (2012) and the iterated local search (ILS) by Subramanian et al. (2013) against problem size, as reported in Uchoa et al. (2017). 
P	We perform all experiments on the eight instances introduced by Kyt¨ojoki et al. (2007).
P	Solutions for the very large-scale instances introduced in Kyt¨ojoki et al. (2007).
P	The performance of a certain configuration is expressed as the average gap between the best solutions found within a specific time and the best known results, as reported by Kyt¨ojoki et al. (2007).
N	According to PassMark Software (2018) this setup is about 4 times faster than the setup used in Kyt¨ojoki et al. (2007) (AMD Athlon64 3000+).
O	Results of different construction heuristics on the very large-scale instances by Kyt¨ojoki et al. (2007) (N denotes the number of nodes).
P	LK has an excellent scaling performance, and is used as the basis to tackle largescale TSPs of tens of thousands of customers (see, for instance, Applegate et al. (2003); Helsgaun (2000)).
P	This complexity can be tackled by imposing a limit on the length of considered substrings, as suggested by Taillard et al. (1997).
O	Additionally, we also investigate a setup marked as 100∗ , in which the 100 shortest distances are stored prior to the start of the heuristic, while all other requested distance values are computed during runtime as in Helsgaun (2000).
O	We test KGLSXXL on the R-instances and the W-instances for differing running times, and compare the performance to the results of the guided variable neighborhood search (GVNS) introduced by Kyt¨ojoki et al. (2007).
O	We conduct this analysis for the smaller instances X-n139-k10 and X-n561-k42 of Uchoa et al. (2017) as well as for the larger instances R3 and R9 of Kyt¨ojoki et al. (2007).
O	We further obtained results from Helsgaun (2017) (LKH-3).
O	This heuristic is an extension of a heuristic for the TSP (Helsgaun, 2000) and transforms a VRP into a standard TSP, while checking for constraint violations after each λ-opt move.
O	The Electric Vehicle Routing Problem with Time Windows (EVRPTW) was introduced by Schneider et al. (2014) as an extension to the Green Vehicle Routing Problem of Erdo˘gan and Miller-Hooks (2012).
O	Many companies now use EV fleets, and their number is steadily increasing (Coplon-Newfield and Park, 2017).
O	Several global companies are in the process of testing or implementing this technology, such as FedEx, UPS, Frito-Lay, AT&T, General Electric, and Coca-Cola (Suiza, 2013).
P	Furthermore, some other global companies, such as Unilever, IKEA and DHL have launched a global campaign to accelerate the shift to EVs from gas and diesel powered transportation (Fairley, 2017).
O	Furthermore, the charging time is a non-linear concave function of the charge amount (Pelletier et al., 2017, Montoya et al., 2017).
O	The EVRP was first studied by Conrad and Figliozzi (2011) assuming a constant recharging time.
O	Erdo˘gan and Miller-Hooks (2012) then studied the routing of alternative fuel vehicles (AFVs) which are refueled up to the tank capacity at alternative fuel stations within a constant amount of time.
P	Schneider et al. (2014) introduced time windows and a linear charging function assuming that the battery is fully recharged.
P	The full charge assumption was first relaxed and models were developed to allow partial charging (Bruglieri et al., 2015; Keskin and C¸ atay, 2016).
O	Some models take into account the fact that the stations have different chargers (Felipe et al., 2014; Sassi et al., 2014; Li-ying and Yuan-bin, 2015; Keskin and C¸ atay, 2018). 
O	Some of them consider a mixed fleet composed of EVs with different characteristics (Desaulniers et al., 2016) whereas in other studies, the fleet includes different ICEVs as well (Sassi et al., 2014; Goeke and Schneider, 2015; Hiermann et al., 2018; Kopfer and Vornhusen, 2018).
O	Some recent papers have dealt with both the location of the stations and the routing (Li-ying and Yuan-bin, 2015; Yang and Sun, 2015; Paz et al., 2018; Schiffer and Walther, 2017). 
P	Although recharging is performed at charging stations in most studies, some papers consider battery swap stations (BSS) where the discharged battery is replaced with a fully recharged one (Liao et al., 2016; Masmoudi et al., 2018; Paz et al., 2018; Wang et al., 2018; Jie et al., 2019), and wireless charging systems (WCS) where the battery is recharged by an inductive charging system placed along the roads while the EV is traveling (Li et al., 2018).
O	Some recent studies have considered EVs within the context of the two-echelon VRP (Jie et al., 2019; Breunig et al., 2018), technician routing (Villegas et al., 2018), reverse logistics (Zhang et al., 2018c), and pick-up and delivery problem (Grandinetti et al., 2016; Madankumar and Rajendran, 2018).
O	Finally, the linear charging function was relaxed by Montoya et al. (2017) and Froger et al. (2017, 2019) who used a concave piecewise linear function.
P	Afroditi et al. (2014) and Pelletier et al. (2016) reviewed EVs in goods distribution.
N	Regarding the unknown service times at the stations, in Sweda et al. (2017) the stations are unavailable with a certain probability.
P	Froger et al. (2017) solved a related problem in which the stations have limited number of chargers (one, two or three) and an EV may need to wait for service if the chargers are busy with recharging other EVs in the fleet.
O	Recently, Bruglieri et al. (2018) studied the waiting times at the alternative fuel stations. 
P	Kullman et al. (2018) introduced EVRP with public-private recharging strategy where the company owns a charger in its depot but the EVs may recharge at public stations as well.
O	Static models include deterministic (Malandraki and Daskin, 1992; Jung and Haghani, 2001; Hashimoto et al., 2008; Jabali et al., 2012; Franceschetti et al., 2017) or stochastic features (Van Woensel et al., 2008; Nahum and Hadas, 2009; Ta¸s et al., 2013; Huang et al., 2017; C¸ imen and Soysal, 2017) whereas Fleischmann et al. (2004), Haghani and Jung (2005), Potvin et al. (2006) and Schilde et al. (2014) deal with dynamic models.
O	Malandraki and Daskin (1992) represented the time-varying travel times during the day by assigning to each arc a travel time function which is a step function of the departure time.
P	Their model was later improved by Ichoua et al. (2003) and Fleischmann et al. (2004) who ensured that a vehicle entering an arc later than another vehicle cannot leave the same arc earlier, which is known as the non-passing or first-in-first-out (FIFO) property.
O	Ichoua et al. (2003) assumed travel speeds that change when the boundary between two consecutive time periods is crossed.
P	Fleischmann et al. (2004) introduced smooth travel time functions.
O	Time windows were considered by Jung and Haghani (2001), Soler et al. (2009), Dabia et al. (2012), Harwood et al. (2013), Wen and Eglese (2015), Kumar et al. (2016), and Spliet et al. (2017) while Ta¸s et al. (2014) addressed the case with stochastic and time-dependent travel times assuming soft time windows.
O	The other VRP variants that deal with time-dependent travel times include pick-up and delivery problems (Zhang et al., 2014; Sun et al., 2018), the orienteering problem (Verbeeck et al., 2014; Black et al., 2015; Verbeeck et al., 2016; Sun et al., 2018), the dial-a-ride problem (Schilde et al., 2014), the inventory routing problem (Cho et al., 2014), green logistics problems (Jabali et al., 2012; Soysal et al., 2015; Norouzi et al., 2017; Franceschetti et al., 2013, 2017; C¸ imen and Soysal 2017; Heni et al., 2018, Zhang et al., 2018a; Rabbani et al., 2018), and city logistics problems (Mancini 2014; Rincon-Garcia et al., 2018).
O	Tas et al (2016) considered the time-dependent service times within the context of the traveling salesman problem.
P	A comprehensive review of the time-dependent VRP is provided by Gendreau et al. (2015).
O	The energy transferred is assumed to be a concave function of the charging time (Montoya et al., 2017) and the cost of charging is proportional to the amount of energy transferred. 
O	In this study, we assume that the EVs operate between 10% and 90% of the battery SoC to minimize the degradation (Pelletier et al., 2017).
O	The charging function is known to be concave (Pelletier et al. 2017).
O	Hence, we adopt a nonlinear charging function and approximate it by a piecewise linear function as in Montoya et al. (2017). 
O	Unlike what is done in Montoya et al. (2017), we assume that all stations are equipped with the same charger type; hence the vehicles are always subject to the same charging function.
N	Matheuristics have been used in vehicle routing problems successfully (Archetti and Speranza, 2014).
P	The ALNS metaheuristic, proposed by Ropke and Pisinger (2006a,b), is a search framework based on iterative destroy and repair phases, that has been successfully employed for solving various VRP variants (Aksen et al., 2014; Goeke and Schneider, 2015; Ko¸c, 2016).
P	We use the random, worst-distance, worst-time, Shaw, proximity-based, demand-based, time-based, zone, random route and greedy route removals which have been used in related studies (Demir et al., 2012, Eme¸c et al., 2016, Keskin and C¸ atay, 2016).
P	We employ greedy, regret-2 and time-based insertion operators, as proposed in Keskin and C¸ atay (2016).
P	We use the greedy station insertion presented in Keskin and C¸ atay (2016).
P	The Fixed-Route Vehicle-Refueling Problem was introduced by Suzuki (2014) and later studied by Bruglieri et al. (2016), Montoya et al. (2017), Ko¸c et al. (2018), and Keskin and C¸ atay (2018) to enhance the routing decisions.
P	We used the piecewise linear charging function for fast chargers proposed in Montoya et al. (2017).
O	We apply a scaling such that the recharging rate during the last piece is equal to the rate applied in Schneider et al. (2014) data.
O	Furthermore, we assumed a battery whose SoC interval between 10% and 90% corresponds to the full capacity value used in Schneider et al. (2014).
P	We adopt the objective function coefficients of Ta¸s et al. (2013).
O	In Feng and Figliozzi (2013), a conventional vehicle and an EV are compared for their fuel consumptions and purchase prices.
O	The parameters of the discrete uniform distributions from which the number of customers to be removed is drawn are set as nc = min{0.1|N|, 30} and ¯nc = min{0.4|N|, 60}, similar to Eme¸c et al. (2016).
N	Desaulniers et al. (2016) highlighted the minor influence of wide time-window constraints on recharging decisions
O	Hence we decided to use instances with narrow time windows in our experimental study, and we randomly selected four instances from each data set of C1, R1, and RC1 of the Schneider et al. (2014) instances.
O	A wide range of economic organizations manage cash for operational, precautionary and speculative purposes (Keynes, 1936).
O	Since the seminal works by Baumol (1952) and Miller and Orr (1966), cash management models follow a inventory control approach.
P	The bound-based control approach is based on the strong assumption of a particular probability distribution for cash flows, which is usually assumed to be a normal, independent and stationary cash flow as in Miller and Orr (1966), Baccarin (2009), Premachandra (2004).
O	Surprisingly, the use of empirical data sets in cash management research is limited to recent contributions such as Gormley and Meade (2007) and Salas-Molina et al. (2017), in which alternative forecasters are used to obtain predictions as a key input to cash management models, and Salas-Molina et al. (2016), in which a multiobjective approach to the cash management problem is proposed.
O	Instead of fitting forecasters as in Gormley and Meade (2007) and Salas-Molina et al. (2017), we here fit cash management models.
P	Furthermore Gormley and Meade (2007) proposed a bound-based model using forecasts as a key input and they used genetic algorithms to find sufficiently good bounds.
P	On the other hand, Salas-Molina et al. (2016) used simulation techniques and the Miller and Orr (1966) model within a multiobjective framework considering not only the cost but also the risk of alternative policies.
P	A paradigmatic case is deep learning because it requires very little engineering and it can take advantage of computational power and data availability (LeCun et al., 2015, Schmidhuber, 2015).
P	On the other hand, random forests are also an interesting technique based on an ensemble of slightly different decision trees (Ho, 1998, Breiman, 2001). 
O	Relevant related works are those based on stochastic programming (SP) to address different cash management problems as in Golub et al. (1995), Gardin et al. (1995), Gondzio and Kouwenberg (2001) and Castro (2009).
O	In these works, the authors rely on SP and a set of previous realizations of random variables to improve current techniques to manage cash (e.g. in automatic teller machines in the case of Castro (2009)).
N	A further advantage of our proposal is that the solution provided is a cash management model of the Miller and Orr (1966) type.
N	This fact avoids the drawback of solving a new (possibly large) problem at each time step when information about a new initial condition is available according to the so-called receding horizon philosophy (Bemporad and Morari, 1999, Camacho and Bordons, 2007).
O	In addition, our approach can be extended to fit other cash management models such as the one proposed by Stone (1972) or by Gormley and Meade (2007).
O	The policy to deploy is elicited by averaging the output of randomly trained models similarly to the methods used in machine learning to train random forests (Ho, 1998, Breiman, 2001).
P	As a benchmark, we use the equations proposed by the Miller and Orr (1966) model to obtain a set of three bounds
P	The Miller and Orr model was the first stochastic cash management model and it has become a framework for subsequent research in cash management (some recent examples are Premachandra (2004), da Costa Moraes and Nagano (2014)).
P	While other models propose a higher number of bounds (see e.g. Eppen and Fama (1969), Stone (1972)), the Miller and Orr model is based on only three control bounds allowing us to limit the number of decision variables and constraints for illustrative purposes.
P	Miller and Orr (1966) proposed a model to control balances for the twoassets system in Figure 1 by assuming that stochastic cash flows ft are generated by a stationary random walk with standard deviation σ.
O	Following the recommendations in Gormley and Meade (2007), we use an indicator function Iq that takes value one when condition q holds, zero otherwise, to rewrite objective function in equation (1) as follows:
P	Miller and Orr (1966) proposed a bound-based model by showing that the optimal policy (when cash flows follow a stationary random walk with standard deviation σ) is obtained by defining three control bounds L, Z and H.
O	Although Miller and Orr set lower limit L to zero in their work, a real cash manager should set a lower limit above zero for precautionary motives as recommended in Ross et al. (2002).
O	After setting lower limit L for precautionary purposes and by minimizing objective function (2) with respect to policy x, Miller and Orr (1966) showed that the optimal policy is given by equation (4) with parameters Z and H set as follows:
N	In what follows, we consider a more general approach than Miller and Orr (1966) with respect to cost functions as described in recent cash management works (see e.g. Gormley and Meade (2007), Salas-Molina et al. (2016)).
O	Within a general formulation of a stochastic problem (Birge and Louveaux, 2011), we have to make decisions under some degree of uncertainty.
O	Then, we need to formulate a multistage stochastic problem with τ time steps (Castro, 2009).
P	Following the recommendations in Ross et al. (2002) about the Miller and Orr model, we also set a minimum cash balance for precautionary purposes.
O	Selecting random subspaces of the input data space has been fruitfully used to construct decision tree models (Ho, 1998, Breiman, 2001).
O	Furthermore, ensemble methods are learning algorithms that construct a set of models and then predict by taking a possibly weighted average of their predictions (Dietterich, 2000).
O	Taking a uniform sample with replacement as in bagging (Breiman, 1996), or weighting the observations to produce a biased selection similarly to boosting (Freund and Schapire, 1996), are suitable procedures.
O	The goodness of fit is usually evaluated by the predictive accuracy that refers to how well the model is able to reproduce the data used to fit the model (Makridakis et al., 2008).
O	By comparing performances of models to trivial strategies, we are implicitly checking if cash management models are worthwhile along the lines of Daellenbach (1974). 
O	Unlike training and test set splitting, cross-validation estimates the generalization power of a model by performing multiple splits (Hastie et al., 2009, Provost and Fawcett, 2013).
N	In this section, we describe a case study based on a real cash flow data set from an industrial company in Spain that has been recently used in Salas-Molina et al. (2017).
O	All the experiments in this case study are performed on Jupyter Notebooks executed on a CPU Intel Core Duo E8400 at 3 GHz with 4 GB of RAM under operating system Windows 10 Professional 64 bits. Mathematical programs are solved through the Python interface of Gurobi optimization software (Gurobi Optimization, Inc, 2017).
P	According to the recommendations in Ross et al. (2002), we set a lower bound L for precautionary purposes.
O	A suitable way to do it is setting a value proportional to the empirical standard deviation of cash flows along the lines of Ben-Tal and Nemirovski (1999), Ben-Tal et al. (2009) for robust optimization.
O	Let us consider as abnormal cash flows those with absolute value above five standard deviations as recommended in Gormley and Meade (2007).
O	The application of BPMs can be found in the areas such as semiconductor manufacturing (Lee et al., 1992), steel casting industries (Mathirajan and Sivakumar, 2006; Mathirajan et al., 2007), shoe factory (Fanti et al., 1996), parallel computing (Al-Salamah, 2015), batch delivery (Joo and Kim, 2017) and multi-layer ceramic capacitor production (Koh et al., 2004). 
O	The typical application of BPMs is in the final testing phase of semiconductor manufacturing, which is known as burn-in model (Uzsoy, 1994).
O	For example, jobs with higher priority should be processed before jobs with lower priority (Azizoglu and Webster, 2001) and the quality requirement levels are different for military orders and civilian orders. 
O	Thus, due to the logistical, management and technical constraints, jobs with different requirements cannot be processed in the same batch (Li and Chen, 2014b) and these jobs are called from incompatible job families.
O	In order to decrease the manufacturing cost and to improve customer satisfaction, mass customization is an effectively way of producing goods and services to match individual customer’s requirements with near mass production efficiency (Tseng and Jiao, 2007).
N	As the problemis strongly NP-hard even for the problem with identical job size 1|batch(b)|Lmax (Pinedo, 2008), we mainly focus on heuristics and improved heuristics to solve the problem under study in a reasonable CPU time.
P	The problem of scheduling batch processing machine derived from burn-in operation in large scale integrated circuits was first introduced by Lee et al. (Lee et al., 1992) in which jobs are of the same size and non-identical processing time.
O	Then problems considering non-identical job sizes were studied by Uzsoy (Uzsoy, 1994) for the objective of minimizing makespan and total completion time.
O	The problem was proven to be NP-hard, thus various effective heuristics including FFLPT (First Fit Longest Processing Time) (Uzsoy, 1994) and BFLPT (Best-Fit Longest Processing Time) (Dupont and Ghazvini, 1998) were commonly used to solve the BPM scheduling problem.
O	Different meta-heuristics (Jiang et al., 2017; Zhou et al., 2016) have been introduced to solve the problem as well and the problem has also be extended to multi-machine environment (Zhou et al., 2017) and different objectives (Parsa et al., 2017; Li et al., 2017).
O	Batch processing machine scheduling with jobs from incompatible job families has been encountered in various production environments, such as heat treatment facilities in steel casting industries (Mathirajan and Sivakumar, 2006; Mathirajan et al., 2007), "bake out" operation in the multi-layer ceramic capacitor (MLCC) manufacturing process (Koh et al., 2004; Koh et al., 2005), etc.
O	Motivated by the operation of diffusion in wafer fabrication, the problems of scheduling BPM with jobs from incompatible job families were studied by Uzsoy (Uzsoy, 1995) for the objectives of minimizing Cmax, Lmax and Ci.
O	The objective of maximum lateness on parallel batch machines with incompatible job families was considered by Malve and Uzsoy (Malve and Uzsoy, 2007).
O	The problem of minimizing the weighted and unweighted number of tardy jobs on a single batch processing machine was studied by Dauzere-Peres and Moench (Dauzere-Peres and Moench, 2013).
O	Jia et al. (Jia et al., 2016) studied the problem of minimizing makespan in parallel batch machines.
O	The problem of considering regular sum objective for batch machine scheduling problems with incompatible job families was studied by Mönch et al. (Moench and Roob, 2018).
O	Due to the significant economic impact of burn-in operation scheduling, Azizoglu and Webster (Azizoglu and Webster, 2001) described a branch and bound procedure to minimize the total weighted completion time on a batch processing machine and jobs in a given family had identical job processing time and arbitrary job sizes.
O	For the burn-in mode with jobs having non-identical job processing time, the problem of scheduling families jobs to minimize the makespan was studied by Nong (Nong et al., 2008).
O	Motivated by the burn-in operation, the problem of minimizing makespan, total rejection penalty (Li and Chen, 2014a) and weighted number of tardy jobs (Li and Chen, 2014b) were studied by Li et al.
O	Cheng et al. (Cheng et al., 2014) considered the scheduling problem of minimizing makespan and total batch completion time on a batching machine with non-identical jobs from multiple incompatible families and batch processing time was determined by the job with the longest processing time in the batch.
O	The machine is a parallel-batching (i.e., p-batch) machine (Lee et al., 1992) which can process a set of jobs simultaneously as a batch.
O	Using the standard three-field α/β/γ notation (Graham et al., 1979), the problem under study can be denoted as 1/sj,pj,fij,dj,p-batch/Lmax.
O	The lower bound of batch number in scheduling BPM can be generated by splitting each job into unit size jobs (Koh et al., 2005).
O	A tighter lower bound of batch number can be generated by placing large jobs in a single batch whose residual capacity cannot accommodate the smallest job of the job family (Li et al., 2013).
O	The upper bound of batch number v can be obtained by using algorithm NFEDD (Next Fit Earliest Due Date) based on Next Fit algorithm (Johnson, 1973).
P	The well-known EDD rule can then be applied to optimize stage 2 (Ghosh and Gupta, 1997).
O	As illustrated in algorithm LB, batches in LB(I) have been also arranged in EDD order, which is optimal for the problem of 1|| Lmax (Pinedo, 2008).
O	Considering the good performance of EDD rule (Uzsoy, 1995) for the objective of Lmax.
P	The Capacitated Vehicle Routing Problem (CVRP) is classically described as a combination of a Traveling Salesman Problem (TSP) with an additional capacity constraint which lends a Bin Packing (BP) substructure to the problem (Toth and Vigo 2014).
O	It can be seen as a Set Partitioning (SP) problem in which the cost of each set corresponds to the distance of the associated optimal TSP tour (Balinski and Quandt 1964).
P	These problem representations emphasize the two decision sets at play: customer-to-vehicle Assignments, and Sequencing choices for each route (Vidal et al. 2013b), a duality that has left long-standing impressions in the literature, from the early developments of route-first cluster-second (Bodin and Berman 1979, Beasley 1983) and cluster-first route-second constructive methods (Fisher and Jaikumar 1981), all the way to the set-covering-based exact methods and matheuristics which are currently gaining popularity.
O	These partial solutions can be viewed as a projection (Geoffrion 1970) of the original solutions S on the space SA defined by a single decision subset (Assignment).
O	Rather than requiring a complete exact solution of each TSP, the dynamic programming approach of Balas and Simonetti (2001), hereafter referred to as B&S, can be employed to perform a restricted route optimization during move evaluations.
O	To evaluate experimentally the potential of the new search spaces, we conduct experiments with a simple local search (LS), and with the unified hybrid genetic search (UHGS) of Vidal et al. (2012, 2014).
O	The exploration of the search spaces S B k for k ∈ {1, . . . , 3} appears to lead to solutions of higher quality on the new instances from Uchoa et al. (2017).
P	Route-first cluster-second algorithms (Bodin and Berman 1979, Beasley 1983) first produce a giant TSP tour, before subsequently assigning consecutive visits into separate trips to produce a complete solution.
O	In cluster-first route-second methods (Fisher and Jaikumar 1981), a clustering algorithm is employed to group customer visits into clusters, followed by TSP optimizations.
O	Finally, petal algorithms (Foster and Ryan 1976, Renaud et al. 1996) are based on an a-priori generation of candidate routes (petals), followed by the solution of a set partitioning problem.
O	Petal algorithms have been naturally extended into hybrid metaheuristics: high-quality routes are extracted from local minima instead of being enumerated in advance, and the solution of the associated set partitioning problems fulfills the role of a large neighborhood search (see, e.g., Ahuja et al. 2002, Muter et al. 2010, Subramanian et al. 2013).
P	Vidal et al. (2013b) established a review of the classical variants and their associated constraints, objectives, and decision sets, called attributes.
O	The attributes were classified in relation to their impact on Sequencing, Assignment decisions, and Route evaluations in heuristics, leading to a structural problem decomposition which serves as a basis for the Unified Hybrid Genetic Search (UHGS) algorithm of Vidal et al. (2014) and allows to produce state-of-the-art results for dozens of VRP variants.
O	Many problem attributes come jointly with new decision subsets, e.g., when optimizing vehicle routing with packing, timing or scheduling constraints (Goel and Vidal 2014, Pollaris et al. 2015, Vidal et al. 2015b), visit choices (Vidal et al. 2016, Bulh˜oes et al. 2018) or service-mode choices (Vidal et al. 2015a, Vidal 2017).
O	Exponential-size neighborhoods have a long history in the combinatorial optimization literature (Deineko and Woeginger 2000, Ahuja et al. 2002, Bompadre 2012).
P	Gutin and Yeo (2003) proved that, for the TSP, no neighborhood of cardinality at least (n − k)! for a given constant k can be searched unless NP ⊂ P/poly.
O	The neighborhood of Balas and Simonetti (2001) is an exponential-size neighborhood for the TSP.
P	Balas and Simonetti (2001) performed extensive experiments, and demonstrated that this dynamic programming procedure can be used as a stand-alone neighborhood to improve high quality local minima of the TSP and its immediate variants.
O	Gschwind and Drexl (2016), and Hintsch and Irnich (2017) employed this neighborhood to solve arc-routing problems with possible cluster constraints, and dial-a-ride problems.
O	Only in one conference presentation (Irnich 2013), the possibility of using the B&S neighborhood in combination with some classical CVRP moves has been highlighted, but the performance of such an approach remains largely unexplored.
O	This algorithm can be readily extended into a wide range of vehicle routing metaheuristics, e.g., tabu search, iterated local search, or hybrid genetic algorithm (Gendreau and Potvin 2010, Laporte et al. 2014).
O	In light of these observations, we have conducted computational experiments on search space S B k , using the dynamic programming algorithm of Balas and Simonetti (2001) to decode each solution, as well as on search space S A using the TSP solver Concorde (Applegate et al. 2006).
O	As detailed in Vidal et al. (2013a), and in a similar way as Johnson and McGeoch (1997) and Toth and Vigo (2003), the search can be restricted to a subset of these moves that reconnect at least one vertex i with a vertex j belonging to the Γ closest vertices of i.
P	We use the concatenation strategy of Vidal et al. (2014, 2015b) to perform efficient cost- and load-feasibility evaluations.
O	As in Vidal et al. (2014), Equations (3–6) are first employed iteratively, in lexicographic order, to obtain information concerning all sequences during the preprocessing phase.
O	The function Hp is a multiplicative hash which depends on the visit permutation (Knuth 1973).
O	In the second case, ρ is set to 31 (multiplier of Kernighan and Ritchie 1988).
O	Guidance techniques are a set of strategies which analyze and exploit the search history to direct the search towards promising or unexplored regions of the search space (Crainic and Toulouse 2008). 
O	For these tests, we considered the simple local search (LS) described in Section 3, as well as a more advanced metaheuristic, the UHGS of Vidal et al. (2012, 2014).
P	To solve the TSP problems when considering the space S A, we used the Concorde solver (Applegate et al. 2006).
O	To conduct the experiments with the UHGS-BS, we used the code base made available at https://github.com/vidalthi/HGS-CARP, from Vidal (2017).
O	As an initial solution, we used the result of the savings algorithm of Clarke and Wright (1964).
O	We considered the 100 recent benchmark instances of Uchoa et al. (2017), as these instances remain highly challenging for metaheuristics and cover a larger variety of instance size and characteristics: demand and customer distribution, depot location, and route length.
P	The leftmost graph represents the percentage gap in terms of solution quality, relative to that of the best known solution (BKS) collected from Uchoa et al. (2017): Gap = 100 × (z − zbks)/zbks, where z is the solution value of the method and zbks is the BKS value. 
P	Finally, this section reports detailed results of UHGS-BS, using the baseline configuration and the tunneling strategy, on the complete set of 100 instances proposed by Uchoa et al. (2017).
O	The results of UHGS-BS are compared to that of the current state-of-the-art algorithms: the hybrid Iterated Local Search (ILS) proposed by Subramanian et al. (2013), and the original UHGS of Vidal et al. (2012, 2014), which were executed 50 times for each instance.
N	Based on the single-thread Passmark benchmark (PassMark software 2018), our CPU is approximately 33% faster than the Intel i7-3960X 3.30 GHz used by Uchoa et al. (2017). 
O	This effort could be mitigated if good and fast lower bounds were proposed for the cost of the routes, therefore permitting to filter a large proportion of moves as in Vidal (2017). 
O	To cope with these complex disaster situations, various types of studies have been carried out in the field of industrial engineering, operations research, management science, and computer science, among others (Altay and Green 2006; Caunhye et al. 2012).
O	Especially, to reduce further damage in the post disaster period, the research in the following areas has been conducted: evacuation planning, relief distribution, casualty transportation, and facility location problems (Hamacher and Tjandra 2001; Osman and Ram 2017; Sheu 2007; Rath and Gutjahr 2014; Yi and Kumar 2007; Talarico et al., 2015; Balcik and Beamon 2008; Rancourt et al., 2015).
O	However, Holguin-Veras et al. (2012) pointed to insufficient research on responses after disasters.
O	Galindo and Batta (2013) also explained that research efforts dealing with recovery after disasters have not received sufficient attention compared to research focusing on the distribution of relief goods.
O	Especially in rural areas, where road networks are sparse and supply chains are limited, the road-network repair should be implemented immediately after man-made or natural disasters because the road destruction can contribute to a high possibility that people will be completely isolated (Kim et al., 2018).
P	In a previous study on the repair of a destroyed network, Aksu and Ozdamar (2014) developed a mathematical model to maximize network accessibility for the evacuation of survivors and the removal of roadside debris.
O	Yan and Shih (2012) developed an integer network flow model with side constraints, to deal with the roadway repair problem, and an ant colony system based hybrid algorithm, to solve the large-scale data efficiently.
P	Duque et al. (2013) proposed a problem for improving path accessibility by developing a knapsack problem based heuristic and a Variable Neighborhood Search (VNS).
P	Avci and Avci (2017) developed the Greedy Randomized Adaptive Search Procedure (GRASP) to solve efficiently a traveling repairman problem with profits.
P	Kim et al. (2017) proposed a repair crew scheduling problem by considering time-varying aspects of the disaster.
O	Karlaftis et al. (2007) pointed out that the importance of resource allocation is undervalued in the existing disaster recovery research and suggested a resource allocation method for road destruction recovery in urban areas.
O	Duque and Sörensen (2011) considered resource allocation issues for repair of disaster-affected rural road networks.
P	Chou et al. (2014) developed Biological-based Genetic Algorithms (BGA) dealing with resource allocation problems in disaster response
P	The BGA developed by Chou et al. (2014) outperformed other solution procedures in cases of the allocation problem.
P	Zhou et al. (2017) developed the emergency resource scheduling problem for a case of multiple periods in which unsatisfied demand and risk for choosing the destroyed road were minimized and optimal roads to rescue were efficiently selected.
O	To minimize additional damages, it is critical to consider not only the repair operations in inaccessible areas but also the scheduling of relief goods delivery to the areas in need (Yan and Shih 2009).
O	The mathematical model considering the repair crew problem developed in this research was influenced by the working paper of Duque et al. (2014).
O	Duque et al. (2016) claimed that the proposed mathematical model is intractable even for the small-scale data and did not include the model in their paper; rather, instead of presenting a mathematical model, they proposed Dynamic Programming for small- and medium-scale data and the Iterated Greedy-Randomized Constructive Procedure for the largescale data. For the research presented herein, the model proposed by Duque et al. (2016) is extended with the added concept of the transportation of relief goods.
O	In a previous study from Duque et al. (2016), the objective function was expressed as the product of the weights, each denoted as wi, for each demand node and the times that the demand nodes become accessible.
O	To verify the solution of the MILP, a numerical experiment solving the small-scale data was conducted. Fig. 3 shows the network data for the numerical experiment (Kim et al., 2017).
O	In the first stage, according to Duque et al. (2016), the repair sequence by the repair crew for the destroyed nodes is determined. 
P	The ACO algorithm is one of the meta-heuristic algorithms that is based on a process of the natural world. It was first proposed by Dorigo and Gambardella (1997). 
O	According to Dorigo and Gambardella (1997a,b) and Dorigo and Di Caro (1999), the ACO algorithm outperformed other metaheuristics for TSP cases.
P	In addition, according to many survey papers in the literature, such as those from Mohan and Baskaran (2012) and Neto and Godinho Filho (2013), the ACO algorithm has shown good results in solving a TSP and related variants.
P	Donati et al. (2008) developed the ACO algorithm to solve the time-dependent VRP.
O	Gajpal and Abad (2009) also studied the VRP with pickup and delivery by the ACO algorithm.
O	Bontoux and Feillet (2008) applied the ACO algorithm to the generalized TSP, namely traveling purchaser problem.
O	Balseiro et al. (2011) combined the ACO algorithm with an insertion heuristic to avoid infeasible solutions at the final stage of the algorithm.
O	The procedure for determining the pheromone is similar to that described by Dorigo and Gambardella (1997).
P	In terms of an extension, the solution approach that Duque et al. (2016) developed only took into account the accessible time of the demand area.
O	For example, k-median (or min sum) and k-center (or min max) as two prominent FL problems have been applied to public facilities and emergency facilities, respectively (Daskin, 2001; Farahani et al., 2010; Davoodi et al., 2011).
O	Thus, to manage the traffic in the network, the antennas need to have similar network load (Bortnikov et al., 2012; Kleinberg et al., 1999).
P	This case may happen in locating fire stations, hospitals, banks, stores, educational, cultural and sport centers, and it is very important in Territory Design (Kalcsics et al., 2005).
N	However, there are some assignment studies like balanced k-center and Tolerant k-center problems (Barilan et al., 1993; Fernandes et al., 2018) in which the assignment rules do not follow the mentioned closest center rule.
O	For a given set of demand points (clients) and a set of potential facility points, in a k-center problem or min max objective FL, the goal is to open k facility centers such that the maximum distance between each client and its closest opened center is minimized (Daskin, 2001; Farahani et al., 2010).
O	This problem is also called k-supplier problem, particularly when the locations of clients and centers are distinct (Nagarajan et al., 2013).
P	When k is a part of the input, the NP-completeness of the k-center problem has been proved (Megiddo and Supowit, 1984; Fowler et al., 1981), and for a constant value k, efficient algorithms have been proposed, e.g., a linear time algorithm for the 1-center problem in the continuous space has been proposed by Megiddo (Megiddo, 1983).
P	For the 2-center problem, Ben-Moshe et al. (2005) proposed an O(nlog 2n) time algorithm, where n is the number of clients.
P	Also, Hwang et al.(1983) proposed an exact O(n √ k ) time algorithm by using a dividing approach.
P	Further, many approximation and heuristic algorithms have been proposed for the k-center problem, e.g., see (Drezner, 1984; Daskin, 2000; Vazirani, 2013).
O	In addition to these studies, there have been multi-objective facility location problems containing a min max objective (Bello et al., 2011; Roostapour et al., 2016).
P	Marín (2011) for the first time proposed the k-balanced problem in 2011.
P	Filipovic´ et al. (2012) proposed a combined heuristic method of a genetic algorithm with an interchange heuristic for the balanced allocation problem.
P	The method was a variable neighborhood search heuristic which utilizes a shaking neighborhood technique in order to avoid getting stuck in local optima which later was improved by Kratica et al. (2012).
P	For example, Miškovic´ and Zorica (2015) and Barbati and Piccolo (2016); Barbati (2013); Barbati et al. (2015) focused on equality measures (such as distance and arrival time) in location problems and proposed different integer programs, along with memetic and heuristic algorithms.
O	Zhou et al. (2002) used a genetic algorithm to provide a balance of transportation cost and customer service.
O	To the best of our knowledge, there is no complexity result on the mentioned balance objectives, while NP-completeness of min max objective (F1) was proved (Megiddo and Supowit, 1984; Fowler et al., 1981).
O	It is notable that an approach for solving k-BCL, particularly k − BCL12, is the ε − constraint approach (Miettinen, 1999).
O	By applying ε − constraint approach to solve k − BCL12 problem and transforming F2(C) into the constraint, a known problem, called capacitated k-center problem (Khuller and Sussmann, 2000; Özsoy and Pınar, 2006), is obtained. In fact, F2(C) which is minimizing the maximum number of clients assigned to each center, can be seen as the maximum capacity of each center, however, each center may have its own capacity.
O	To this end, we use the fact that problems of minimum vertex cover and minimum dominating set on cubic planar graphs are NP-complete (Alimonti and Kann, 2000; Masuyama et al., 1981) (see also the appendix).
O	Our algorithm is a multi-objective hill-climbing (Coello et al., 2002) approach which is use the graph of the Delaunay Triangulation (de Berg et al., 2008) for generating the neighbor(s) of a solution in each iteration.
O	VD, can be computed in O(klog k) time by Fortune’s algorithm (Deb, 2001).
O	Also, DT can be computed by directly checking the empty circle property (Karasakal and Silav, 2016) between any three sites.
O	Because of the local change between C and C the objective values F1(C), F2(C) and F3(C ) can be efficiently updated by using the objective values F1(C), F2(C) and F3(C) in O(k + nlog k) worst case time (Gowda et al., 1983).
O	The second goal, if there are many Pareto optimal solutions, is to find a diverse set of them (Deb, 2001).
O	There are several approaches to achieve diversity (Özsoy and Pınar, 2006; Masuyama et al., 1981) which their core is measuring the number of solutions near to each solution, or computing the distance between a solution and its nearest solutions in the objective space.
O	Using Voronoi regions and N nearest neighbor queries for the clients, the objective values of each solution C, can be determined in O(nlog k) time (Deb, 2001).
O	For each solution, it can be done in O(k + nlog k) worst case time (Gowda et al., 1983), i.e., in O(Nk + Nnlog k) time in total.
P	Note that the framework of the proposed MOAkBCL algorithm is similar to the NSGA-II framework (Deb et al., 2000).
N	Since there is no benchmark problem for the k-balanced problem, we obey some benchmark problems for the k-center and k-mean clustering problems (Fränti and Sieranoja, 2017) (Also, see http://cs.joensuu.fi/ sipu/datasets/).
P	As popular approaches, multi-objective evolutionary algorithms (Coello et al., 2002; Deb, 2001) extensively have been applied to find Pareto optimal solutions of multiobjective problems.
P	Similar to (Davoodi et al., 2009; Karasakal and Silav, 2016) and many other studies, we can employ evolutionary algorithms to solve the k-BCL problem.
P	Also, other parameters such as the size of population, crossover and mutation are important in balancing between exploring and exploiting in these algorithms (Coello et al., 2002; Deb, 2001).
P	However, most of the popular metrics are proper for measuring the algorithms regarding just one goal (instead of simultaneously measuring) (Coello et al., 2002; Deb, 2001).
O	To compare between MOAkBCL and NSGA-II, we use set coverage metric (scm) (Zitzler et al., 2000) to compare the Pareto-optimality of the final obtained solutions, and spacing metric (sm) (Schott, 1995) to compare the diversity goal. Both of them are easy to understand and also to implement.
O	To this end, we use the presented integer linear model in Marín’s study (Marín, 2011) and apply the ε − constraint method and run the solver software for minimizing F1(C) under the constraint F3(C) < ε, for ε = 1, 2, …, n.
P	The idea behind this reduction is originated from (Masuyama et al., 1981) which is a proof for NP-completeness of the k-center problem by a reduction of dominating set problem on planar graphs of maximum degree 3.
O	It is proved that the minimum dominating set on cubic graphs is NP-complete (Alimonti and Kann, 2000), however, to employ the idea in (Masuyama et al., 1981), we need to show that the result also holds for the planar cubic graphs which is easy by some elaboration.
P	To this end, by using graph G, we construct a planar Euclidean graph H as an instance for k − BCL12 problem with the following properties (Masuyama et al., 1981).
P	On the other hand, Masuyama et al. (1981) showed that all the joint and node points can be covered by k ≤ p + L, so the proof is complete.
O	For a more formal introduction we refer to Laporte (2007).
O	In the last decade many successful heuristics have been developed that can solve instances of several hundred customers to near-optimality in a few minutes of computing time (Vidal et al., 2013a).
P	Most heuristics are based upon metaheuristic designs, and a comprehensive survey about the plethora of heuristic designs is provided by Salhi (2014). 
O	Many authors have shown that high-quality results can be achieved with different designs, most notably with genetic algorithms (Vidal et al., 2013a), a mix of exact solvers and heuristics (Subramanian et al., 2013) and memetic algorithms (Nagata and Br¨aysy, 2009).
O	Complete metaheuristic designs revolve around an effective local search, e.g., variable neighborhood search (Mladenovi´c and Hansen, 1997) or guided local search (Voudouris and Tsang, 2003), and it has been particularly successful to solve the Traveling Salesman Problem (TSP) (Lin and Kernighan, 1973).
P	In the context of the VRP, many effective local search operators have been developed over the years, and have been condensed in an online library (Gro¨er et al., 2010).
P	That such an analysis and refinement of local search can result in impressive solution methods has already been shown for the TSP by Helsgaun (2000).
O	Local search is one of few general approaches to combinatorial optimization problems with empirical success (Johnson et al., 1988).
O	Lin and Kernighan (1973) (LK) have generalized this idea for the TSP in a computationally feasible way.
O	Further operators include the ejection chain and the GENI insertion operator (Gendreau et al., 1992).
O	This is the underlying principle of the variable neighborhood search metaheuristic framework (Mladenovi´c and Hansen, 1997), but it is extremely common in the area of vehicle routing (S¨orensen et al., 2008).
O	For example, the active-guided evolution strategies of Mester and Br¨aysy (2007), use relocate, swap, 2-opt and Or-exchange, the hybrid genetic algorithm of Vidal et al. (2013b) uses relocate, swap and 2-opt, and the iterated local search of Subramanian et al. (2013) uses relocate, 2-opt, Or-exchange and CROSS-exchange.
O	A metaheuristic that drastically restricts neighborhoods is, e.g., granular tabu search (Toth and Vigo, 2003).
P	Another very effective approach to prune the neighborhood of more complex operators is sequential search (Irnich et al., 2006), which constitutes a generalization of the partial gains criterion in LK.
P	One of the most effective heuristics for the TSP is the heuristic by Lin and Kernighan (1973) (LK), which has been further refined by Helsgaun (2000) and Applegate et al. (2003) to solve instances with more than 100.000 customers.
P	For a more elaborate description of LK we refer the interested reader to Helsgaun (2000).
O	The CROSS-exchange operator (CE) is a generic local search operator that tries to exchange two substrings ˆri and ˆrj of two different routes ri and rj (Taillard et al., 1997).
O	In an embedded neighborhood several simple moves are combined to form one compound move (Ergun et al., 2006).
P	An example for a compound move is the ejection chain introduced in Glover (1996) and formalized in Rego (2001) for the VRP.
O	One approach that does not rely on randomization nor on additional algorithmic components is guided local search (GLS) (Voudouris and Tsang, 2003).
O	Even though GLS is not as popular as other metaheuristics (for instance tabu search or variable neighborhood search), it has been successfully applied to the TSP (Voudouris and Tsang, 2003) and the VRP (Mester and Br¨aysy, 2007).
P	Kilby et al. (1999) experimentally found λ ∈ [0.1, 0.3] to be a good choice while Mester and Br¨aysy (2007) use λ = 0.01.
O	An obvious idea is to focus on the most expensive edges as in Mester and Br¨aysy (2007), since these contribute more weight to the objective function.
O	Details of both the methodology and the results of this study are beyond the scope of this paper, and can be found in Arnold and S¨orensen (2018).
O	We adopt the same definition for the width of an edge as in Arnold and S¨orensen (2018), and compute the width of an edge as the distance measured along the axis perpendicular to the line connecting the depot with the center of gravity of the route.
P	A starting solution is constructed with the popular and relatively simple heuristic by Clarke and Wright (1964).
O	All experiments are executed on instances 26 to 55 by Uchoa et al. (2017).
P	The performance is expressed as the average gap between the best solutions found within a specific time horizon and the best known results, as reported by Uchoa et al. (2017).
P	A good example is the α-nearness defined in Helsgaun (2000) for the TSP.
P	Some successful heuristics for bin-packing algorithms like First Fit Decreasing (Simchi-Levi, 1994) try to group large items first, and we translate this intuition to the VRP by trying to assign customers with a high demand first.
P	This approach successfully reduced the number of routes in the starting solution of 33 out of 100 instances in the benchmark set by Uchoa et al. (2017), and in some cases improved the solution quality significantly.
O	In contrast, most state-of-the-art heuristics utilize randomness to diversify the search and escape local minima (Gutjahr, 2010).
P	The exact role and the benefits of including random elements are rarely investigated, notwithstanding the fact that reliance on randomness has some significant disadvantages and, according to some researchers, can prevent the development of better, deterministic search components, see e.g., Glover (2007).
O	In the Multi-Depot Vehicle Routing Problem (MDVRP) customers can be delivered from a given set of depots (Montoya-Torres et al., 2015). 
P	For a more elaborate description of the problem we refer to Cattaruzza et al. (2016).
O	In short, we generate a large pool of different VRP solutions and hope to find at least one that satisfies the additional constraints, an approach often used in the literature (Cattaruzza et al., 2016). 
O	More concretely, each solution obtained after the perturbation and the optimisation phase in Algorithm 3 is evaluated with the three simple bin-packing heuristics First Fit, First Fit Decreasing and Best Fit (Simchi-Levi, 1994).
O	We test the performance of KGLS on the entire instance set by Uchoa et al. (2017) (U).
P	Finally, we use the benchmark set by Taillard et al. (1996) to investigate the performance on MTVRP instances.
P	We compare the performance of KGLS with some of the most efficient VRP heuristics in the literature: the hybrid genetic algorithm (HGSADC) of Vidal et al. (2013b), the iterated local search (ILS) of Subramanian et al. (2013), and the classical adaptive large neighborhood search (ALNS) of Pisinger and Ropke (2007).
O	For the MTVRP we use the results from the ALNSP by Fran¸cois et al. (2016) and the memetic algorithm (MA) by Cattaruzza et al. (2014) as comparison. 
O	As a result, the average gap on those 87 instances to the BKS reported in Cattaruzza et al. (2016) is -0.06%, computed in 26 seconds (compared to a 0.68% gap of MA and to a 0.69% gap of ALNSP on those instance, obtained after similar computing times).
P	In 1997, Burke et al. presented an introduction to automated examination timetabling using information collected from UK Universities, where the authors catalogued and structured common types of constraints, and described popular approaches used at the time.
P	In 2009, Qu et al. summarized earlier surveys on examination time tabling and presented a state of the art of solution methods.
P	Gogos et al. and Arbaoui et al. proposed a preprocessing step to reveal hidden hard constraints that can be deduced prior to solving.
O	A selection of investigated methods includes: Graph Ordering Heuristic, Tabu Search, Simulated Annealing, Great Deluge, Hill Climbing with or without Late Acceptance, Bin Packing Heuristic, Evolutionary and Nature Inspired Algorithms and Hyper-Heuristic.
P	Recently, Alzaqebah et al. obtained the best result found so far for one instance of the Toronto benchmark using a hybrid bee colony approach, simulated annealing, and late acceptance hill climbing.
O	Hybridizing integer programming and a decomposition approach was investigated by Qu et al.
O	An integer programming phase for assigning exams to rooms was used by Gogos et al.
P	For a specific problem, MirHassani proposed a Mixed Integer Programming (MIP) model.
P	Recently, a column generation approach was proposed by Woumans et al.
P	To the best of our knowledge, Arbaoui et al. presented the first work on spacing soft constraint lower bounds for ITC2007 problems.
P	McCollum et al. proposed a new mathematical model for the ITC2007 examination timetabling track that provides a meaningful basis for hard and soft constraints.
P	Fonseca et al. proposed an improved version of this model that made a better use of memory and they encountered fewer out-of-memory events on sizeable instances.
P	Arbaoui et al. ran an improved formulation on each term considered individually, and for certain terms optimal values were attained.
P	Soghier et al. investigated bin packing heuristics and proposed an adaptive hybrid hyper-heuristic approach.
P	Data-dependent Dual-Feasible Functions (DDFF) have previously been proposed for building lower bounds for binpacking problems (see Carlier et al.).
P	We focus on two terms, and for the purposes of comparison we use the improved approach ade available to the community by Tom`as M¨uller [1], the winner of the ITC2007 examination timetabling track.
P	To the best of our knowledge, the first attempt to design lower bounds for these soft constraints was proposed by Arbaoui et al.
N	In practice it was observed by Fonseca et al. for formulation F', and by Arbaoui et al. [6] for formulation M, that these formulations could not easily be run on the current generation of solvers.
O	A very popular application of the two together is the so-called Prescriptive Analytics field (Bertsimas and Kallus, 2014), where ML is used to predict a phenomenon in the future, and MO techniques are used to optimize an objective over that prediction.
O	A popular research area is now also to use Machine Learning to improve heuristics decisions in Mixed Integer Linear Programming (MILP) algorithms, for example in the branching procedure (Alvarez et al., 2017; Khalil et al., 2016; Lodi and Zarpellon, 2017) or in decomposition techniques (Kruber and Marco, 2017).
P	The paper by Bello et al. (2016) introduces a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning; the authors study the traveling salesman problem and train a recurrent network to predict a probability distribution over different solutions.
O	In Dai et al. (2017) the authors investigate instead how to learn a heuristic algorithm by exploiting the structure of the instances of interest; the resulting approach is applied to different optimization problems over graphs, namely the minimum vertex cover, the maximum cut and the traveling salesman problems.
O	The work of Hottung et al. (2017) integrates ML and tree search to derive a truncated branch-and-bound algorithm using bound estimates; the proposed method, called deep-learning assisted heuristic tree search, is applied to the so-called container pre-marshalling problem.
O	The problem of deciding at which node of a branch-and-bound tree a heuristic should be run is instead addressed in Khalil et al. (2017), where ML is used to predict whether a certain heuristic would improve the incumbent if applied at a given branching node.
O	Finally, the very recent special issue IFC, Passerini et al. (2017) contains a collection of papers that combine ML with constraint programming/optimization.
O	In particular, the work of Berg and Järvisalo (2017) studies the applicability of Boolean optimization (maxSAT) to clustering problems, while Bartlett and Cussens (2017) uses integer linear programming for learning Bayesian network structures, and Bessiere et al. (2017) investigates an architecture for acquiring constraint programming constraints from classified examples.
O	In this work, we will focus on a specific application, already studied by the first author in Fischetti and Monaci (2016), namely the offshore wind park layout optimization problem.
P	Different techniques have been studied to solve this optimization problem; the reader is referred to, e.g. Cetinay et al. (2017); Gao et al. (2016); McWilliam et al. (2012) for some heuristic methods, and to Fischetti and Pisinger (2018) for a more detailed literature overview on the problem.
P	In this work we will use the mathematical optimization framework proposed by Fischetti and Monaci (2016) as this is the tool currently used by our industrial partner Vattenfall BA Wind.
O	It was estimated in Barthelmie et al. (2009) that, for large offshore wind farms, the average power loss due to turbine wakes is around 10–20% of the total energy production.
P	The MILP-based approach proposed in Fischetti and Monaci (2016), takes about 10 h for a complete optimization.
N	We defined and optimized over 3000 instances using the MO tool developed in Fischetti and Monaci (2016).
O	For further details, see Fischetti (2014).
O	In this paper we used Jensen’s model (Jensen, 1983) to compute the interference caused by a turbine: the interference is modelled as a cone centered in the upwind turbine.
N	Additional constraints and sophisticated algorithms to solve large scale instances have been developed by the first author (see Fischetti and Monaci, 2016), but their description is out of the scope of the present paper.
P	We used a Jensen’s model to compute it Jensen (1983);
O	This model works on larger instances compared with equivalent models in the literature involving 2- index variables yij = xixj; see Fischetti (2014) for details.
O	As shown in Fischetti and Monaci (2016), one can then take wind scenarios into account by simply defining Pi := K k=1 πkPk i (i ∈ V ) and Iij := K k=1 πkI k ij (i, j ∈ V ).
P	To solve large-scale instances (with 20 000+ possible positions) some ad-hoc heuristics and a MILP-based proximity search (Fischetti and Monaci, 2014) heuristic have been used on top of this basic model.
P	We refer the interested reader to Fischetti and Monaci (2016) for details.
O	Optimization for the difficult case (true production) was instead obtained through the MILP-based heuristic of Fischetti and Monaci (2016), with a time limit of 1 h on a standard PC using IBM ILOG CPLEX 12.6.
P	We used the Root Mean Squared Error (RMSE) Bishop (2006) to measure the quality of our ML models.
P	The interested reader is referred to Bishop (2006) for an in-depth discussion on the topic.
O	The hyperparameters of the models (e.g., the number of layers and units of the NN or the kernel type in the SVR) are chosen using the scikitlearn (Pedregosa et al., 2011) function GridSearchCV, that exhaustively considers all parameter combinations on a grid (5-fold crossvalidated on the training set).
O	These problems are rooted in the work of Demaine et al. and Romich et al.
O	Romich et al. considered a sensor placement problem in which the sensors must be connected to each other and each demand point has to be covered.
O	This problem is similar to the simple plant location problem (Kuehn and Hamburger).
O	When the number of facilities is a given value p, then the problem is similar to the classical p-median problem (Hakimi).
O	nother variant consists of locating exactly p facilities in order to maximize the total demand within a distance R of a facility, as in Church and ReVelle, for example, or at most p facilities if these have fixed costs (Kolen and Tamir).
O	Related problems in which the facilities are interconnected are the so-called tree of hubs location problem (Contreras et al.) and some more general network design problems where the underlying connected network is not necessarily a tree (Contreras and Fernandez).
O	Laporte and Rodr´ıguez Mart´ın surveyed related cycle location problems in which the interconnections are restricted to cycles.
O	An initial solution is generated by means of a hybrid construction algorithm inspired from the greedy randomized adaptive search procedure (GRASP) (see, e.g., Feo and Resende, Resende and Werneck, Resende and Ribeiro).
P	AM is also called 3D printing, as the objects are made from 3D-model data through joining the materials, as opposed to subtractive traditional techniques (Li et al., 2017).
P	AM processes provide many significant advantages, such as design flexibility, high accuracy, resource efficiency, and material efficiency, over traditional techniques (Kucukkoc et al., 2016).
P	The total AM market is predicted to rise nearly six-fold to $12 billion in 2025, driven by growth in various industries including prototypes, moulds and aerospace (Vicari, 2015).
O	Moreover, NASA additively manufactured a turbopump, used as a rocket engine fuel pump (made with hundreds of parts including a turbine that spins at over 90,000 rpm) (Marshall, 2017).
P	For example, Cooper et al. (2012) addressed the development of additively manufactured hydraulic components for Formula 1 racing cars to reduce weight and improve efficiency.
O	Khajavi et al. (2014) investigated the potential impact of AM technology on the configuration of spare parts supply chains.
O	In the sense that the parts are scheduled in batches on the machines, the problem may look similar to the well-known batch scheduling problem with arbitrary job sizes (Brucker et al., 1998; Li et al., 2013) in the literature.
O	In its simplest form, only one batch machine is considered and the single batch machine scheduling problem (Brucker and Kovalyov, 1996) arises when there are batches and setup times required between these batches (Albers and Brucker, 1993).
P	If there is a single batch machine to which a set of jobs (with the same processing times) needs to be allocated in batches, the makespan problem is equivalent to a well-known bin packing problem (Koh et al., 2004).
O	The bin packing problem is strongly NP-hard based on Garey and Jackson (Garey, 1997), so minimizing the makespan in the single batch machine scheduling problem with unequal processing times (or arbitrary job sizes) is strongly NP-hard (Koh et al., 2004).
N	When there are parallel batch machines (Pm|batch|Cmax), the problem of minimizing makespan (which is already strongly NP-hard in a single batch machine environment), becomes even harder to solve (Damodaran et al., 2011).
O	To cite a few, Cheng et al. (1994) aimed to minimise the total flow time of items produced on a batch machine.
O	Ghosh and Gupta (1997) addressed the problem with the aim of minimising maximum lateness.
P	Relatively recent studies have addressed the parallel batch machine scheduling problems with identical and non-identical machines and proposed heuristic/metaheuristic techniques due to the complexity of the problem (see, for example, Jia and Leung (2015), and Zhou et al. (2017) for algorithms for solving parallel identical batch machine scheduling problems; and Li et al. (2013), and Shahidi-Zadeh et al. (2017) for algorithms for parallel non-identical batch machine scheduling problems).
O	Trindade et al. (2018) discussed four different versions of batch scheduling problems, considering a single processing machine or parallel processing machines and considering jobs with or without release times.
O	Li and Zhang (2018) provided more details on exact and heuristic methods to minimise makespan in the single batch machine scheduling problem, and Méndez et al. (2006) made a state-of-the-art review of optimisation methods for the short-term scheduling of batch processes.
P	Potts and Kovalyov (2000) presented a comprehensive survey on batch scheduling problems and Webster and Baker (1995) presented an overview of algorithms and complexity results for scheduling batch processing machines.
O	While there are similarities between the batch scheduling problems and AM machine scheduling problems studied in this paper, the AM machine scheduling problem differs from the batch scheduling problem in several ways (regardless of considering single or parallel machines) (Li et al., 2017).
O	Secondly, as the processing time of a job is calculated via a function, different sets or combinations of parts will lead to different costs in the AM machine scheduling problem (Li et al., 2017)
P	Kucukkoc et al. (2016) introduced the AM machine scheduling problem to maximise the utilisation of AM machines in terms of the production areas used.
P	Li et al. (2017) introduced the problem of planning AM machines.
P	Kucukkoc et al. (2018) proposed a genetic algorithm approach to minimise maximum lateness in the multiple machine environment.
P	Chergui et al. (2018) addressed the production scheduling and nesting problem in additive manufacturing and proposed a heuristic approach.
P	Fera et al. (2018) presented a cost-based model for the scheduling problem of a single AM machine to minimise the weighted total of earliness and tardiness costs.
P	Dvorak et al. (2018) studied the AM machine scheduling problem with part due dates to minimise the number of tardy parts (referred to as tardy builds in the corresponding paper) and addressed to the main challenges of the problem.
P	Fera et al. (2018) addressed the single AM machine scheduling problem to minimise time and cost.
P	Zhang et al. (2016) analysed the multi-parts placement problem in AM and proposed a two-step strategy.
P	The problem is a special case of well-known nesting problem, referred to as NP-hard (Zhang et al., 2016).
O	Zhang et al. (2017) focussed on the optimisation process of build orientation for multi-part production in additive manufacturing and proposed a two-stage approach as a solution method.
O	There also are some conceptual works which aim at addressing both the build orientation and 2D packing and scheduling problem, see for example Oh et al. (2018).
N	There are some papers focusing only the problem of determining building orientations or placements of multi-parts on a tray (Zhang et al., 2016), rather than scheduling the AM machines.
P	Monma and Potts (1989) showed that the makespan problem is NP-hard for two identical parallel batch machines.
P	The test data were mostly derived from the work of Li et al. (2017) and organised in such a way to represent the problem characteristics considered in this research.
O	According to different customer orders, each wafer requires a specific mold, also called reticles (Ham & Cho, 2015).
P	The RCPMS problem to minimize the makespan was first studied by Garey & Graham (1975) who showed that the greedy list schedules can obtain an approximation ratio of 3 − m/3, where m is the number of identical parallel machines.
O	Blazewicz, Kubiak, Rock & Szwarcfiter (1987) considered the RCPMS problem with non-preemptable jobs to minimize total flow time and presented an O(n 3 ) algorithm for the case where each job requires one unit of additional resource.
P	Blazewicz, Dror & Weglarz (1991) and Reklaitis (1996) provided a comprehensive review of resource-constrained problems, including applications in the semiconductor manufacturing and chemical processing industries.
P	Hoopes & Mazzola (1996) provided mathematical formulations for static and dynamic versions of the parallel-machine scheduling problem with resource flexibility.
P	Daniels, Hua & Webster (1999) extended the formulation of the RCPMS problem to the flexible-resource case, where the job assignment to the machines is unspecified, and presented heuristics to find approximate solutions.
P	More recently, Edis, Oguz & Ozkarahan (2013) reviewed the literature related to the RCPMS problem and presented integer programming models for two problems.
O	Niemeier & Wiese (2015) studied the identical parallel-machine problem with an orthogonal resource constraint (where the sum of the resource requirements of the jobs active at any given time does not exceed 1) and presented an algorithm with an approximation ratio of 2+ ∈.
O	For example, Dell’Amico & Martello (1995) presented a branch and bound algorithm based on sophisticated lower and upper bound computations and several dominance properties.
P	An exact cutting plane algorithm based on the identification of valid inequalities was proposed by Mokotoff (2004).
P	Dell’Amico, Iori , Martello & Monaci (2008) presented an exact algorithm which is based on a specialized binary search and a branch-and-price scheme.
P	Graham (1966) performed a worst-case analysis of the Longest Processing Time (LPT) rule as well as that of an arbitrary list schedule.
P	A more sophisticated heuristic for the problem, called MULTIFIT, was presented by Coffman, Garey & Johnson (1978).
P	The MULTIFIT algorithm, based on techniques from bin-packing, has a bound of 1.22, which was improved to 1.2 by Friesen (1984).
O	Lee & Massey (1988) used the result of the LPT rule as the base and then applied the MULTIFIT algorithm with less iterations.
P	Gupta & Ruiz-Torres (2001) presented a new heuristic based on bin-packing and list scheduling, called LISTFIT.
P	Min and Cheng (1999) proposed a genetic algorithm (GA) to minimize the makespan for the scheduling problem with identical parallel machines.
P	Lee, Wu & Chen (2006) proposed a Simulated Annealing (SA) approach with hill-climbing moves to tackle the makespan problem on identical parallel machines. 
P	Davidovic, Selmic, Teodorovic & Ramljak (2012) studied the static scheduling of independent tasks on homogeneous multiprocessor systems and proposed a Bee Colony Optimization (BCO) algorithm.
O	Tian, Liu, Yuan & Wang (2013) presented a Discrete Particle Swarm Optimization (DPSO) algorithm to solve a two-stage assembly scheduling problem, where the first stage is a workstation consisting of several identical parallel machines and the second stage is a single assembly machine workshop.
O	Brucker, Dhaenens-Flipo, Knust, Kravchenko & Werner (2002) reviewed the complexity of parallel-machine scheduling problems with a single server.
O	The simulated annealing and genetic algorithms proposed by Hasani, Kravchenko & Werner (2014) can satisfactorily solve problems with up to 1,000 jobs, and the fast O(n 2 ) heuristics from Hasani, Kravchenko & Werner (2016) can solve even larger instances with up to 10,000 jobs close to optimality.
O	While the parallel-machine scheduling problems with tool requirements have been considered in the existing literature (Beez˜ao, Cordeau, Laporte & Yanasse, 2017; Fathi & Barnette, 2002; Paiva & Carvalho, 2017), to our knowledge, except for a single paper by Hong, Sun, & Li (2008) that considers the minimization of total tardiness on unrelated parallel machines with mold constraints and set up times, no existing algorithms consider scheduling problems with mold constraints.
O	The problem P2|mi |Cmax is NP-hard at least in the ordinary sense since the problem without a mold constraint, P2||Cmax is known to be NP-hard in the ordinary sense (Garey & Johnson, 1979; Sethi, 1977).
P	However, as it is the case for several MILP problems (Lodi & Tramontani, 2013), removal of the constraints (7) from the formulation, the CPU time needed to solve a problem instance often increased.
P	Therefore, we simplify the Mixed Integer Linear Programming (MILP) formulation for the problem Pm||Cmax proposed by Mokotoff (2004) to optimally solve the problem P2||Cmax which is a lower bound for the makespan of the problem P2|mi |Cmax.
P	For the structure of Algorithm DPSO, we refer to Liao et al. (2012).
P	In the VNS algorithm proposed by Hansen and Mladenovi´c (2008), inversion and insertion procedures were applied for changing the neighborhood.
P	Therefore, the processing time pi is either randomly taken from uniform distributions represented by U(1, 10) and U(1, 100), respectively, or is randomly taken from binomial distributions, represented by B(1, 10; ρ) and B(1, 10; ρ), respectively, where ρ = 0.4, 0.5 or 0.6 is the parameter used to generate random numbers from three different binomial distributions using the method proposed by Boco (2000).
O	This observation is similar to the result found by Hasani, Kravchenko & Werner (2016) for the 2-machine scheduling problem with a single server.
N	However, this mechanism does not take advantage of the benefit of centralized inventories, which is described in Eppen (1979).
O	A comprehensive review of inventory systems subject to several demand classes can be found in Kleijn and Dekker (1999) and Arslan et al. (2007), and characterizations can be found in Teunter and Haneveld (2008).
O	The service-level approach introduces service-level constraints in place of the shortage costs for each demand class, where the stock availability - the servicelevel provided by an inventory system - is measured in three main ways (Axsäter, 2006): Service-level type-I (i.e., the probability of no stockout per order cycle); fill rate (i.e., the fraction of demand that can be satisfied immediately from stock on-hand); and, ready rate (i.e., the fraction of time with positive stock on-hand).
O	Detailed information on service-level measures can be found in Schneider (1981), Zipkin (1986), and Chen and Krass (2001).
O	In this paper, we adopt the SLC approach for cost optimization. Under this approach, Dekker et al. (2002) already analyzed the CL policy when the inventory system worked under a continuous review of the lot-for-lot policy with several demand classes, lost sales, and Poisson demand.
O	Similarly, Arslan et al. (2007) formulated an SLC model under fill-rate service levels to obtain the optimal parameters of a CL policy with multiple demand classes under the assumptions of Poisson demand, deterministic lead time, and continuous review (Q, r) policy.
P	Under single class demand and different inventory policies, Anderson and Lagodimos (1989) studied the FCFS and LIFO allocation rules, and recently Van Foreest et al. (2018) has presented a reservation policy as an alternative to the FCFS allocation rule.
O	Given this complexity, the literature has focused on manageable but sub-optimal rules, e.g., the threshold clearing mechanism from Deshpande et al. (2003) and the FCFS type clearing scheme from Arslan et al. (2007).
O	Möllering and Thonemann (2010) analyzed a periodic reviewbase stock policy with two demand classes, deterministic lead time, discrete demand distributions, and full backorders.
O	Wang et al. (2013b) analyzed the same model as Möllering and Thonemann (2010) did; however, instead of using the same policy, they considered an anticipated rationing policy that reserved inventory for high-priority classes by employing a constant-critical level and incoming replenishment for the next period.
O	A hybrid formulation was presented in Wang et al. (2013a), which analyzed a continuous review (Q, r, C) policy with two demand classes—full backorders and deterministic lead time—and Poisson demand.
P	To allocate backorders when multiple outstanding orders exist, they adopted the threshold-clearing mechanism of Deshpande et al. (2003).
O	Vicil and Jackson (2016) analyzed the continuous review (S − 1, S,C) policy with two demand classes under the priority-clearing policy.
O	Recently, Vicil and Jackson (2018) studied a continuous review lot-for-lot ordering, and a threshold-level-based allocation and replenishment policy for two priority-demand classes using Poisson demand and full backorders under fill-rates and waiting time service levels.
O	On the contrary, Escalona et al. (2017b) considered a continuous review (Q, r, C) model with full-backorder under type-I service-level constrains and two customer classes when demand volume was large (FMCG items).
P	To allocate backorders when multiple outstanding orders exist, Escalona et al. (2017b) adapted the threshold-clearing mechanism of Deshpande et al. (2003) to work under strictly increasing non-negative demands.
O	This paper builds on the work of Escalona et al. (2017b) by considering other service-level measures in the design of a CL policy for fast-moving items.
P	In this paper, we consider the thresholdclearing mechanism of Deshpande et al. (2003) under strictly increasing non-negative demands and the hitting time approach to clear backlogged orders (Escalona et al., 2017a; Escalona et al., 2017b).
P	Using the expressions developed by Escalona et al. (2017b), the service-level type-I, provided to the high- and low-priority class under strictly increasing non-negative demand, are respectively α1(r,C) = P(D(L) ≤ r − C).
O	Escalona et al. (2017b) also showed that α2(r, C) increases in r and decreases in C, and α1(r, C) increases in r and C.
O	Following Zipkin (1986), the fill-rate provided to class i is defined as βi(Q,r,C) = 1 − Ai(Q,r,C) μi,
O	Note that under strictly increasing non-negative demand, the inventory position in steady state is uniformly distributed over (r,r + Q], irrespective of the lead-time demand (Serfozo and Stidham, 1978; Zheng, 1992).
O	Furthermore, under single-class demand and continuous demand, fill-rates and readyrates are equivalent (Axsäter, 2006).
O	In a continuous review (Q, r, C) policy with full backorders and deterministic lead time, the expected on-hand steady-state inventory is E(OH∞(Q,r,C)) = Q 2 + r − μL + E(B∞ 1 (Q,r,C)) + E(B∞ 2 (Q,r,C)), where E(B∞ i (Q,r,C)) is the class-i steady-state backorder, i = 1, 2 (Escalona et al. (2017b)).
O	Note that Eqs. (15)-(16) are a standard form of expressing backorders used by Zipkin (1986).
O	Furthermore, (β-SLC) can be optimally solved using the procedure described in Axsäter (2006) or Rosling (1999).
O	It is easy to show that model (2α-SLC) is a relaxation of the reformulation of (α-SLC) with α = α1, because μ1 μ B(r − C  ,Q) + μ2 μ B(r − C,Q) = B(r − C  ,Q) − μ2 μ  B(r − C  ,Q) − B(r − C,Q)  ≤ B(r − C  ,Q) ≤ B(r,Q), where the inequalities emerge from C  ≤ 0, and B(x, Q) is strictly decreasing in (x, Q) (Zipkin, 1986).
O	In other words, a target (or a space point) is said to be covered if its coverage measure satisfies some predefined threshold (Wang 2010). 
P	First introduced by Gage (1992), the barrier-coverage problem mostly deals with finding an effective deployment scheme of sensors to detect targets crossing a barrier line.
P	Benkoczi et al. (2015) provide a more recent example that considers a barrier as a line segment and develop a model to solve the problem of determining the locations of mobile sensors with arbitrary coverage ranges.
P	Liu and Towsley (2004) extend the barrier-line problem to a two-dimensional problem by treating a barrier as a belt-shaped region.
O	Galyaev and Maslov (2011) and Washburn (2015) used game theory to analyse barrier problems for mobile sensors.
O	Galyaev and Maslov (2011) determined optimal patrol trajectories and speeds for sensors performing barrier searches in a water environment.
P	Washburn (2015) considered straight-line and circular barriers with mobile sensors and developed an analytic theory based on a model using a two-person zero-sum game to analyze penetration conditions.
P	In his earlier work, Washburn (1982) derived an upper bound on the probability of detecting a target with a mobile sensor that is patrolling a channel.
O	Arora et al. (2004) provide a detailed review of the performance of WSNs applied to intrusion detection.
P	Cardei and Wu (2006) and Fan and Jin (2010) also provide a good review of the key factors and issues surrounding barrier coverage in a WSN. 
O	Chen et al. (2007) considered developing a barrier along a belt-shaped region using sensors of the same type.
P	Chen at el. (2008) proposed a methodology to determine and repair weak sensing zones of a barrier to address the reliability issue.
O	He et al. (2012) focused on budget constraints in their model.
O	In subsequent work, they included energy consumption in their model (He et al. 2013, 2014).
O	Similarly, Cheng et al. (2014) considered the energy-management problem of sensor networks and proposed a density-barrier construction algorithm which minimizes the moving distances of mobile sensors.
O	Saipulla et al. (2008, 2009) studied the performance of air-dropped sensor barriers under the assumption that sensor locations are affected by environmental factors, such as wind and terrain.
O	In another study, Saipulla et al. (2013) considered the performance of barriers for different sensor-deployment strategies and studied the impact of sensor mobility on coverage.
O	Ssu et al. (2009) studied the performance of barriers composed of randomly deployed sensors with directional sensing capability. 
O	Different from previous studies, Chen et al. (2015), Gong et al. (2016) and Wang et al. (2016) consider barrier-coverage problems for bistatic sensor networks which consist of two types of sensors, transmitters and receivers, working in pairs.
O	Wang et al. (2017a) considered a barrier-coverage problem for sensors with location errors.
O	Wang et al. (2017b) analysed the use of probabilistic coverage models and uncertainties in WSN coverage problems
O	Karatas and Onggo (2016) considered a simple case of a barrier coverage problem in which multiple types of sensors are located to candidate locations along a line-shaped barrier with the objective of maximizing the total detection probability of targets crossing it.
P	In a more recent study, Karatas (2018) tackled a specific sensor location problem which considers locating a given number of sensors along a belt-shaped region for a hybrid barrier and point coverage application.
P	Interested readers can also refer to Karatas et al. (2019) for a review of location problems, including barrier coverage problems, observed in military context.
O	This is different from the use of a simulation to validate or test the robustness of a solution obtained from an optimisation model (e.g. Wang et al. 2009, Tao et al. 2012, Karatas and Onggo 2016).
O	One of the earliest reviews of OvS research was published in 1987 (Meketon, 1987).
O	Recently, Fu (2015) provided a comprehensive overview of OvS models. 
O	The same classification is also used in Abo-Hamad and Arisha (2011). 
P	The combination of metaheuristics and simulation for discrete optimisation problems is referred to as simheuristics and reviewed in Juan et al. (2015).
O	Amaran et al. (2016) classify OvS techniques based on whether they are applicable to problems with discrete or continuous variables, and whether they apply local or global optimisation.
O	Xu et al. (2015) classify OvS techniques based on how they handle the noisy, computationally expensive, and black-box nature of simulation optimization.
P	OvS has been applied in several areas (Amaran et al. 2016, Xu et al. 2015).
O	For example, Ahmed and Alkhamis (2009) used a statistical technique to find the optimum number of resources (doctors, nurses and lab technicians) in an emergency department to maximize patient throughput and reduce patient time in the system, subject to deterministic constraints (i.e. total budget and bounds on the number of resources) and a stochastic constraint (i.e. average waiting time).
O	For example, Avramidis et al. (2010) combined simulation with ILP to determine an optimum staff schedule in a call centre that minimized the total cost of staff subject to several expected service-level constraints. 
P	Lin and Chen (2015) combined GA and optimal computing budget allocation (OCBA) approach to generate an optimal flow shop schedule in a semiconductor assembly facility.
P	De Keizer et al. (2015) used a hybrid Mixed Integer Linear Programming (MILP) and discrete-event simulation model to address the design of logistics network to distribute perishable products.
P	Juan et al. (2014) applied simheuristics method to minimize the total inventory and routing cost with stochastic demand.
O	Saif and Elhedhli (2016) combined Mixed Integer Programming and discrete-event simulation model to minimize the total cost of a cold supply chain which took into account routing, inventory and green gas emission costs.
P	GA metaheuristics was used in McCormack and Cotes (2015) to find good base-station locations and determine emergency-vehicle fleet allocation at each station to maximize the expected survival probability of patients.
O	For example, Yin et al. (2017) used GA metaheuristics to determine optimum wind-farm micrositing which considers wind uncertainty (e.g. direction, speed and probability of occurrence).
P	Chang and Lin (2015) used a meta-modelbased technique to establish an optimum hybrid renewable-energy system design that minimized the expected total cost subject to meeting the demand for power.
P	Rytwinski and Crowe (2010) developed a cellular automata simulation to model the stochastic and complex behaviour of wildfire.
O	The application in these two areas have been reviewed by Alrabghi and Tiwari (2015) and Abo-Hamad and Arisha (2011), respectively.
O	OvS has also been applied to estimate the parameters of their simulation model (Kuo et al. 2016).
P	Amaran et al. (2016) provide a good summary of OvS literature reviews published until 2011.
P	It should be noted that, in the barriercoverage problem, simulation has been widely used to test the correctness or robustness of optimisationmodel results (e.g. Wang et al. 2009, Tao et al. 2012, Karatas and Onggo 2016).
N	Today’s expensive WSN infrastructures mostly depend on centrally-deployed hub-and-spoke networks (Basagni et al. 2004). 
O	Lying at the heart of the network-design domain, hub-and-spoke network applications are abundant in fields such as the military, telecommunications, computer networks and transportation (Contreras 2015).
P	Farahani et al. (2013) provide a good review of methods to solve hublocation problems.
O	A lateral distance, L i p d represents the shortest distance from sensor at i to the path p traversed by a target, i.e., the “distance of closest approach” (Arnold and Bram, 1962).
O	In reality, a region (or a target) can be monitored (covered) cooperatively by multiple sensors, such that each sensor contributes to the detection performance of a target (Karatas, 2017). 
P	There are basically two different types of sensing models adopted by researchers in the literature: discsensing model, and probabilistic sensing model (He et al. 2013).
P	In their study, Camm et al. (1990) prove that setting the penalty parameter – such as M1 and M2 used in constraint sets (10) and (11) – to a tight value helps in reducing the CPU time a solver needs to find a solution.
P	To this end, we adopt a special linearization technique as used by Morton et al. (2007), Salmerón (2012) and Karatas (2017, 2018). 
O	In telecommunications standardization bodies and open source code communities, this rising area is referred to as Mobile Edge Computing (MEC) (Patel, 2014).
P	Thanks to MEC deployment, the user experience can be augmented by drastically reducing the server access latency, so as to enable collaborative networking, tactile Internet (i.e., communications made possible by an extremely low latency such as augmented reality applications) (Aijaz et al., 2017), and more generally computation offloading to a nearby cloud with the goals to control mobile device energy consumption and to run computationally heavy applications using tiny low-power devices (Tran et al., 2017).
P	The technology to perform MEC orchestration operations is becoming mature (Lindemeier, 2016; Secci et al., 2016).
O	Concerning edge computing infrastructure design, pioneering experimental activities on the design of cloudlet systems have shown the huge impact of cloud access latency reduction in terms of throughput gain (Satyanarayanan et al., 2009).
O	Subsequent works have investigated the design of cloud network overlay protocol and controller for the adaptive mobility of virtual machines as a function of the mobility of users (Secci et al., 2016), and the capacity planning in the placement of edge computing facilities (Ceselli et al., 2017).
O	These studies have proceeded in parallel with standardization efforts at ETSI (ETSI, 2014),
O	Prominent examples include MAUI (Cuervo et al., 2010), CloneCloud (Chun et al., 2011), Cuckoo (Kemp et al., 2012), AIOLOS (Verbelen et al., 2012), Thinkair (Kosta et al., 2012) and COSMOS (Shi et al., 2014), which are frameworks allowing mobile devices to run (part of) their applications in edge cloud facilities, for different operating systems, with different goals (e.g., energy saving, execution time, etc.), and at different levels of operating system intrusiveness.
O	Here, the authors of Esteves et al. (2011) allocate computation resources using a capital-budgeting technique typically used in the field of financial option valuation.
O	Authors of Kristensen (2010) approach the resource allocation problem by foraging computation resources from different mobile devices.
O	Therefore, our work pertains to the field of cognitive network management, an emerging research area in computer networks which investigates the integration of data analytics into planning and operation of next-generation mobile networks (5GPPP, 2014).
O	The motivation for cognitive network management arises from the fact that the traffic demand in mobile networks is increasingly characterized by significant fluctuations in space and time, due to the diverse activities of subscribers at different times and locations (Furno et al., 2017a; Xu et al., 2017), as well as to the heterogeneity of mobile services (Marquez et al., 2017).
P	The principles of cognitive network management are well-established (Assem et al., 2015; Zheng et al., 2016), and are envisioned to apply across enabling technologies such as small cells (Lee et al., 2014), Cloud Radio Access Networks (C-RAN) (Foukas et al., 2016),Software Defined Networking (SDN) and Network Function Virtualization (NFV) (Rost et al., 2016).
P	For instance, established techniques used for Cloud resource management, such as receding horizon (Ardagna et al., 2014), could be used to fine-tune the fixed allocations returned by our proposed scheme.
O	Our setting requires to tackle a multiperiod extension of the famous Generalized Assignment Problem (GAP) (Martello and Toth, 1990).
O	For instance, in Shtub and Kogan (1998) the authors use a dynamic multiresource GAP to perform capacity planning; recent examples include (Rawitz and Voloshin, 2017), in which a flexible cell selection in cellular networks is tackled, and Arjona Aroca et al. (2016) in which an allocation of VM to physical machines is considered, all extending GAP models.
O	We point to Morales and Romeijn (2005) for a detailed methodological review on the GAP and its extensions.
N	In Freling et al. (2003) the authors face a single-source allocation problem with a flexible model and an effective Branch-and-Price algorithm; however, their model does not allow to handle limited capacity, which is a crucial feature in our application.
N	The multiperiod allocation problem discussed in Murthy and Seo (1999), in which a dual ascent technique from Murthy (1993) is adapted to a telecommunication networks applications, is similarly missing the feature of handling limited capacities.
O	Such a strategical decision belongs to the realm of infrastructural design and, as we extensively discuss in Ceselli et al. (2017), requires substantially different models and solution techniques.
O	At the same time, one may expect features and computational challenges similar to those of multi-period location problems (Nickel and da Gama, 2015).
O	Recent approaches on that field include (Gourdin and Klopfenstein, 2008): the authors face a multi-period concentrator location and dimensioning problem, providing MILP formulations and reduction techniques, and solving to optimality in less than one hour of computation instances with up to 30 clients, 10 candidate location sites and 15 time periods, or 100 clients, 30 candidate locations and 5 time periods.
N	In Castro et al. (2017) the authors introduce exact methods for a capacitated multi-period facility location problem in which however, unlike our case, the demand of each client can be fractionally served by multiple facilities.
O	We finally mention the recent contribution of Halper et al. (2015), where the authors propose MILP formulations and local search heuristics for an uncapacitated pmedian location problem involving two periods: in the first the location of facilities is given, while in the second it can be changed at a price.
O	We provide an example of temporal clustering analytics, which builds on the methodology of Furno et al. (2017b), in Section 6.1.
O	Following the Dantzig–Wolfe reformulation principle (Wolsey, 1998), let Pi = {(xt ik, yt ikl) : (3),(4),(5),(6),(7)} represent the convex hull of the feasible region respect to constraints (3)–(7).
O	The remaining is basically a network flow matrix, which is known to be totally unimodular (Wolsey, 1998).
O	We implemented our algorithms in C++, using CPLEX 12.6 2013 to solve the master LP subproblems, running tests on an Intel i7 4 GHz workstation equipped with 32GB of RAM.
O	We have access to a dataset of real-world mobile traffic demands (Barlacchi and et al., 2015), encompassing two months with a time granularity of fifteen minutes. 
O	Then, we create ten clusters of access points using a standard k-means model, taking as input the euclidean distances between APs, optimizing it with the classical heuristics of Hartigan and Wong (1979).
O	Let d (resp. d¯ ) be the minimum (resp. maximum) demand observed in any AP and time slot in Barlacchi and et al. (2015).
O	We choose a single day at random from the two months included in the dataset (Barlacchi and et al., 2015), we perform a direct query to the demand of each AP at each time slot in that day, and then we perturb all demands with noise, uniformly drawn at random in the interval [−5%, +5%].
O	Dataset C is obtained by considering a random week taken from the dataset Barlacchi and et al. (2015), and merging the time-slots in either 168 slots of 1 h each (1 h), 84 slots of 2 h (2 h), 56 slots of 3 h (3 h), 42 slots of 4 h (4 h) or 38 slots obtained by the clustering methods described in Section 6.2 (clust).
O	To this end, we rely on the complete real-world dataset in Barlacchi and et al. (2015), and run actual analytics on it so as to generate the demand profiles.
O	The first approach we take in order to infer demand profiles exploits a well-known property of mobile traffic, i.e., its weekly periodicity (Keralapura et al., 2010).
P	We experimented on such an option, adapting the temporal clustering solution presented in Furno et al. (2017b) to our needs.
P	we perform two separate hierarchical clusterings, respectively using the total volume and normalized distance metrics introduced in Furno et al. (2017b);
O	Our source code is released under GPL license and available online (Ceselli et al., 2018), together with our test instances.
O	The project has been partially funded by Università degli Studi di Milano (Piano Sostegno alla Ricerca 2015-2017) and Regione Lombardia - Fondazione Cariplo, grant n. 2015-0717, project REDNEAT.
P	To do so, several authors, such as Dauzère-Pérès and Lasserre (2002) and Gaudreault et al. (2011), point out the importance of integrating operational scheduling decisions with tactical planning decisions.
O	More recently, Yugma et al. (2015) show the opportunities related to integrating scheduling and process control in semiconductor manufacturing.
O	Scheduling all jobs in a semiconductor manufacturing facility is so complex that the problem needs to be decomposed, i.e. jobs are scheduled in each workshop separately (see Moench et al. (2011)).
O	Forcinstance, Yugma et al. (2012) and Jung et al. (2014), and more recently Knopp et al. (2017), consider scheduling problems in the cleaning and diffusion workshop, while Rotondo et al. (2015) consider the scheduling of jobs on wet-etch tools.
P	Scheduling approaches for this workshop have been proposed for instance in Cakici and Mason (2007) and Bitar et al. (2016).
O	APC is usually associated with the combination of Statistical Process Control (SPC), Fault Detection and Classification (FDC), Run to Run (R2R) control, and more recently Virtual Metrology (VM) (see for instance Moyne et al. (2000)).
O	Although usually studied separately, scheduling and APC are actually often related in semiconductor manufacturing (Yugma et al., 2015).
O	As shown in the survey paper of Tan et al. (2015), R2R control is becoming critical in high-mix semiconductor manufacturing processes.
O	A R2R controller is often associated with each machine and each job family, and uses data from past process runs to adjust the settings of the machine for the next run (see for example Musacchio et al. (1997) or Jedidi et al. (2011)).
O	Hence, as presented in Obeid et al. (2014), an additional time constraint is defined on the scheduling problem to impose that the execution of two jobs of the same family lies within a given time interval on the same (qualified) machine.
N	Note that defining the right qualifications of job families to machines, studied for instance in Johnzén et al. (2011) and Rowshannahad et al. (2015), is outside the scope of this paper.
O	It is also important to note that the time constraints considered in this paper are different from the time constraints, also called time windows or maximum time lags, studied for instance in Wu et al. (2010), Klemmt and Moench (2012) and Sadeghi et al. (2015).
O	Li and Qiao (2008) and Cai et al. (2011) study related problems, except that they allow qualification procedures to be performed, the number or the type of machines is different and the threshold is expressed in number of jobs instead of in time. 
O	The scheduling problem addressed in this paper has been studied in Obeid et al. (2014), where two Integer Linear Programs (IP1 and IP2) and two constructive heuristics are proposed.
P	Finally, two heuristics are introduced that improve the solutions obtained in Obeid et al. (2014).
O	This section starts by briefly recalling the two Integer Linear Programs ((IP1) and (IP2)) presented in Obeid et al. (2014).
P	The two models described in Obeid et al. (2014), as well as the model proposed in this paper, are based on time-indexed variables.
P	To solve larger instances, a new model, called (IP2), is introduced in Obeid et al. (2014).
O	Finally, Constraints (8) are the main difference between the model presented in this paper and (IP2) in Obeid et al. (2014).
O	In the last 20 years, some methods based on artificial intelligence techniques have been successfully used to deal with different classes of scheduling problems, and in particular Constraint Satisfaction (CS) (Brailsford et al., 1999).
O	The implementation of algorithms able to solve CS problems is known as Constraint Programming (Van Hentenryck, 1999).
O	To date, there are several CP approaches that have been successfully employed to tackle scheduling problems in manufacturing environments, such as batch plants (see Jain and Grossmann (2001) and Maravelias and Grossmann (2004)).
P	Two constructive heuristics are presented in Obeid et al. (2014
P	These heuristics are not described in this paper and the reader is referred to Obeid et al. (2014) for more details.
P	Simulated Annealing (SA) belongs to the class of randomized local search algorithms and was developed by Kirkpatrick et al. (1983) to handle hard combinatorial problems.
P	SA has demonstrated its ability to solve scheduling problems (Teghem, 2002).
O	Finally, following the recent work of Kao et al. (2018), considering the dynamic status of machines ("equipment health") seems very relevant to allow a machine to "lose" its qualification depending on its status.
O	From a broader perspective, defining the "critical" qualifications of job families to machines to maintain in a schedule, based on the work on flexibility measures in Johnzén et al. (2011) and Rowshannahad et al. (2015), should be appealing for production managers.
O	Vadlamani et al., Prasad and Thuente, and Mpitziopoulos et al. present detailed overviews of various types of jammers and commonly used jamming techniques and strategies.
P	Whitaker and Hurley and Chapman et al. emphasize that building an effective and efficient radio communication network that can maintain the minimum level of desired signal on each receiver depends mainly on the locations of the transmitters.
O	Among these, Commander et al. and Commander et al. determine the minimum number of jammers and their locations to obtain the desired effect.
O	Feng et al. consider the location of jammers that will partition the communication network into disconnected components.
P	Vadlamani et al. find out not only the locations but also the channel hopping strategies in order to minimize the expected throughput of the opponent’s communication network. 
O	Additionally, Vadlamani et al. deal with the location of jammers under flow-jamming attacks.
P	The defender in the second stage, investigates the best strategy to optimize the flow of information after observing the location strategy of the attacker by solving the Simultaneous Routing and Resource Allocation (SRRA) problem of Xiao et al.
P	Nicholas and Alderson are the first to apply the tri-level game theoretic optimization framework to design wireless mesh network topologies that are robust to jamming. 
P	The authors extend Shankar’s work by considering a continuous space for jammer locations.
N	Contrary to the mentioned works that consider locating facilities under deterministic conditions, Daskin and Batta et al. maximize the expected coverage by considering the probability that a facility may not be able to serve a demand point.
P	Similarly, Patel et al. determines locations of sensors over a time horizon to maximize the expected coverage of data by considering the probability of a link failure.
O	Detailed information for existing solution methods for BPPs can be found in surveys by Labbe, Colson et al. , Dempe and in textbooks by Dempe and Bard.
O	A similar approach under a different context is used by Alekseeva et al.
O	Hua and Elsayed (2016) model the UAVs as systems with spatially distributed units and obtain the reliability metric for UAVs with one level of rotors.
O	For example, Mueller and D'Andrea (2014) introduce a possible equilibrium for quadcopter (a four-rotor helicopter that is lifted and propelled by four rotors) with two opposing failed propellers: two remaining opposing propellers produce equal thrust force and UAV rotating about vertical axis.
P	Moreover, based on control reallocation scheme proposed by Marks et al. (2012), octocopter can maintain stability when up to four rotors (two pairs) fail.
P	Salvia and Lasher (1990) introduce a two-dimensional consecutive k n -out-ofsystem.
P	Zuo (1993) proposes a rectangular and cylindrical two-dimensional consecutive k n -out-ofsystem.
O	Boushaba and Ghoraf (2002) consider a three-dimensional system fails when units in a cubic grid of size k fail. 
O	For example, Habib et al. (2010) consider that the system fails when the total number of failed units exceeds the maximum number allowed or when a consecutive (r,s)-out-of-(m,n) units fail.
O	More complex configurations are explored by Akiba and Yamamoto (2001), Chang and Huang (2010), Mahmoudboushaba and Zinebazouz (2011) and Gharid et al. (2010, 2011).
P	Sarper (2005) firstly discusses the balance requirements for UAVs (Descent Systems of Planetary Vehicles).
P	Hua and Elsayed (2016) develop a higher balance requirement for one-level UAVs.
O	The reliability estimation and analysis of multi-level UAVs when rotors rotate in the same direction is discussed in Guo and Elsayed (2018).
O	Achtelik et al. (2012) analyze the control reallocation strategy when one rotor fails in a quadrotor, hexagon-shaped hexacopter and the triangle-shape two-level helicopter. 
N	Based on the controllability testing algorithm introduced by Du et al. (2015), for an PPNNPN hexacopter as shown in Figure 3, the system is controllable when it is balanced and 2-out-of-3 pairs of rotors are operating properly. 
N	This waiting time reduces the probability for customers making orders online (Lowe et al. 2014).
O	Many major e-commerce companies such as Amazon, Alibaba, or JD already offer SDD in many cities worldwide, often within a few hours after order (Stevenson and Black 2018).
O	However, companies struggle to develop cost-efficient SDD-delivery concepts (Ram 2015).
O	As already experienced in regular parcel delivery, the “last-mile” causes the majority of delivery costs (Bernau et al. 2016).
N	As previous studies show, delivery vehicles are usually not able to serve more than 30 customers per day (Ulmer 2017a, Klapp et al. 2018a, Voccia et al. 2019).
O	An alternative way of delivery are pickup stations (Savelsbergh and Van Woensel 2016).
P	Currently, Amazon is developing an own network of pickup stations (Lunden 2017). 
P	Pickup stations have several advantages for customers (Vakulenko et al. 2018, Yuen et al. 2018):
P	Furthermore, customers are flexible when to pickup their goods and do not need to wait at home for a delivery to arrive (Verlinde et al. 2018).
O	Many companies invest substantially in both autonomous ground vehicles and autonomous air vehicles (drones) (McFarland 2017, Condon 2019, Mitchell 2019).
N	One major issue is that in the foreseeable future, not every street or region in the city may be eligible for autonomous travel, especially, for (not fully) autonomous vehicles of the SAE-levels 3 and 4 (SAE International 2016, NHTSA 2017). 
O	Instead, autonomous vehicle routing will be limited to standardized operations in selected streets (Beirigo et al. 2018, Scherr et al. 2018, 2019).
O	Furthermore, the “last-meter” delivery process to the customer is still an unresolved issue for both autonomous ground and air vehicles (Kolodny 2017).
O	In Heutger and Kuckelhaus (2016), DHL ¨ recently stated that “Machine-to-person handover of parcels and letters may be beyond the current capability of autonomous technologies [...]. Instead, machine-to-parcel station handover is certainly achievable.”
O	The loading and delivery environment can be controlled and the processes can be automated (Haas and Friedrich 2017).
O	There are already several concepts for delivery to parcel stations with autonomous ground vehicles (Heutger and Kuckelhaus 2016, Tipping and Kauschke 2016).
P	DHL delivers goods by drones to some parcel stations (DHL 2016) and in Singapore, the SKYWAYS system for delivery to parcel stations by drones is planned to be implemented soon (Chong 2017).
O	imultaneously, drones are developed that are able to carry many parcels at once, especially beneficial for consolidated delivery to parcel stations (Murphy 2017).
O	A PFA usually bases on a practical common-sense idea (Powell 2011).
O	Most of the pickup stations are located at large streets, many on the main ring road of Braunschweig where (supervised) autonomous vehicles already drive successfully since 2010 (Nothdurft et al. 2011).
O	The most prominent example for this problem class is the “flying sidekick”-TSP (Murray and Chu 2015, Agatz et al. 2018, Carlsson and Song 2017, Ha et al. 2018).
O	In Cheng et al. (2018), drones can serve several customers within one trip. 
P	Arbanas et al. (2016, 2018) consider other extensions of the flying sidekick-TSP, where several autonomous ground vehicles are coordinated to support drones in their delivery operations.
O	Ulmer and Thomas (2018) suggest dynamic delivery with separate fleets of drones and vehicles.
O	Dayarian et al. (2018) use drones to dynamically resupply conventional delivery vehicles on the road.
O	However, there is some work on dynamic routing of autonomous vehicles for other purposes than delivery, for example Bullo et al. (2011) and Basilico et al. (2016).
O	Bullo et al. (2011) dynamically routes robots to conduct services in an area. 
O	Basilico et al. (2016) addresses a problem where drones dynamically patrol in a certain area.
O	Early work is presented by Azi et al. (2012).
O	To determine suitable trips, they apply the multiplescenario approach (MSA) by Bent and Van Hentenryck (2004).
N	In contrast of the SDDPSAV, Azi et al. (2012) assume that customers can be rejected.
O	Voccia et al. (2019) use the MSA to analyze waiting at the depot for consolidation. 
O	Ulmer et al. (2018) shift the computational burden to an offline learning phase by means of value function approximation (VFA).
O	For a single vehicle, Ulmer et al. (2018) analyze the value of preemptively returning to the depot to pick up more goods. 
O	Ulmer (2017b) uses VFA to price different delivery deadlines based on the current workload of the vehicles.
O	Dynamic dispatching problems are considered by Klapp et al. (2018a,b), van Heeswijk et al. (2019), and Rivera and Mes (2017).
P	Klapp et al. (2018a,b) split the time horizon into hours.
P	The work by van Heeswijk et al. (2019) considers the challenge of dynamically dispatching goods from a consolidation center to a set of locations.
O	Similar to van Heeswijk et al. (2019), Rivera and Mes (2017) use dynamic dispatching for freight selection.
O	Furthermore, our PFA addresses the tradeoff between fast dispatches and consolidation observed in Rivera and Mes (2017).
O	For example, flexibility in the delivery location is also addressed in problems such as the close-enough traveling salesman problem where customers can be served within a radius around their locations, for example, for meter reading (Carrabs et al. 2017).
O	Other research addresses flexibility by roaming delivery locations over the course of the day, for example, at work or at home (Reyes et al. 2017).
O	The question whether to wait or leave is also addressed in dynamic queuing problems, for example, for salespersons or at taxi stands (Zhang et al. 2018).
O	The phenomenon of stochastic customer pickup is related to dynamic inventory routing problems, where goods are consumed over time (Coelho et al. 2014).
O	Other related work is vehicle routing with release dates where goods become subsequently available at the depot (Archetti et al. 2015).
O	These problems have uncertainty in customer orders and in the ready times at the restaurant (Ulmer et al. 2017).
O	Finally, the direct dispatching from a warehouse to a set of stations shows similarity to the first level of two-echelon routing from warehouses to satellites (Zhou et al. 2018) and also part of the general concept of a physical internet (Crainic and Montreuil 2016).
P	With scarce resources, consolidation becomes important (Ulmer et al. 2017).
O	The depot is set to the DHL delivery base in Braunschweig and the pickup stations represent the “DHL Packstation” scheme in Braunschweig (Persiel 2017).
O	As an example, autonomous vehicles of SAE-level 3 have driven on the main ring road of Braunschweig since 2010 (Nothdurft et al. 2011).
P	To search the space of policies, we therefore adapt the method introduced in Brinkmann et al. (2019).
P	For updates and exploration, we draw on the same tuning as presented in Brinkmann et al. (2019).
O	This is significantly more than the usual number of deliveries of about 20 to 30 reported in the same-day delivery literature (Ulmer 2017a).
P	Depending on their ports of origin and destination, ships can save on average 463 km compared with going around the Jutland Peninsula (Denmark), cf. WSV (2018).
O	It might also come at lower total travel cost if - depending on fuel price levels - savings in bunker cost outweigh the transit charge that has to be paid for using the canal, see Heitmann et al. (2013).
P	Optimizing traffic decisions for the Kiel Canal has been investigated so far by Lübbecke (2015) and Lübbecke et al. (2018).
O	The base problem and MIP model of Lübbecke (2015) and Lübbecke et al. (2018) are extended by (1) deciding on speeds of ships rather than assuming constant given speeds, (2) restricting waiting times to guarantee maximum transit times for ships, and (3) capturing limited capacities of siding segments.
O	There, decisions are made on when to use a lock chamber for ships traveling up- or downstream and on which ships to handle in each lock-operation-cycle such that the maximum or total ship waiting time is minimized, see e.g. Campbell et al. (2007), Smith et al. (2009), Verstichel et al. (2015) and Passchyn et al. (2016).
O	Although the Kiel Canal has locks at both ends too, the locks are operated independently of the traffic management, see Luy (2011).
P	The early paper of Griffiths (1995) provides queuing models for the Suez Canal taking into account the canal’s layout and the used convoy system.
P	Ulusçu et al. (2009) provide an optimization model for determining the order in which ships go through the Strait of Istanbul.
P	Sluiman (2017) generalizes the research of Ulusçu et al. (2009) by providing an optimization model, a greedy-rule method, and improvement heuristics for scheduling ships for a waterway with a single narrow segment.
O	Ship scheduling for a single bi-directional channel segment is also investigated in Zhang et al. (2017).
O	Lalla-Ruiz et al. (2018) present a MIP model and a Simulated Annealing heuristic for scheduling ships that have to pass the Yangtze river delta for accessing the ports of Shanghai.
P	The Yangtze river is furthermore investigated by Tan et al. (2018) who design a schedule for ships that visit different ports along that river.
O	Righini (2016) investigate a network of bi-directional and one-way inland waterways in northern Italy.
O	Zhang et al. (2016) consider a single navigation channel where ships have to be scheduled such that a berth is available when they enter the port through the channel.
O	Zhen et al. (2017) focus on berthing decisions where the navigation channel is subject to tide cycles.
O	Jia et al. (2018) schedule in- and out-going ships for a terminal with respect to a limited anchorage area within the terminal.
O	Corry and Bierwirth (2018) combine traffic management for a navigation channel with berth allocation, where berths are modeled as additional channel segments.
O	The corresponding planning problem has been investigated by Lübbecke (2015) and Lübbecke et al. (2018).
O	The problem considered here also has similarities to scheduling trains on a single rail track, where opposing trains can only meet at siding tracks or at stations, see Lusby et al. (2011) for a literature review.
P	Works in this field like Szpigel (1973), Kraay et al. (1991), Kraay and Harker (1995), Higgins et al. (1996), Zhou and Zhong (2007), Castillo et al. (2011), Lamorgese and Mannino (2015) and Gafarov et al. (2015) propose exact and heuristic solution methods for various settings that differ in requirements on train arrival and departure times, dwell times at stations etc.
P	This section describes the mathematical model for the basic version of the planning problem that reflects the current decision making policy of the canal authority, see also Lübbecke et al. (2018).
P	A similar approach for speed optimization has been proposed by Andersson et al. (2015).
N	These requirements are mentioned in Lübbecke et al. (2018) but not considered any further.
O	Minimum infeasible subsets appear in many MIP models and solution methods, for example in Combinatorial Benders’ Decomposition (see e.g. Codato and Fischetti (2006) for a general analysis and Verstichel et al. (2015) for an application to lock scheduling).
O	They are also known as “minimal covers” in the context of knapsack cover inequalities for MIP models, see e.g. Gu et al. (2000).
O	In other words, the waiting position of a ship is not determined in the model, which is different to Lübbecke et al. (2018) who also define a position for a waiting ship.
P	For solving the base problem without extensions, Lübbecke et al. (2018) propose a labeling algorithm that, for a given fixing of all z-variables, determines segment entering times and waiting times for all ships.
O	In single track train scheduling, a common solution approach is to first solve a relaxed problem that typically ignores all potential conflicts and then gradually resolve the occurring conflicts to obtain a feasible solution to the overall problem, see e.g. Szpigel (1973), Higgins et al. (1996), Kraay et al. (1991), Kraay and Harker (1995), Zhou and Zhong (2007) and Castillo et al. (2011).
P	For example, Higgins et al. (1996) and Zhou and Zhong (2007) solve a relaxed problem and then iteratively resolve the earliest conflict in a Branch-and-Bound scheme.
O	For example, when setting  = 1 (i. e. only one infeasibility is resolved per iteration) and ignoring the two speedup mechanisms, the procedure would resemble the Branch-and-Bound schemes of Higgins et al. (1996) and Zhou and Zhong (2007).
O	This is far beyond what is relevant for a planner in practice, but Lübbecke et al. (2018) considered such instance size for their local-search approach and, thus, it is tested here if the matheuristic can handle this too.
N	A test with the linear relaxation of the ui,s,v-speed variables as proposed by Andersson et al. (2015) and discussed in Section 3.4 was also onducted but did not reveal an advantage.
N	For models ‘base’ and ‘capacity’, runtimes are about a quarter of a minute which is as fast as the approach of Lübbecke et al. (2018).
O	The reader is referred to Altay and Green, Caunhye et al.alindo and Batta, Hoyos et al, and the references therein for the details.
P	The problems that were observed in 1999 Marmara Earthquake motivated several studies, such as JICA report, Görmez et al, Kılcı et al, and Cavdur et al.
O	In the literature, this phenomenon of having consecutive disasters is called multi-hazard, which is defined as the combination of various hazards in a defined area (Kappes et al.).
O	While reviews by Owen and Daskin and Current et al. examine both deterministic and stochastic facility location models, Snyder and Caunhye et al. discuss only stochastic nature of facility location problems and all agree that the complexity of location problems are captured best by stochastic modeling.
O	With an enormous literature on facility location, the application of those models to humanitarian logistics is abundant (see, e.g. Altay and Green, Simpson and Hancock, Galindo and Batta), especially with an emphasis on humanitarian logistics, as can be seen in Özdamar et al, Kovács and Spens, and Leiras et al.
O	The review papers by Ortuño et al, Liberatore et al, and Grass and Fisher indicate the essence of the effects that stochasticity creates in humanitarian logistics.
P	Bayram et al. and Kongsomsaksakul et al. propose models to minimize the total evacuation time by locating shelters and assigning evacuees to shelters.
P	“Green routing” is a concept first introduced by Kara et al. (2007) by observing the fact that “cost” is not usually directly proportional to distance traveled but also the load of the vehicle.
P	Bektas¸ and Laporte (2011) introduce the Pollution-Routing Problem (PRP) with a more accurate fuel consumption model that considers speed and load as decissions.
O	Variants of the PRP have since been studied, such as time-dependency (Jabali et al., 2012), with backhauling (Ubeda et al., 2011), with pickup and delivery (Oberscheider et al., 2013) and with inventory considerations (Mirzapour Al-e hashem and Rekik, 2014).
P	For comprehensive survey on green routing and green logistics, we refer the reader to Sbihi and Eglese (2010), Dekker et al. (2012), Lin et al. (2014), Demir et al. (2014b) and Eskandarpour et al. (2015), and to Ubeda et al. (2011), Figliozzi (2011) and Varsei and Polyakovskiy (2017) for case studies.
O	The Location-Routing Problem (LRP) is a generalization of the vehicle routing problem that exploits the interdependency between the location decisions of facilities and routing decisions of vehicles, as it has been shown that making the two decisions independently can result in suboptimal solutions (Salhi and Rand, 1989). 
O	The basic LRP involves locating facilities and routing a fleet of vehicles from the facilities to serve a given set of customers, with the aim of minimizing the cost of location and routing (see, e.g., Prodhon and Prins, 2014; Drexl and Schneider, 2015, for comprehensive surveys).
O	The first one is by Govindan et al. (2014), who describe a bi-objective two echelon LRP with time windows which arises in a perishable food supply chain network with manufacturers, distribution centers and retailers.
P	The second study is by Koc¸ et al. (2016) in which the authors analyze the impact of location, fleet composition and routing on emissions in urban freight transportation.
P	The authors present a location-routing problem with heterogeneous fleet of vehicles in a city logistics concept, which use the comprehensive modal emission model (CMEM) proposed by Scora and Barth (2006), Barth et al. (2005), Barth and Boriboonsomsin (2008) to calculate emission.
P	Toro et al. (2017) study a bi-objective green capacitated locationrouting problem with two objective functions that minimize the operational cost and fuel consumption and CO2 emission.
P	Tricoire and Parragh (2017) present a green city hub location routing problem with heterogeneous fleet and with two objectives; to minimize the total cost and minimize CO2 emissions.
P	In a recent study, Khoei et al. (2017) propose a green Weber problem and its time-dependent version, where the authors combine the location decision of a single facility and the speed decisions of the vehicles sent to customers to minimize the total amount of CO2 emissions.
O	Similar to other related studies (Bektas¸ and Laporte (2011), Demir et al. (2012), Demir et al. (2014a)) that use CMEM as the emission model, we also treat speed and load as decision variables in order to estimate the emissions more accurately.
P	The CMEM was proposed by Scora and Barth (2006), Barth et al. (2005), Barth and Boriboonsomsin 2008) in order to estimate fuel consumption for heavy-goods vehicles.
P	Similar inequalities to VI 3 were first proposed by Achuthan et al. (2003) for a vehicle routing problem, and later adapted for a location-routing problem by Karaoglan et al. (2012). 
P	This second subproblem is the speed optimization problem introduced by Demir et al. (2012) for road transportation.
P	In order to solve the SOP, we use the speed optimization algorithm (SOA) which was first proposed by Norstad et al. (2011) and Hvattum et al. (2013) for maritime transportation.
O	The algorithm was first adapted by Demir et al. (2012) for ground transportation.
O	The authors stated that the algorithm finds the optimal solution due to convexity of the objective function (Hvattum et al., 2013).
P	We adapt the algorithm proposed by Demir et al. (2012) to our problem.
N	The main difference of our algorithm compared to the one proposed by Hvattum et al. (2013) is while calculating the vehicle speeds we consider the time windows, the minimum and maximum speed limits and the optimal speed (v*).
P	Kramer et al. (2015) developed a speed and departure time optimization algorithm, which is very similar to the algorithm proposed by Demir et al. (2012).
O	Ninety percent of the world’s goods are transported by ships, and without shipping, “half the world would starve and the other half would freeze!” due to the lack of affordable alternatives for food and goods transport (ICS, 2017).
O	On the quayside of a container terminal, there are a number of routine operational problems to address (Zeng et al., 2015).
N	The QCSP is the most complex quayside operational problem in a container terminal (Al-Dhaheri and Diabat, 2017).
O	With a view to reducing the complexity of the problem, researchers have considered the QCSP by including only the essential considerations (Diabat and Theodorou, 2014).
O	They include imposing the vessel stability constraint (Al-Dhaheri and Diabat, 2017; Wang et al., 2013), the yard congestion constraint (Choo et al., 2010), integrating berth allocation with crane assignment (Abou Kasm et al., 2019; Meisel and Bierwirth, 2013; Türkogulları et ˘ al., 2016), and integration of truck scheduling (Tang et al., 2014).
P	The origins of QCSP in mathematical programming can be traced to when it was first introduced by Daganzo (1989).
O	The problem was shown to be NP-complete (Lee et al., 2008; Türkogulları et ˘ al., 2016). 
P	Due to its intractability, many researchers have attempted to solve QCSP using heuristics– see Kim and Park (2004), Kaveshgar et al. (2012), Rodriguez-Molins et al. (2014), Legato and Trunfio (2014), Guo et al. (2014), Al-Dhaheri et al. (2016), and Zhang et al. (2017).
O	Others, including Peterkofsky and Daganzo (1990), Zhu and Lim (2006), Choo et al. (2010), Guan et al. (2013), Chen et al. (2014), and Al-Dhaheri and Diabat (2015) have applied exact solution methods, such as the B&P algorithm, and commercial software to solve mathematical formulations of QCSP.
O	Extensive surveys and classifications of QCSP can be found in the work of Bierwirth and Meisel (2015) and Boysen et al. (2017).
N	Specifically, the model introduced by Daganzo (1989) does not consider the non-crossing constraint.
P	The same model was solved by Peterkofsky and Daganzo (1990) using a Branch-and-Bound (B&B) algorithm for large-sized instances.
O	A classification is given by Parragh et al.
P	Cort`es et al. presented this problem as a generalization of the Pickup and Delivery Problem (PDP) which can allow to reduce costs comparing to the PDP.
P	The classic PDP has been well studied and surveys have been proposed by Berbeglia et al. and Parragh et al.
O	The possibility to transfer shipments from a vehicle to another in a PDP is first mentioned by Shang and Cuff.
P	Another formulation was proposed by Cort`es et al.
O	Masson et al. presented a special case of the PDPT called the Pickup and Delivery Problem with Shuttle routes (PDPS).
P	An insertion-based heuristic was also proposed by Thangah et al.
P	A Variable Neighbourhood Descent (VND) was proposed by Tchapnga Takoudjou et al. to solve the PDP.
O	An adaptive LNS was used by Masson et al.
P	To check the route feasibility, we use the algorithm described by Masson et al.
P	LNS were already proposed to solve the PDPT by Qu and Bard and Masson et al.
O	The columns ”QB”, ”MLP” and ”LNS” represent respectively the algorithms from Qu and Bard, from Masson et al, and the one presented in this paper.
O	Both algorithms (LNS and GA) are tested on PDPT benchmarks used by Qu and Bard and Mitrovi´c-Mini´c and Laporte.
O	The value of the parameters β1, β2, β3 are the same as Qu and Bard and the value of nbIterM ax is the same as Masson et al.
O	As far as we know, these benchmarks have been used only by Mitrovi´c-Mini´c and Laporte and Masson et al.
O	The results are compared with the results from Masson et al.
O	The ”Masson et al.” column contains the cost of the solutions found by Masson et al. for these instances.
P	Gabrel et al. applied the framework to solve a location transportation problem.
N	In this Section, we present a solution approach based on the method used by Thiele et al.
O	As identified in O’Kelly and Miller, such a network topology is desirable because it reduces and simplifies network construction costs, centralizes commodity handling, and allows carriers to take advantage of economies of scale through the consolidation of flows.
P	Nagy and Salhi present a mathematical programming formulation for the HLRP where the vehicles are allowed to do the pickup and delivery on separate routes. 
O	Z¨apfel and Wasner consider the problem of planning and optimizing the hub-and-spoke transportation networks for cooperating third-party logistics providers.
O	C¸ etiner et al. address a hub location and routing problem under a multiple allocation setting for the Turkish postal services.
O	The single allocation version of a similar problem is considered by Camargo et al., where an upper bound is imposed on the lengths of the tours made by the vehicles in order to ensure service quality.
O	Rodr´ıguez-Mart´ın et al. address an HLRP where exactly p hubs must be installed and thus the fixed costs for using a location as a hub are disregarded.
O	Rieck et al. consider a generalized HLRP with multiple products and the possibility of direct shipments between pickup and delivery locations.
P	Beardwood et al. develop an analytical expression to estimate the distance covered by a traveling salesman in an area with a uniform density of destinations.
P	Distance approximations for multiple stop routes are developed by Christofides and Eilon, Eilon et al., and Daganzo.
P	Langevin et al. present a detailed review of applications of the CA modeling framework in freight distribution up to 1996.
P	Jabali et al. address the fleet composition problem, which is a variant of the vehicle routing problem where the CA is used for assessing the routing cost.
P	Yang et al. solve a fuzzy p-hub center problem by using an improved hybrid PSO algorithm and combining PSO with genetic operators and local search (LS) to update and improve particles for the subproblems.
P	Bailey et al. propose a PSO algorithm for the uncapacitated single allocation hub location problem (USAHLP).
P	A notable exception to the standard problem described above is the work presented by Lei et al. in the context of chemical industries.
N	The work of Amorim et al. does not propose a specific solution method for the problem, but includes a comparison of two different formulations with sequencing decisions in the context of perishable goods.
P	More recently, Miranda et al. formulated a MIP and proposed several relax-and-fix (RF) heuristics to solve a particular PRP in small furniture companies.
O	Miranda et al. extended their previous work in furniture companies by considering a more general situation with sequence-dependent setup times on the production line and a limited fleet of heterogeneous vehicles for distribution.
O	From the production planning point of view, the work of Miranda et al. represents a scenario wherein the production line corresponds to a liquid painting line, which does not require specialized equipments and has negligible setup times.
N	On the other hand, Miranda et al. address a scenario with a powder painting line, which provides superior quality finishing but requires highly specialized equipments and time-consuming sequencedependent setup operations.
N	In this paper we study the same scenario addressed by Miranda et al., which, besides sequence-dependent setup times and a heterogeneous fleet of vehicles, includes producing multiple items needed to assemble different final products, routes that may span one or more periods, multiple time windows and customer deadlines, among others.
P	Computational experiments reported by Miranda et al. showed that feasible solutions can be found for instances with up to 15 customers in reasonable computing times by using a general-purpose solver and, therefore, efficient solution methods are necessary to solve larger instances.
P	Our heuristic is inspired by the work of Absi et al. and decomposes the problem into two subproblems, which are then solved iteratively until a given stopping criterion is met.
O	This formulation was recently proposed by Miranda et al.
O	Similar ideas have recently been used by Absi et al. and by Chitsaz et al.
P	The idea of using local branching constraints as a diversification mechanism has been successfully explored by Adulyasak et al. and Chitsaz et al. to solve a single item PRP.
O	Algorithm 2, based on Absi et al., shows how to update both cir and ¯τir, respectively (Algorithm 1, line 19).
O	Note that in Absi et al.only the cost needs to be updated, whereas in our problem we need to update the approximation for both the distribution cost and time.
P	This idea has been recently explored by Caceres-Cruz et al. to solve the heterogeneous fixed fleet vehicle routing problem with multi-trips.
P	Additionally, we also generated a set of random instances following a similar approach to the one proposed by Gramani et al., for the cutting stock problem, and Armentano et al., for the multi-product PRP.
O	The overall production capacity usage is defined to be around 60%, ollowing the work of Amorim et al.
P	In this section we compare our heuristic with a previous approach proposed by Miranda et al. for a PRP in small furniture companies.
P	Lamothe et al. extended Yih’s study to be a job-shop case, but their proposed branch-and-bound dynamic algorithm can only deal with a single new job for each time.
O	To deal with multiple new jobs, a mixed integer programming (MIP) model is formulated by Zhao et al., which completely reschedule all the remaining and new jobs in a robotic cell for obtaining an optimal new schedule. 
N	Feng et al. formulate a more complicated MIP model to deal with a special case where the robotic cell is equipped with multi-capacity reentrant workstations.
O	Che et al. treated the small perturbations of the robot’s fixed routes and moving times in their proactive scheduling problem by developing a bi-criteria optimization algorithm to minimize the cycle time and maximize the stability of the new schedule. 
O	Yan et al.considered the limited adjustment to the existing schedule to trade-off the efficiency and stability of the rescheduling. 
O	Chauvet et al.kept the existing schedule unchanged and inserted a single new job’s processing stages and transportations into the available time intervals of the workstations and the robot, respectively.
P	Yan et al. extended Chauvet et al.’s algorithm to construct a local feasible schedule with a given inserting sequence of new jobs.
O	As addressed in Zhao et al., when new jobs arrive at the robotic cell at time point 𝑇 , the robot may be transporting an existing job from a workstation to the next one.
O	Hybrid flow shop (HFS) is a generalization of flow shop where a stage can have two or more parallel machines (Pinedo, 2012).
O	The two-stage HFS problem is already NP-hard on minimizing the makespan (Gupta, 1988), multistage HFS scheduling problems with additional system assumptions are NP-hard in strong sense.
O	Recent and comprehensive reviews on the HFS scheduling problems are available in Ruiz and V´azquez-Rodr´ıguez (2010) and Ribas et al. (2010).
O	On the other hand, most HFS studies focus on problems with throughput related measures, such as minimizing makespan or mean flow time (Choi et al., 2005; Ruiz and V´azquez-Rodr´ıguez, 2010).
O	The scheduling problem can be denoted using a triplet α|β|γ notation derived from Vignier et al. (1999) and Ruiz and Rodriguez (2010).
O	Xiao et al.(2000) applied a basic GA to minimize makespan in a basic m-stage HFS. 
P	Engin et al. (2011) implemented a new mutation operation able to make use of critical job information to obtain high quality neighborhoods.
O	In some studies, like Tavakkoli et al.(2009) and Chamnanlor et al. (2015), different search techniques are incorporated into the GA framework to improve the algorithm performance.
O	These include well-known algorithms like simulated annealing (Naderi et al., 2009), tabu search (Wang and Tang, 2009), particle swarm optimization (Liao et al., 2012), ant colony system (Ying and Lin, 2006), immune evolutionary algorithm (Zandieh et al., 2006), iterated greedy algorithm (Urlings et al., 2010), artificial bee colony (Cui and Gu, 2015; Li et al., 2016). 
P	In recent years, there emerges a trend to apply new metaheuristics like water flow-like algorithm (Pargar and Zandieh, 2012), firefly algorithm (Marichelvam et al., 2014a), cuckoo search algorithm (Marichelvam et al., 2014b) to solve HFS problems.
P	Adler et al.(1993) developed a scheduling system based on dispatching rules for supporting paper bag factories
P	Lee et al. (2004) proposed a heuristic for minimizing total tardiness in m-stage HFS with identical machines.
P	Later, by extending the famous NEH approach (Nawaz et al., 1983), Choi et al. (2005) proposed several heuristics for minimizing total tardiness in a HFS with reentrant lots.
O	Jungwattanakit et al. (2009) implemented and compared several methods including dispatching rule, constructive heuristic and metaheuristics for minimizing the weighted sum of makespan and number of tardy jobs in a HFS with unrelated machines. 
O	The efficiency of SA is shown also in Naderi et al. (2009).
O	The proposed SA was reported to be superior to several algorithms including the aforementioned GA in Kurz and Askin (2004) and the immune evolutionary algorithm in Zandieh et al (2006).
O	On the other hand, Li et al. (2015) proposed a GA to address the HFS scheduling problem with batch processing machines to minimize makespan and total weighted tardiness, respectively.
P	Recently, Pan et al. (2017) proposed four IG-based methods to minimize a convex combination of job tardiness and earliness and obtained state-of-art results.
O	These are like the container handling systems in Chen et al. (2007), the cardboard boxes production system in Alfieri (2009) and the printed circuit-board assembly lines in Yaurima et al. (2009).
O	Similar strategy was adopted in Yaurima et al. (2009) and Rashidi et al.(2010).
O	The decision variables of both the partitioning and scheduling problems are encoded and optimized in search technique, as in Chen et al. (2007).
O	In such schedules, the decision variables are reduced to the machine assignment decision of each operation, and the sequence of operations on each machine, with a solution cardinality of Qm i=1 (n+mi−1)! (mi−1)! (Urlings et al., 2010).
P	In many studies, the NEH algorithm (Nawaz et al,1983) is used to generate initial solution.
O	However, when using NEH to handle total tardiness, selection difficulties arise as the sub-schedules are giving identical objective values (Choi et al., 2005).
O	Another promising operator is the order-based crossover (OBX, Yaurima et al., 2009), by homogeneously sampling order information from both parents, this operator compromises between the exploitation and exploration abilities.
P	The restart procedure is based on the one used in Ruiz et al (2006), we add a new mutation operator to increase the gene diversity of the new population.
O	For any instance, the job release dates are set to 0, and the due dates are generated using the method of Choi et al.(2005).
N	Yet for tardiness criteria, RPI is no longer adaptable because it may provide a division by zero when the schedule has no tardy jobs (Naderi et al., 2009).
O	Five state-of-art algorithms are selected, including one deterministic heuristic NEH EDD (Ruiz et al., 2008); two efficient metaheuristics, i.e., GA by Ruiz and Maroto (2006), denoted as GA Ruiz, and SA by Naderi et al.(2009), denoted as SA Naderi; and two recent proposed metaheuristics, i.e., ABC by Cui et al. (2015), denoted as ABC Cui, and GA by Li et al.(2015), denoted as GA Li.
O	Shift Minimization Personnel Task Scheduling Problem (SMPTSP) is the assignment of predetermined tasks (with known start and finish times) to a minimum number of heterogeneous workers with predetermined shifts (Krishnamoorthy et al.).
P	The main motivation for solving large scale SMPTSP is from Dowling et al. who solved the SMPTSP in the task optimizer module of their software for monthly rostering of approximately 500 full-time ground staff for the airport operations of a major international airline at one of the busiest international airports.
P	Many real life instances of the SMPTSP are hard to solve and involve over 500 workers/shifts and over 2000 tasks/jobs (page 40, Krishnamoorthy et al., [1]).
O	A smaller application of SMPTSP is reported in Kroon et al. where the management of the Schiphol airport in Amsterdam was interested to identify the future required gate capacity of the airport under various flight intensity scenarios.
O	This paper addresses large instances of SMPTSP, but uses a method simpler than the one proposed by Krishnamoorthy et al.
P	We used the 137 Krishnamoorthy et al., 10 Smet et al., and 100 Fages & Lapègue’s test problems to test the proposed heuristic method.
O	Kroon et al. called SMPTSP tactical fixed interval scheduling problem, and showed that solving it to optimality is NP-hard.
O	Kolen et al. called SMPTSP interval scheduling, and among other things, showed that if there is only one type of tasks and the employees are homogeneous, SMPTSP can be solved optimally in O (|J| log |J|) operations, where |J| = number of tasks.
O	Krishnamoorthy et al. used the lagrangian relaxation heuristic method to solve large instances of SMPSTP.
O	Smet et al. used a 2-phase algorithm to solve all of Krishnamoorthy’s test problems.
P	Krishnamoorthy et al., Page 39, called this problem Pack and suggested an algorithm that finds a shortest path in a directed graph with arc lengths equal to the negative of task durations.
O	The results of the Greedy heuristic for each instance, including the best results reported by Krishnamoorthy et al., Lin and Ying , Smet et al., and Fages and Lapègue are displayed in Tables 1 to 3, respectively.
O	Table 1 shows the results for Krishnamoorthy et al’s 137 test problems.
P	It shows that the Smet et al., Fages & Lapègue, and Greedy heuristic methods are better than the other solution methods.
O	Table 2 shows the available results for Smet et al.’s 10 test problems.
O	Comparison of the available solution methods for Smet et al.’s 10 test problems (based on Table 2).
P	It and Smet et al’s method are better than the others when they are used to solve the 3 sets of test problems. 
N	But note that Smet et. al’s method requires a commercial ILP Solver and it is doubtful that it can solve some of the very large instances (over 500 workers) of SMPTSP.
O	A recent review of the main SVRPs variants studied in the literature can be found in Gendreau et al. (2014).
O	It should be noted that SPRs and CCPs are not exclusive and can be used in mixed formulations (see for example Errico et al. (2016)).
O	Exact methods can be found in the VRPSD literature and are divided in two approaches: the L-shaped algorithm (see Laporte & Louveaux (1993)) and branch-and-price based.
P	L-shaped methods (Gendreau et al. (1995), Hjorring & Holt (1999), Laporte et al. (2002), Jabali et al. (2014), Biesinger et al. (2016), Salavati-Khoshghalb et al. (2017a)) have been the preferred approach to solve the VRPSD and are able to optimally solve instances with up to 100 customers and few vehicles when discrete distributions are considered (Laporte et al. (2002)).
O	In Jabali et al. (2014) normal distributions are used to model the demands attaining optimal solutions to instances with 60 to 80 nodes and two to four vehicles.
O	More recently, a branchand-cut-and-price algorithm is implemented by Gauvin et al. (2014) and tested on instances with up to 101 customers and 15 vehicles.
N	Gendreau et al. (1996) propose a Tabu Search called tabustoch to tackle the extension of VRPSD where additionally, customers are present or not with a given probability.
O	Yang et al. (2000) consider the VRPSD under restocking possibilities, thus adopting strategies for preventive restocking.
O	In Bianchi et al. (2006), hybridization of five metaheuristics with different objective function approximations and preventing restocking is devised.
P	More recently, Goodson et al. (2012) propose a Simulated Annealing procedure to solve the VRPSD that uses a cyclic-order encoding to represent solutions.
O	Mendoza et al. (2010) use a Memetic Algorithm to solve the multi compartment VRPSD which is a generalization of the classic VRPSD.
P	In a more recent study, Mendoza et al. (2015) designed a Greedy Randomized Adaptive Search Procedure (GRASP) enhanced with Heuristic Concentration (HC) to solve the VRPSD with maximum route duration constraints.
P	To the best of our knowledge, the method of Mendoza et al. (2015) reports the best overall results for Christiansen & Lysgaard (2007) testbed.
P	Luo et al. (2016) addressed the VRPSD with weight-related costs and solved it by means of an Adaptive Large Neighborhood heuristic using several approximate methods.
N	Sarasola et al. (2016) dealt with the VRPSD with dynamic requests meaning that previously unknown customers can be received and sched80 uled over time.
O	Nguyen et al. (2016) study the VRPSD with time windows using a Satisficing Measure Approach (SMA).
O	A multi-objective version of the VRPSD considering total traveling distance, total driver remuneration, number of vehicles and drivers remuneration balance is proposed in Gee et al. (2016) and solved using a multi-objective evolutionary algorithm.
O	However, other recourse policies have been studied and implemented through several studies: preventive restocking policies are extensions of the classical recourse where return trips to the depot are performed even if the vehicle is not empty to avoid future failures (Yang et al. (2000); Bianchi et al. (2006); Biesinger et al. (2016); Luo et al. (2016); Zhang et al. (2016); Salavati-Khoshghalb et al. (2017a,b)); pairing strategies allow the cooperation 100 of multiple vehicles (Ak & Erera (2007)); split deliveries between paired routes (Lei et al. (2012)) in which some customers are served by two vehicles; and backup routes (Erera et al. (2009)) that receive customers from primary routes.
O	Although the use of more complex recourse policies can represent a significant saving relative to simpler ones (Ak & Erera (2007)), the latter have been preferred since they allow more tractable models and stable tactical routes (Gendreau et al. (2016)).
O	For this reason only the works of Biesinger et al. (2016) and Salavati-Khoshghalb et al. (2017a) deal with exact methods for the VRPSD using recourse actions different than the classical one. 
O	Biesinger et al. (2016) consider a restocking policy for the generalized VRPSD using a single vehicle.
O	In Salavati-Khoshghalb et al. (2017a) optimal restocking policies allow vehicles to decide between a visit to the depot to replenish or proceeding to the next customer.
O	Furthermore, we consider as other authors (Christiansen & Lysgaard (2007); Gauvin et al. (2014); Goodson et al. (2012); Mendoza et al. (2015)), that ψ distribution has a cumulative property, i.e. the sum of demands probability functions is also ψ distributed.
P	To avoid multiple failures in a route r, the expected demand is limited to be at most equal to the maximum capacity Q, this assumption has been already used by many authors such as Laporte et al. (2002), Christiansen & Lysgaard (2007), and Gauvin et al. (2014).
O	The distance measure used is the broken pairs of Campos et al. (2005) that counts the number of times a pair of consecutive customers in a first individual is broken in a second one.
O	This representation has been already used by authors such as Mendoza et al. (2010) for the multi-compartment VRP with stochastic demands, Mendoza & Villegas (2013) and Goodson et al. (2012) 180 for the VRPSD, and Mendoza et al. (2015) for the VRPSD with time duration constraints.
P	By doing so, it is expected that constructed routes will have a lower probability of failures (see Mendoza et al. (2010)), introducing important information to the population.
O	The RNN works without considering recourse costs following the ideas of Bianchi et al. (2006) and Goodson et al. (2012).
O	Second, the detailed results of previous authors such as Mendoza et al. (2015) show that the deterministic costs represent much of the best solutions costs in the VRPSD (more than 95%). 
O	Nevertheless, recourse costs cannot be completely omitted since optimal deterministic solutions can be as far as 10% of optimal VRPSD solutions (Gauvin et al. (2014)).
O	Demands are assumed to be Poisson distributed as in the papers from Christiansen & 335 Lysgaard (2007); Goodson et al. (2012); Mendoza & Villegas (2013); Gauvin et al. (2014); Mendoza et al. (2015), with expected values equal to the deterministic demand values.
O	Moreover, travel costs are calculated as the Euclidean distance between two nodes rounded to the nearest integer as done in Gauvin et al. (2014).
O	For Christiansen & Lysgaard (2007) benchmark, the Best Known Solutions (BKS) are either taken from Gauvin et al. (2014) in which 38 solutions are proven to be optimal, or from Mendoza et al. (2015) and Goodson et al. (2012) which use heuristic approaches.
O	Random variables and computation probabilities were generated by the library of Stochastic Simulation in Java (L’Ecuyer et al. (2002)).
O	Table 1 summarizes the results of the proposed MA+GRASP compared to those of Mendoza et al. (2015) (GRASP-HC), and Goodson et al. (2012) obtained with a Simulated Annealing (SA) procedure.
P	It should be noted that some of these have been already addressed by Gauvin et al. (2014) so ten instances have a proven optimal solution. 
O	Moreover, travel costs are calculated as the Euclidean distance between two nodes rounded to the nearest integer as done in Gauvin et al. (2014).
O	Authors like Laporte et al. (2002) or Jabali et al. (2014) had reported the difficulty of their exact approaches to solve the VRPSD when the number of vehicles increases or when the filling coefficient (FC) approaches one. 
P	This method is an extension of the time-to-target plots (ttt-plots) introduced by Feo et al. (1994).
O	Dillenberger et al. were the first to address setup carry-over in a big-bucket lot sizing problem with sequence-dependent production costs for a problem on parallel machines.
P	Goren et al. introduced a genetic algorithm hybridized with a fix-and-optimize heuristic.
O	Angelelli et al. [2] suggested relying on the reduced costs associated with the variables of the linear relaxation of the problem to define both the initial kernel and the sequence of buckets.
P	The parliaments of Netherlands and Norway recently passed new motions that will end sales of new cars powered by fossil fuels after 2025 (Edelstein, 2016). 
O	Similarly, German Federal Council accepted a resolution that bans the sales of fossil fuel cars by 2030 (Khan, 2017).
O	Many municipalities, government agencies, non-profit organizations, and private companies are converting their fleets to include AFVs, either to reduce their environmental impact voluntarily or to meet new environmental regulations (Erdoğan and Miller-Hooks, 2012).
O	The number of moving parts in EVs are much less than that of ICEVs and EVs do not require regular oil changes (Feng and Figliozzi, 2013).
P	The main advantages of EVs are zero tailpipe emission, high efficiency, and low operating noise (Pollet et al., 2012).
P	In addition, due to the regenerative breaking, break wear is used less which reduces the maintenance costs (Hiermann et al., 2015). 
N	On the other hand, operating an EV fleet has several drawbacks such as low achievable driving range, limited number of recharging stations, and long battery recharging times (Touati-Moungla and Jost, 2011).
O	Bruglieri et al. (2015) relaxed the full charge restriction and allowed partial recharging with any quantity up to the battery capacity, which is the current practice in real-world applications.
P	Conrad and Figliozzi (2011) present the recharging VRP where the EVs are recharged at selected customer locations during the service. 
O	Erdoğan and Miller-Hooks (2012) considered Green VRP (GVRP) in which AFVs are refueled at stations en-route.
O	Schneider et al. (2014) introduced EVRP with Time Windows (EVRPTW) as an extension to GVRP.
P	Desaulniers et al. (2016) tackled the same problem by considering different charging strategies and developed branch-price-and-cut algorithms to solve them optimally.
O	Hiermann et al. (2016) addressed the Fleet Size and Mix Vehicle Routing Problem with Time Windows where the fleet consists of EVs only.
P	Bruglieri et al. (2015) and Keskin and Çatay (2016) relaxed the full recharge restriction and allowed batteries to be recharged up to any level.
P	The latter extended the model of Schneider et al. (2014) to formulate EVRPTW with Partial Recharges (EVRPTW-PR) and proposed an ALNS approach that improved some of the best-known results in the literature.
O	Bruglieri et al. (2016) formulated a more effective mathematical model for GVRP by reducing the number of variables and eliminating dominated stations for each pair of customers.
P	Felipe et al. (2014) addressed EVRP by allowing partial recharges using multiple technologies.
O	Recently, the impact of fast charging option in the presence of time windows has also been studied by Çatay and Keskin (2017).
O	Montoya et al. (2017) is the first study that extended EVRP to consider nonlinear charging functions.
P	New formulations of this problem are proposed in Froger et al. (2017). 
P	Hof et al. (2017) also investigated EVRP with BSSs and developed an Adaptive VNS (AVNS) approach to solve it.
O	Recently, Schiffer and Walther (2017) proposed a location-routing model for EVRPTW allowing partial recharging at customer locations.
O	Paz et al. (2017) also modeled the same problem considering multi depots and battery swapping option in some locations. 
O	Pelletier et al. (2017) integrated battery degradation into the model and optimized charging schedules at the depot.
O	A detailed survey of the goods distribution with EVs can be found in Pelletier et al. (2016) and Pelletier et al. (2017).
O	The charging levels can be classified into three categories: Level 1 (1.4 kW to 1.9 kW), Level 2 (4 kW to 19.2 kW), and Level 3 (50 kW to 100 kW) (Yilmaz and Krein, 2013).
O	The charge durations are linear with respect to time at the first phase of charging which corresponds to almost full battery while the second phase is non-linear and can take hours to obtain a fully charged battery (Montoya et al. 2017).
O	On the other hand, it is a common industrial practice to operate within the first phase because recharging the battery up to full capacity can adversely affect its lifespan (Sweda et al., 2017).
O	To describe the general setting and highlight the advantage of using fast chargers we employ the instance c104c10-s3 of Schneider et al. (2014) which involves ten customers and three stations.
P	We follow the notation used in Keskin and Çatay (2016) for ease of understanding.
P	The second model is a simple modification of EVRPTW-PR formulation and was first presented in Çatay and Keskin (2017).
P	We refer the interested reader to Archetti and Speranza (2014) for the details of the approach and an overview of implementations.
P	ALNS is a neighborhood search approach which was proposed by Ropke and Pisinger (2006a, 2006b) and has been utilized to solve several types of VRPs (Pisinger and Ropke, 2007; Ribeiro and Laporte, 2012; Demir et al., 2012; Aksen et al., 2014; Grangier et al.,2016; Emeç et al.,2016; Koç et al., 2016) including EVRPs (Goeke and Schneider, 2015; Hiermann et al., 2015; Schiffer and Walther, 2016; Keskin and Çatay, 2016; Wen et al., 2016; Schiffer et al., 2017).
P	A similar problem was also solved by Montoya et al. (2017).
P	Montoya et al. (2017) solve the EVRP by using a sequence-first split-second approach where they first construct a TSP tour and then split it to extract vehicle routes by ignoring the EV range limit.
O	For most of the ALNS destroy and repair mechanisms we resort to the neighborhoods utilized in Keskin and Çatay (2016).
O	In addition to the well-known CR heuristics widely used in the literature such as Random, WorstDistance, Worst-Time, Shaw, Proximity-based, Demand-based, Time-based, Zone, Random Route, and Greedy Route removals, we utilize Remove Customer with Preceding Station and Remove Customer with Succeeding Station operators introduced by Keskin and Çatay (2016) where customers are removed along with the station visited immediately before or after serving that customer.
P	We use Random and Worst-Distance Station removals proposed in Keskin and Çatay (2016).
O	Our approach is similar to the ideas presented in Bruglieri et al. (2016).
O	We test the performance of the proposed matheuristic on the EVRPTW data sets that Schneider et al. (2014) generated and on the GVRP-MTPR data set of Felipe et al. (2014).
O	The larger instances of Schneider et al. (2014) include 100 customers and 21 recharging stations. 
O	The data set of Felipe et al. (2014) is referred to as FORT instances and consists of two different configurations involving five and nine stations. 
O	In ALNS, we used the same parameter values as reported in Keskin and Çatay (2016) (see Appendix B). For the recharging speed and cost of different chargers, we used the values given in Felipe et al. (2014).
O	These results are in parallel with Desaulniers et al. (2016) who highlighted the minor influence of wide time-window constraints on recharging decisions.
P	To the best of our knowledge, Felipe et al. (2014) is the only study that addressed EVRP with multiple charger types and partial recharges but without considering time windows and using a different objective function which minimizes total cost of energy and battery degradation.
O	Our aim in performing test (B) is to investigate the trade-off between the solution quality and computation time, and to make a fair comparison with the heuristic of Felipe et al. (2014) in terms of run time.
N	Felipe et al. (2014) reported an average run time of 647 seconds whereas our matheuristic (A) and (B) spent on the average 1352 and 186 seconds, respectively.
O	The detailed results of the experiments for the problem of Felipe et al. (2014) using FORT instances are presented in Tables C.1-C.3.
O	Yoosefelahi et al. study a multiobjective RALBP with an efficiency-based objective, cycle time minimization, as well as two cost-oriented objectives, fixed and variable cost minimization. 
O	Recently, Nilakantan et al. have considered cost minimization with selection of robotic alternatives.
P	The first set uses the 32 instances proposed by Gao et al. for the robotic ALBP shown in Table 3.
O	We next analyze the results for the dataset derived from Gao et al. and in a second step we focus on the instances created according to Otto, Otto, and Scholl.
O	In the literature of KEPs, L = K is considered in e.g., Manlove and O’Malley (2012); L > K is considered in, e.g., Glorie et al. (2014), Dickerson et al. (2016b) and Plaut et al. (2016); and L = ∞ in, e.g., Anderson et al. (2015).
O	The CCMcP is NP-hard, except for the case when K = 2 (see, e.g., Abraham et al., 2007 and Biró et al., 2009). The CCCCP is also NPhard (see, e.g., Anderson et al., 2015).
P	To the best of our knowledge, there has not been any polyhedral study of the CCMcP or the CCCCP, except for Mak-Hau (2015).
O	In the context of kidney exchange, 2- and 3- cycles are very common, although the largest exchange cycle performed involved nine PDPs (see, e.g., SF Gate, 2015). 
O	Integer programming models developed for the CCMcP and the CCCCP, (which are mostly developed in the context of KEP), can be classified into three main branches: (a) arc-based models with a small number of variables, but exponentially many constraints (see, e.g., Roth et al., 2007 for CCMcP and MakHau, 2017 for CCCCP); (b) cycle-based models with a small number of constraints, but exponentially many variables (see, e.g., Abraham et al., 2007 and Roth et al., 2007 for CCMcP and Anderson et al., 2015 for CCCCP); and (c) arc-based compact models that create multiple clones of the directed graph, making it possible to have both variables and constraints be polynomial in size (see, e.g., Constantino et al., 2013 and Dickerson et al., 2016b).
O	In terms of solution methodologies, branch-and-price based methods are applied with implementation details presented in, e.g., Abraham et al. (2007), Glorie et al. (2014), Klimentova et al. (2014) and Plaut et al. (2016).
O	A summary review of the performances of Abraham et al. (2007), Manlove and O’Malley (2012), Constantino et al. (2013), Glorie et al. (2014), Klimentova et al. (2014), Anderson et al. (2015), and MakHau (2017) can be found in Mak-Hau (2017), together with details on the sizes of problem tackled, and values K and L tested.
O	Recently, Dickerson et al. (2016b) presented a new polynomial size formulations for the CCMcP and the CCCCP with bounded L wherein a binary variable is used to determine whether an arc is used in a particular position of a cycle in a particular copy of the digraph–a concept that is an extension to the extended edge formulation proposed by Constantino et al. (2013) and with a stronger LP relaxation (LPR) bound.
P	The paper also provided a proof that the LPR of PIEF is as strong as the cycle-formulation (see Abraham et al., 2007 and Roth et al., 2007).
O	Multi-objective approaches are discussed in Glorie et al. (2014), and implemented in Manlove and O’Malley (2012) and Manlove and O’Malley (2015).
O	The method of Manlove and O’Malley (2012) and Manlove and O’Malley (2015) promote the use of shorter cycles e.g., 2-cycles as well as 3-cycles with a back arc (i.e., the subgraph of the 3-cycle contains at least one 2-cycle, such that if one arc is broken, i.e., one transplant cannot move forward, the remaining two PDPs can still perform a kidney exchange).
N	On the other hand, failure-awareness has also been addressed in, e.g., Dickerson et al. (2013), Klimentova et al. (2016), Pedroso (2014), and Li et al. (2014).
P	Heuristic approaches for multi-criteria objective function for the CCCCP is also proposed in Mak-Hau and Nickholds (2015).
O	In other work, Dickerson et al. (2016a) presented a graph theoretical approach to kidney exchange, and Jia et al. (2017) presented approximation algorithms.
O	It appears that as of this date, the state-of-the-art algorithms for the CCCCP for L = ∞ is Anderson et al. (2015) and that for bounded L is Dickerson et al. (2016b).
P	A one-dimensional mathematical induction proof framework was proposed in Fischetti (1991) and proofs of facet-defining properties were developed for several classes of constraints.
P	In Fischetti and Toth (1997), separation algorithms were developed for these facetdefining constraints, and a branch-and-cut-and-price algorithm was implemented to obtain exact solutions for ATSP instances.
O	In ATSP with side constraints, Ascheuer et al. (2000) presented proofs of validity for a number of inequalities for the ATSP with timewindows, and described a lifting procedure for constructing new valid constraints.
O	In Prize-collecting TSP, polyhedral results can be found in, e.g., Balas (1989, 1995).
O	In ATSP with Replenishment Arcs (RATSP), polyhedral results on facet-defining constraints and branch-and-bound (BNB) exact algorithm where an AP-based Lagrangean relaxation was solved in each node of the BNB tree can be found in, e.g., Mak and Boland (2006, 2007). 
O	Polyhedral results for the Quadratic Selective TSP can be found in, e.g., Mak and Thomadsen (2006).
P	In Black and White TSP, valid inequalities were developed in Ghiani et al. (2006), therein separation algorithms and branch-and-cut were discussed.
O	In Vehicle Routing-family of problems, polyhedral results can be found in, e.g., Cornuejols and Harche (1993) for the Capacitated Vehicle Routing Problem, and Mak and Ernst (2007) for the Vehicle Routing Problem with Precedence and Time-windows Constraints.
O	The work of Bauer et al. (2002) presented results on how facetdefining inequalities on related polytope can be transformed into facet-defining inequalities for the CCCP polytope and vice versa.
O	Polyhedral results can be found in, e.g., Nguyen and Maurras (2001).
O	Relevant polyhedral work can be found in, e.g., Hartmann and Özlük (2001).
O	Perhaps the closest combinatorial optimization problem to the CCMcP is the Cardinality Constrained Covering TSP (CCCTSP), see, e.g., Patterson and Rolland (2003) where multiple cycles are allowed, the cardinality of these cycles are bounded by an upper bound (in the case of CCMcP, K), and a lower bound (in the case of CCMcP, 0).
P	In this section, we review the basic arc formulation for the CCMcP proposed in Roth et al. (2007), propose a number of facetdefining and valid constraints, and present relevant polyhedral results, including: the dimension of the CCMcP polytope, the proofs of some trivial and non-trivial facets, and the validity proofs of some newly identified constraints.
P	A number of studies in the literature have either implemented or derived some alternative forms of the arc formulation that was first presented in Roth et al. (2007).
O	For example: Abraham et al. (2007) showed that the basic arc formulation is not computationally effective (hence the motivation of our paper); Constantino et al. (2013) compared extensively the arc formulation, the cycle formulation, and two polynomial-size formulation; Anderson et al. (2015) proposed combined arc and cycle formulation for the CCCCP; and Dickerson et al. (2016b) also proposed polynomial size formulation for both of CCMcP and the CCCCP. 
O	In Anderson et al. (2015), cardinality violation constraints are added in the manner of lazy constraints.
O	(The Arc Formulation of Roth et al., 2007).
O	See Mak-Hau (2015) for a proof of Proposition 1.
O	(Dimension of the CCMcP Polytope, MakHau, 2015).
O	We entered the entire set of feasible integer solutions for small n and K, and used PANDA (Lörwald and Reinelt, 2015) to obtain insights on the structure of some of the facet-defining constraints.
P	Integer programming models are proposed in Anderson et al. (2015); Glorie et al. (2014); Mak-Hau (2017).
O	Notice also that the model is similar to that of the exponential-size SPLIT model presented in Mak-Hau (2017), thought the chain size violation elimination is polynomial in size therein as a Miller-Tucker-Zemlin (Miller et al., 1960) type constraint is used for subtour elimination for the chain variables, however the constraint (29) presented in this paper is exponential in size.
O	We used K = 3 and L = |P| + 1 (same K and L values used in Anderson et al. (2015)
O	In our numerical tests for CCCCPs, we used L = |P| + 1 in order to conform to the characteristics of the clinic data set used in Anderson et al. (2015), we replaced the righthand-side of (35).
O	Either with a user-selected arc density, or arcs generated according to Australian Blood Type distribution (see Australian Blood Type, 2016)–each donor/recipient is given a randomly generated blood type, with probability distribution following the blood type distribution.
O	We have generated three data sets. Set 1 Instances are generated using features and arc density estimated from the data cases presented in the Anderson et al. (2015), and are used in Tables 1–4. In Table 1, CCMcPs are considered.
O	We performed our numerical experiments with the same parameter settings used in Anderson et al. (2015), that is, we tested K = 3 and L = |P| + 1.
O	Again, we used K = 3 and L = |P| + 1 in these runs (same as those used in Anderson et al., 2015).
P	A big thank you to my husband Dr David Hau for installing PANDA (Lörwald and Reinelt, 2015) on all my machines, providing medical advices, and for proofreading this article.
O	Parragh et al (2008) classified VRPDPs into four sub-problems: the VRP with Clustered Backhauls (VRPCB), VRP with Mixed Linehauls and Backhauls (VRPMB), VRP with Simultaneous Delivery and Pickup (VRPSDP) and VRP with Divisible Delivery and Pickup (VRPDDP).
P	Since Dror and Trudeau (1989) introduced the Split Delivery Vehicle Routing Problem (SDVRP), which is well known in the literature, a growing number of academics have worked in the field of split demand.
O	(Dror et al, 1989, 1990; Belenguer et al, 2000; Ho and Haugland, 2004; Archetti et al, 2006a, 2006b, 2008a, 2008b, 2011, 2014; Derigs et al, 2010; Moreno et al, 2010; Wilck IV and Cavalier, 2012a, 2012b; Berbotto et al, 2014; Khmelev and Kochetov, 2015; Silva et al, 2015; Ozbaygin et al, 2018). 
O	There has been nearly 30 years since the earliest concept of split (Dror et al, 1989, 1990), and in the previous 10 years, split was only considered in single demand problems (VRPs) until the first paper on the VRPMB with split loads (SVRPMB) appeared in Mosheiov (1998).
O	On the base of Mitra’s research, Wang et al (2010) considered time windows in the VRPSPDP.
P	Yin et al (2013) proposed a special mathematical model for the VRPSPDP, in which two new pre-conditions were assumed.
P	Chen et al (2017) proposed two a priori split strategies and aimed to split each customer’s demands into several discrete items. 
P	The next paper to focus on discrete demands was presented by Fu et al (2017), who introduced a related problem to the DSDVRP named the VRP with Soft Time Windows Split Deliveries by Order (VRPSTWSDO). Xia et al (2018) extended the research on the VRPSTWSDO.
O	Gulczynski et al (2010) considered a special SDVRP in which split deliveries are allowed only if a minimum fraction of a customer’s demand is serviced by a vehicle and named the problem the SDVRP with Minimum Delivery Amounts (SDVRP-MDA). 
O	Recently Wassan et al (2013), Nagy et al (2015) and Polat (2017) focused on this interesting but rarely addressed problem.
O	Koç and Laporte (2018) provided a general overview of papers for the VRPDPs until 2017 with detailed comparison of computational performance of solution methods, while Irnich et al (2014) provided a general overview of VRPDPs papers from 2002 to 2014, without comparisons of computational performance.
O	Parragh et al (2008) comprehensively reviewed the pickup and delivery problems until 2007.
O	Later, Wassan (2007) combined the TS by Osman and Wassan (2002) with an adaptive memory programming scheme, which is capable of maintaining a set of elite solutions for searching the unexplored regions of the solution space and has been extended for further research on the VRPSDP (Wassan, 2008), the VRPDDP (Wassan, 2013) and the VRP with Restricted mixing of Deliveries and Pickups (VRPRMDP) (Nagy et al, 2013).
P	Hoff et al (2009) studied lasso solution strategies for the VRPDPs and developed a TS heuristic that is capable of generating lasso solutions.
P	Chen and Wu (2006) developed a hybrid heuristic based on the record-to-record travel (RRT) method of Dueck (1993) and tabu lists from the TS.
P	Meanwhile, Yu and Qi (2014) proposed two TS algorithms based on RRT for an industrial application case study.
O	In addition, there are other relevant papers applying TS in the literature, such as Alfredo Tang Montané and Galvão (2006), Bianchessi and Righini (2007), Zachariadis et al (2009).
P	Wang et al (2010) introduced a novel cluster-first-routing-second heuristic, Competitive Decision Algorithm (CDA), to solve the problem of VRPSPDP with Time windows (VRPSPDPTW).
P	The only TS heuristic for the VRPSPDP was introduced by Yin et al (2013), except the Adaptive Guidance (AG) mechanism with TS from Archetti et al (2006b) for the Vehicle Routing Problem with Splits and Clustered Backhauls (SVRPCB) (Lai et al, 2015).
P	Wassan and Nagy (2013) and Nagy et al (2015) studied the problem both using the meta-heuristic of reactive Tabu Search algorithm, and Polat (2017) solved the problem by a parallel variable neighborhood search.
O	The VRPDSPDP is a special version of the VRPSPDP, which is an NP-hard problem (Yin et al, 2013); the VRPDSPDP is also an NP-hard problem.
O	Nagy et al. (2015) only created two co-located customers and have already run into difficulties solving even small problems to optimality. 
P	The ―splitshift‖ operator proposed by Nagy et al (2015) for the VRPDDP is worth mentioning at this point.
O	Merge operators often accompany split operators, however, if a delivery and pickup entity of a customer should find themselves next to each other in subsequent moves, Nagy et al (2015) think that a separate operator is not required to bind them together again.
P	Local search methods have been proved effective in solving the VRPSDP (Subramanian et al, 2010; Zhang et al, 2012).
P	Gendreau et al (2002) introduced penalties, produced a mix of feasible and infeasible solutions and avoided the possibility of being trapped in a local minimum in a tabu route algorithm for the VRP.
O	The idea of each tabu list is learned and adjusted from Fu et al (2005). 
P	Chen et al (2017) proposed two a priori split strategies to split deliveries in the SDVRP in advance and not during the algorithm procedure, aiming to split each customer’s demands into several parts of goods (discrete batches or orders) so that it is possible to make full use of a vehicle’s capacity.
O	The data provided by Yin et al (2013) were used in this experiment.
N	The results obtained by our algorithm (both strategy 20/10/5/1/x and strategy 25/10/5/1/x) are better than the best-known one (Yin et al, 2013).
O	Yin et al (2013) used a fluctuation coefficient of the solution to evaluate the solution quality, which is calculated by the equation below:
N	The solution achieved by strategy 20/10/5/1/x (308.74) is the best one so far, which reduces by 3.55% the total distance of Yin et al (320.11).
O	We list ten results compared with Yin et al (2013) in Table 10. Fig. 16 further illustrates the quality comparison over ten runs. 
N	The fluctuation coefficients of our algorithm (2.4% of 20/10/5/1/x and 2.57% of 25/10/5/1/x) are both better than those of Yin et al (2013) (2.69%).
P	In addition, the average solution values from our proposed algorithm are lower than the average solution in Yin et al (2013).
O	Fluctuation comparison between results from the proposed method and results from Yin et al (2013).
O	The same data were also used by Wang et al (2015).
O	However, in Wang et al (2015) the objective of the VRPSPDP was to minimize the total travel distance and the number of assigned vehicles was not restricted. 
O	As a result, in Wang et al (2015) a few shortest distances are found using more vehicles. 
O	The minimum and actual number of assigned vehicles of optimal / upper bound solution, 20/20/5/1/x, 25/10/5/1/x, Mitra et al (2008), Wang et al (2015) and the best-known, separated by ― / ‖ , are given in column 3.
O	Both Mitra (2008) and Wang et al (2015) obtained the problem solutions with same (minimum) number of vehicles used for Set 1. 14 out of 25 best-known solutions for Set 1 are proved to be optimal (bold underlined data in columns named ―Opt./UB‖).
O	ecause Wang et al (2015) obtained all problem solutions of the minimum number of vehicles for Set 1, we only compare the problems of Sets 2 and 3. 
O	The data provided by Chen et al (2017) were used in this experiment.
P	Comparison of the results from the proposed method, Mitra (2008), Wang et al (2015) and the best-known
O	Comparison between results from the proposed method and those from Chen et al (2017)
O	Instance results from Chen et al (2017) and the proposed method
O	In the other 13 problems with the same number of vehicles, 53.85% are not worse than those in Chen et al (2017).
P	Overall, 68% (17 out of 25) instances have solutions from Chen et al (2017) that have been improved (fewer vehicles or shorter distances) or reproduced the same solutions by our TS algorithm.
O	Two well-known families of routing problems dealing with periodic deliveries are Periodic Vehicle Routing Problems (Campbell and Wilson, 2014, PVRPs) and Inventory Routing Problems (Bertazzi and Speranza, 2012, 2013; Coelho et al., 2013, IRPs).
O	Another vehicle routing problem dealing with periodic demand over a planning horizon is the Flexible Periodic Vehicle Routing Problem (Archetti et al., 2017, FPVRP).
O	The FPVRP was recently introduced in Archetti et al. (2017), where alternative mathematical programming formulations where presented and it was shown that, theoretically, it can produce arbitrarily large improvements with respect to both the PVRP and the IRP, in terms of the overall routing cost.
O	In particular, in Archetti et al. (2017) it was only possible to solve instances with up to 20 customers and five time periods within four hours of computing time, using CPLEX with the tightest of the two formulations proposed.
N	While the focus of Archetti et al. (2017) was to introduce the FPVRP and to analyze the savings that it may produce with respect to the PVRP and the IRP, in this paper we propose an effective solution method for the FPVRP.
O	Moreover, the average gap of the obtained solutions relative to the lower bounds produced by the formulation of Archetti et al. (2017) is 3.83%.
N	In this section we provide the definition of the FPVRP and recall the load-based MILP formulation proposed in Archetti et al. (2017), which was the most effective of the two proposed formulations.
O	The load-based MILP formulation for the FPVRP proposed in Archetti et al. (2017) uses the following sets of decision variables:
N	As shown in Archetti et al. (2017), the FPVRP is a very hard problem and only instances of small size can be solved to optimality.
O	We solve each CVRP through the VRPH package of the Coin OR library (Groër et al., 2010).
O	Routing(DP) is the solution of the CVRP for each time period on the basis of the distribution plan DP obtained through the VRPH package (Groër et al. (2010)).
O	Alternative ii) corresponds to the tenure proposed by Archetti et al. (2012), which considers a fixed value plus a random term from an interval that depends on the number of customers and the number of routes of the current solution.
O	A set of 5 PVRP benchmark instances from the literature (see, for instance, Chao et al. (1995); Cordeau et al. (1997); Baldacci et al. (2011)) and available at http: //neumann.hec.ca/chairedistributique/data/pvrp/old/.
O	A set of 35 FPVRP instances with clustered customers similar to the ones generated in Archetti et al. (2017).
O	These instances have been slightly modified from the ones in Archetti et al. (2017).
O	In particular, the second step consists in solving a Capacitated Vehicle Routing Problem (Ralph et al., 2003, CVRP) for each time period on the basis of the information provided by the DP.
O	In this table, column BestKnown shows the value of the best-known solution for each instance, columns in block FPVRP reproduce the results of Archetti et al. (2017) obtained with the FPVRP formulation (best solution, best lower bound, and computing time), columns in Math-FPVRP give information related to the performance of the two-phase algorithm (best solution found, average solution among all runs, and the required average and minimum/- maximum computing times).
O	In addition, the lower bounds produced by the FPVRP formulation of Archetti et al. (2017) are, in general, pretty tight.
P	The FPVRP was introduced in Archetti et al. (2017) where MILP formulations were proposed, which allowed to solve instances with up to 20 nodes.
P	Drexl and Kimms (2001) wrote the seminal paper on the CCSLSP and proposed a column generation algorithm to solve it.
O	Drexl et al. (2006) and then Yavuz (2013) incorporated constraint propagation into tree search algorithms of branch-and-bound and iterated beam search, respectively.
O	Bolat and Yano (1992) provide the basic analytical approach to determine Ho and No in presence of binary processing times such that variants requiring an option take longer than cycle time and others take shorter than cycle time.
O	Golle et al. (2010) analyze the differences between feasibility of sequences with respect to option spacing constraints (Ho: No) and the actual workloads in individual workstations along the line.
O	We refer the interested reader to these works and to Lesert et al. (2011) for further reading on selecting critical options and quantifying their spacing constraints, and focus on sequencing cars with given option constraints.
O	The problem is N P-hard even when all options constraints are 1:2, i.e., Ho = 1 and No = 2 for all options (Estellon and Gardi, 2013).
P	The CSP, first proposed by Parrello et al. (1986), has received significant interest from the constraint programming community (Dincbas et al., 1988; van Hentenryck et al., 1992; Tsang, 1993). 
O	A test-bed for the CSP is included in CSPLib: a library for constraints (Gent and Walsh, 1999).
O	Siala et al. (2015) build a unified heuristic algorithm that can be characterized based on four components. 
P	The operations research community has also shown some interest in the CSP, especially around 2005, when an extended version of the CSP was the topic of the French Society of Operations Research and Decision Aid’s bi-annual challenge; see Solnon et al. (2008) and the references therein for papers developed for the stated challenge.
P	Fliedner and Boysen (2008) highlight a potential problem with simply counting the number of violations for (o, t) pairs and propose a new objective function counting such violations only if the variant scheduled in a position requires the over-demanded option.
O	Bautista et al. (2008a) define upper-over-assignment and upper-under-assignment that measure deviations from maximum option allowances, and then combine them in a weighted objective function.
O	Thiruvady et al. (2014) develop a hybrid Lagrangian-ACO matheuristic for the same problem.
O	Bautista et al. (2008b) extend this formulation by defining a minimum number of occurrences in each option window and adding lower-over-assignment and lower-under-assignment terms to the objective function and solve the arising problem via a GRASP heuristic.
O	We refer the reader to Gagné et al. (2006) and Golle et al. (2014), and the references therein for further reading on soft-constraint approaches to the CSP.
O	The level scheduling problem (LSP) traces back to Monden’s (1983) seminal book on the Toyota Production System.
O	It has been studied in several forms and under various names such as just-intime scheduling, mixed-model assembly line scheduling, and production smoothing (Yavuz and Akçali, 2007).
O	The LSP is naturally multilevel, is commonly known by the name output rate variation (ORV) and is N P-hard (Kubiak, 1993).
O	The first exact algorithm developed for the ORV is a dynamic programming procedure of Kubiak et al. (1997).
P	Stronger mathematical properties of the problem are shown and incorporated into improved dynamic programming procedures by Fliedner et al. (2010); Miltenburg (2007).
P	A recent Branch-and-Bound algorithm developed by Pereira and Vilà (2015) uses multiple improved lower and upper bounds and is the most capable solution developed for the ORV so far.
P	This (single-level) version of the problem focusing on leveling the appearance of product variants in the schedule can be transformed to an assignment problem and be solved in polynomial time (Kubiak and Sethi, 1991).
O	A simpler approach is due to Inman and Bulfin (1991, 1992), who define an ideal position rv,i = i − 1 2  T/dv for each copy of each variant and treat the ideal positions as due dates.
O	Note that this approach bypasses the discussion of how (in)appropriate of an approximation PRV may be to ORV (see Boysen et al., 2009, for example), and provides a reasonable measure for more complex settings such as the CCSLSP.
O	The seminal work on the CCSLSP is due to Drexl and Kimms (2001).
O	The authors adopt min-sum squared deviations between actual and ideal positions of variant copies as objective function, where ideal positions are based on Inman and Bulfin’s (1991) paper.
O	Drexl et al. (2006) study the CCSLSP with a min-sum absolute deviation objective function built again on deviations between actual and ideal positions of variant copies.
O	Yavuz (2013) extends the constraint propagation algorithm by constructing a linear framework to exploit the option constraints.
O	Although beam search is commonly used as a single-pass constructive heuristic, the multi-pass (iterated) implementation in this paper is a complete tree search method, which is a branch-and-bound implementation generalizing Sewell and Jacobson’s (2012) cyclic best first node selection strategy
O	The author uses the same formulation in Drexl et al. (2006) and extends their 36-instance testbed to 54 instances with up to 100 cars.
P	The present paper extends the works of Drexl et al. (2006) and Yavuz (2013).
O	The problem instances designed by Drexl et al. (2006) are all known to be feasible.
O	This branching strategy is different from the commonly used assigning an unscheduled job to the earliest available position strategy (see Drexl et al., 2006; Fliedner and Boysen, 2008, for example).
O	Siala et al. (2015) compare this branching strategy to an alternative in which the positions are filled starting from the middle and towards the ends in an alternating fashion, and they find that their proposed method performs slightly but consistently better.
O	Our strategy is in line with that of Siala et al.
P	As for node selection, we adopt the depth-first node selection strategy consistently with Drexl et al. (2006).
O	Using the Drexl et al. (2006) instances allows us to compare the results obtained by the proposed algorithm to the earlier works.
O	The first 36 instances are the original instances developed by Drexl et al. (2006) and the next 18 were added by Yavuz (2013).
N	We have solved the last unsolved instance (40H7) from Drexl et al. (2006) in just over four minutes of computation time
N	Including 40H7, there were nine unsolved instances in Yavuz (2013), who allowed an hour of computation time.
P	In three instances (60H5, 80E7 and 100E7), Yavuz’s (2013) solutions have been verified to be optimal; and in all others our branch-and-bound algorithm improves both lower and upper bound, even solves two of them.
N	Overall, the number of unsolved instances in Yavuz’s (2013) instances is reduced to four
O	The Drexl et al. (2006) instances are designed to be very tight. 37 of the 42 instances had ¯ = 1 and the remaining five ranged from 0.962 to 0.986.
O	Uncertain environments in accordance with Govindan et al. (2017) can be categorized in three main groups as follows:
P	Escudero et al. (2017) presented a multi-period stochastic mixed 0–1 problem arising in tactical supply chain planning (TSCP).
O	Likewise, Megahed and Goetschalckx (2017) developed a two-stage stochastic programming model for the comprehensive tactical planning of supply chains under demand and supply uncertainty with the application of the wind turbine industry.
O	Fattahi et al. (2017b) developed a multi-stage stochastic program (MSSP) with the concern of SCN redesign.
P	With this focus, Kenan et al. (2017) developed a model for integrated flight scheduling and fleet assignment problem (SFSFAM) in the airline industry.
P	Hence, Park et al. (2017) proposed a new vendor evaluation framework by incorporating stochastic discrete event simulation and data envelopment analysis (DEA) approaches.
O	Heydari et al. (2017) proposed a model that features different shaping modes, including one fast mode and one slow mode.
O	Developing MSSPs and presenting efficient solution approaches for them is a challenging issue and, recently, Golari et al. (2017), Fattahi et al. (2017a), Fattahi et al. (2017b), and Escudero et al. (2017) have dealt with this issue.
P	Further discussion on the various modeling approaches can be found in (Boyce et al., 2001; Chen et al., 2011; Farahani et al., 2013; Mun, 2007; Peeta and Ziliaskopoulos, 2001; Szeto et al., 2012; Szeto and Wong, 2012; Yang and H. Bell, 1998), and for a historical view on the development of this field the reader is referred to (Boyce, 1984; Friesz, 1985; Magnanti and Wong, 1984; Migdalas, 1995).
O	This line of research can be traced to earlier studies, e.g., (Akamatsu, 1996; Bell et al., 2002; Kurauchi et al., 2003), however, it was novel and of limited application until researchers started to take inspirations from Google’s PageRank algorithm.
O	However, the use in urban road networks was first fully analyzed by Crisostomi et al. (2011) through a macroscopic road network model that applied Markov chain’s analytical tools to infer non-evident properties.
P	Faizrahnemoon et al. (2015) applied Crisostomi’s Markov chain model to multi-modal public transportation network, and proposed changes to the network design to improve its performance.
O	Although metaheuristics have been used in road network optimization (Jia et al., 2009; Long et al., 2014; Sharma and Mathew, 2011; Szeto et al., 2014), to the best of our knowledge, optimizing road network traffic through GAs while using Markov chains has not been investigated yet.
O	Other approaches to existing network infrastructure traffic optimization can be found in (Long et al., 2010; Zhang and Gao, 2007).
P	Crisostomi et al. (2011) proposed a new paradigm for modeling road networks by deploying Markov chain analysis tools to urban traffic modeling.
O	Road network sensory technologies are increasingly being deployed in road networks as they mature and become more affordable, however alternative approaches, such as Moosavi and Hovestadt (2013) and Yang et al. (2014), can be utilized to fill in the gap if such infrastructures are still under development in the area of application. 
O	The Kemeny constant has been shown to be independent of the starting state (Kemeny and Snell, 1960). 
O	The MCTA model was implemented in Python using its scientific computing libraries: SciPy (Jones et al., 2001) and MatPlotLib (Hunter, 2007).
O	While presenting the mathematical foundations for the MCTA, Crisostomi et al. (2011) pointed out that its application is limited to large scale aggregates of vehicles and a set period of time.
O	The reader is directed to (Prashker and Bekhor, 2004) for review on route choice models use in SUE, (Prato, 2009) and (Liu et al., 2010) for review on route choice modeling in traffic assignment in general, and (Chen et al., 2012) and (Zhou et al., 2012) for a more recent view on commonly used logit-based SUE models.
O	Our GA logic was implemented in python using the DEAP library (Fortin et al., 2012), and utilized standard crossover and mutation operators.
P	GA parameters were set using the Response Surface Design (RSD) approach proposed by Kucukkoc et al. (2013). 
P	Kim and Hong (2006) proposed two variations for the BRP.
P	Caserta et al. (2009) proposed a binary representation of the stacking area, which simplifies the transition from the current state of the stacking area to a state generated by a relocation or retrieval.
P	Lee and Lee (2010) introduced a new variant of the problem where each relocation has a cost equal to the distance between the two stacks over which the relocation is performed.
P	Caserta and Voß (2009) presented a new heuristic, named corridor method, to the pre-marshalling problem.
O	In another work, Caserta et al. (2011) applied the corridor method to the BRP without limiting the height of the stacks.
O	The BRP was shown to be NP-Hard by Caserta et al. (2012).
P	Jovanovic and Voß (2014) proposed a new heuristic approach to the BRP, which considers not only the current block to be relocated but also the next block to be relocated.
P	This heuristic reduced by 5% the number of relocations on average when compared to solutions generated by another heuristic proposed by Caserta et al. (2012).
O	Expósito-Izquierdo et al. (2014) presented several exact algorithms based on the A∗ search algorithm to solve the BRP.
N	They found 17 optimal solutions from 45 instances that were considered by Caserta et al. (2012).
P	Tanaka and Takii (2016) presented a new lower bound to the number of relocations based on previous lower bounds created by Kim and Hong (2006) and Zhu et al. (2012).
O	The use of Pattern Databases (PDBs) in exact algorithms has been effectively applied to solve challenging problems such as 16- puzzle (Culberson and Schaeffer, 1998) and Rubik’s cube (Korf, 1997).
O	For the BRP, it was first considered by Ku and Arthanari (2016), which use the concept of abstract states and PDB to shorten the exploration of the search space.
O	The other one derives from the creation of pattern databases (PDB) (Felner et al., 2004) and is denoted by LB-PDB.
O	We also explore, in this exact algorithm, the use of pattern databases as a way of memorizing part of the search space, in the same way as was done by Ku and Arthanari (2016).
O	The BRP problem under this restriction is still NP-hard and this assumption is used in several works from the literature (Caserta et al., 2012)
P	The IDA∗ was first introduced by Korf (1985) in 1985 as a general search method, and has been applied to several problems.
O	For the BRP problem, it was first considered by Zhu et al. (2012), and then by other authors such as Tanaka and Takii (2016).
P	Tanaka and Takii (2016) proposed a heuristic to generate a valid solution to the BRP, which is the one we use to generate an initial olution.
N	First, we present some lower bounds from the literature (Kim and Hong, 2006; Tanaka and Takii, 2016; Zhu et al., 2012) in Sections 4.1–4.3.
O	This lower bound was presented by Kim and Hong (2006) and it was named Lower Bound 1 (LB1) throughout the literature.
P	Lower Bound 2 (LB2) was proposed by Zhu et al. (2012).
P	Lower Bound 3 (LB3) was also proposed by Zhu et al. (2012) as an improvement over LB2.
O	Tanaka and Takii (2016) used this observation to create a new lower bound, which they denote by LB4.
O	In the second case Tanaka and Takii (2016) show that we can only consider the case where the current block is relocated to the stack with minimum target block value.
O	A Pattern DataBase (PDB) (Culberson and Schaeffer, 1996) basically consists of precomputed optimal solutions for small instances of the problem that are stored in a hash table.
O	 We notice that the use of PDB and abstract states as a caching strategy of precomputed solutions was used before for the BRP problem by Ku and Arthanari (2016).
P	The abstract state we use is essentially the same one used by Ku and Arthanari (2016).
P	Recall LB2, the lower bound proposed by Zhu et al. (2012) and explained in Section 4.
O	These ideas were used before by Ku and Arthanari (2016).
O	For the computational experiments, we downloaded and used the dataset of instances of Zhu et al. (2012).
O	These are the instances used in other experiments such as the ones performed by Tanaka and Takii (2016).
O	According to Zhu et al. (2012), the dataset is generated in the following manner.
O	We compared the use of the lower bound LB3 (Zhu et al., 2012), LB4 (Tanaka and Takii, 2016), and our two new lower bounds named LB-LIS and LB-PDB.
N	Note that LB4, which was proposed by Tanaka and Takii (2016), is considered the best lower bound up to date, however in our experiments the results of the exact algorithm using it only outperforms the results of LB-PDB.
O	The other lower bound is denoted by LB-PDB, and takes the maximum between LB2 (Zhu et al., 2012) and partial solutions saved in a pattern database in series of steps to calculate a valid lower bound.
O	The algorithm found more optimal instances when using LB-LIS, compared to other lower bounds of the literature, such as LB3 (Zhu et al., 2012) and LB4 (Tanaka and Takii, 2016).
O	As was done by Ku and Arthanari (2016), we used the concepts of abstract states and pattern databases as a way to save solutions during the exploration of the search tree.
N	Thus, Liu et al. proposed a heuristic method to solve the autonomous agile satellite scheduling problem, and in a previous work, we have developed an adaptive large neighborhood search heuristic (ALNS) for the single agile satellite scheduling problem with time-dependent transition time. 
P	Malladi et al. proposed an integer programming method to solve the problem.
O	Globus et al. compared several evolutionary algorithms for both the separate satellite and integrated scheduling of a two-satellite constellation.
P	Bianchessi et al. proposed a deterministic constructive algorithm with look-ahead and back-tracking capabilities.
P	Wang et al. considered a single-orbit multiple satellites scheduling problem and proposed a hybrid ant colony optimization algorithm for it.
O	Gao et al., Liu et al., Wu et al. and Wang et al. scheduled the multiple orbits on different satellites collectively, thus reducing the multi-satellite scheduling problem to a multi-orbit scheduling problem.
O	Huang et al. considered the two objectives of total observation reward and load balance among the satellites.
O	Chien et al. described the NASA SensorWeb of several EOSs, which could detect unexpected science events autonomously.
O	Dungan et al. and Frank et al. proposed a stochastic sampling method with contention heuristics.
O	Lemaˆıtre et al. casted the multiple AEOSs scheduling problem into a simpler problem.
O	Wang et al. studied the scheduling problem of an agile-satellite constellation with download considerations and proposed a priority-based heuristic that assigns tasks to the satellites with the most conflict-free VTWs.
O	Ntagiou et al. proposed an ant colony optimization heuristic for the coverage planning of large area targets for agile satellites constellations.
O	Xu et al. proposed a priority-based constructive algorithm which evaluates the importance of VTWs from different satellites according to the sum of priorities of all the tasks that have conflicts with the VTWs.
O	However, in sustainable SCM (SSCM) managers focus on triple bottom lines including social, environmental, and economic criteria (Dyllick and Hockerts, 2002).
O	Assessing SSCM is one of the important issues in organizations (Seuring, 2013).
O	Also, best-practice companies use Big Data resources and increase their performance (McAfee and Brynjolfsson, 2012).
O	Big data is a term from dataset which are huge and complex (Ohlhorst, 2012). 
O	Also, Big Data has triggered demand of experts in information management so that more than 15 billion dollars were spent for processing information by AG, IBM, Oracle, Microsoft, HP, Dell, SAP, and EMC (Syed et al., 2013).
O	Researches show linkage between sustainability and Big Data in SCM (Davenport, 2006). 
O	Hazen et al. (2014) discussed that whether Big Data can be used to increase operational and financial-based SCM results.
O	The Big Data deals with collection and storage of large data set (Dekker et al., 2013).
O	Zhong et al. (2016) summarized challenges of SCM as following 5Vs:
O	One of the techniques for evaluating the SSCM is data envelopment analysis (DEA) (Mirhedayatian et al., 2014; Azadi et al., 2015).
O	Furthermore, DEA models have been used along with Big Data (e.g., Li et al., 2017; Zhu et al., 2017; Chen and Jia, 2017; Chu et al., in press).
O	Recently, some researchers have attempted to integrate Big Data into DEA to evaluate efficiency of decision making units (DMUs) (Chen and Jia, 2017).
O	Li et al. (2017) used Big Data theory for assessing efficiency of China‘s forest resources.
P	Zhu et al. (2017) proposed DEA-based approach for assessing efficiency of natural resource utilization.
N	As addressed by Zhu et al. (2017), Big Data causes many issues in DEA.
O	For instance, waste and pollutions are undesirable outputs (Farzipoor Saen, 2010).
O	NDEA model was suggested, for the first time, by F ̈re and Grosskopf (2000).
O	Then, a multi-stage structure for NDEA model was proposed by Lewis and Sexton (2004). 
O	Tone and Tsutsui (2009) recommended a slack-based NDEA model which can deal with intermediate measures.
O	In pessimistic efficiency score, DMUs‘ efficiency scores are compared with inefficiency frontier (Wang and Chin, 2009).
O	Then, BCC (Banker-Charnes-Cooper) model was suggested by Banker et al. (1984).
P	To deal with these situations, NDEA model was proposed by F ̈re and Grosskopf (1996; 2000). 
O	Yu and Lin (2008) utilized NDEA model to evaluate technical efficiency, service effectiveness, and technical effectiveness of networks.
O	Also, NDEA model was suggested by Tone and Tsutsui (2009) for calculating overall and partial efficiency of DMUs.
O	Liang et al. (2008), Cook et al. (2010), and Chen et al. (2009) presented a model for dealing with DMUs with network structures.
O	During past decades, companies have considered factors such as quality, flexibility, price, and reputation to select suppliers (Bai and Sarkis, 2010). 
O	Nowadays, sustainability factors play vital role in assessing performance of suppliers (Kleindorfer et al., 2005). 
O	Linton et al. (2007) assessed sustainability of supply chains by focusing on environmental and social factors.
P	Govindan et al. (2013) proposed a fuzzy multi-criteria approach for measuring sustainability of suppliers.
O	Mirhedayatian et al. (2014) assessed green supply chains through NDEA.
P	Azadi et al. (2015) suggested a new fuzzy DEA model for evaluating efficiency and effectiveness of suppliers in SSCM context.
P	Khodakarami et al. (2015) developed two-stage DEA models for evaluating sustainability of supply chains.
O	Mani et al. (2016b) evaluated social sustainability in Indian manufacturing companies.
O	Seuring et al. (2008) discussed that social aspects are relatively and significantly less researched than environmental issues.
O	By focusing on total interpretive structure modeling, Dubey et al. (2016) assessed sustainability of SCM.
O	Mani et al. (2016a) suggested concept of supply chain social sustainability (SCSS). 
P	DMUs with more desirable outputs and less undesirable outputs relative to less input are identified as efficient (Cooper et al., 2007).
O	For the first time, F ̈re et al. (1989; 1996) and Yaisawarng and Klein (1994) dealt with undesirable outputs.
P	Jahanshahloo et al. (2005) proposed a non-radial DEA model by taking into account both undesirable inputs and outputs.
O	Jahanshahloo et al. (2013) assessed efficiency of DMUs using RAM model and super-efficiency in the presence of undesirable outputs.
P	Dubey et al. (2017) addressed role of Big Data analytics in supporting world-class sustainable manufacturing (WCSM).
O	Wang et al. (2016) assessed role of supply chain analytics (SCA) in logistics and SCM by applying Big Data methods
O	Impact of Big Data and predictive analytics on supply chain and organizational performance were evaluated by Gunasekaran et al. (2017).
O	Using Big Data, Papadopoulos et al. (2017) examined a theoretical framework to explain flexibility in supply chain network.
O	Li et al. (2017) investigated efficiency of forestry resources of China based on Big Data and DEA.
P	Zhu et al. (2017) proposed a DEA model to evaluate efficiency of natural resource utilization of China from 2005 to 2012 based on Big Data.
P	Chu et al. (in press) proposed a DEA model and a Big Data approach for assessing environmental efficiency.
P	To calculate relative efficiency of networks, Cook et al. (2010) proposed NDEA model.
N	Note that NDEA model proposed by Cook et al. (2010) can compute optimistic efficiency and cannot assess pessimistic efficiency.
O	Note that Khodakarami et al. (2015) considered number of green products as an economic factor.
O	Big Data can be used to evaluate social crises (Robert et al., 2008).
O	Detailed reviews on ALBPs have been reported by Baybars (1986), Ghosh and Gagnon (1989), Erel and Sarin (1998), Scholl and Becker (2006), Becker and Scholl (2006), Boysen et al. (2007), Battaia and Dolgui (2013) and Sivasankaran and Shahabudeen (2014).
O	However, the studies on the stochastic version of ALBP remain to be done since Moodie and Young (1965).
O	Several studies on stochastic ALBPs (Kottas and Lau 1973, 1976, 1981; Kao 1976, 1979; Carter and Silverman 1984; Henig 1986; Carraway 1989; Sarin and Erel 1990; Suresh and Sahu 1994; Liu et al. 2005; Guerriero and Miltenburg 2003; Erel et al. 2005; Chiang and Urban 2006; Urban and Chiang 2006; Ağpak and Gökçen 2007; Baykasoğlu and Özbakır 2007; Chiang et al. 2016; Özcan 2010; Li et al. 2014a, 2014b; Chiang et al. 2016, Delice et al. 2016, Tang et al. 2017) have been reported in the literature.
O	Uncertainty associated with task time is quite common in assembly line balancing practice and the stochastic ALBPs literature, where task times are independent and normally distributed (Chiang et al. 2016).
O	The skill-based manpower allocation problem where processing times are normally distributed and vary among workers in a labour intensive cellular manufacturing system is addressed by Egilmez et al. (2014).
O	Similarly, in the literature, there are many studies on workforce planning incorporating skills in stochastic environment (Bruecker et al. 2015).
P	For such cases, Özcan et al. (2009a) proposed a fuzzy multiple criteria decision making model to two-sided assembly line balancing problem.
P	Tsujimura et al. (1995) developed a genetic algorithm to solve the problem. 
P	And also, Gen et al. (1996) solved assembly line balancing problem with fuzzy processing time by using genetic algorithms with the objective that minimize the total operation time in each work station. 
P	A cost model with the objective of minimizing the total labour cost and the expected incompletion cost is developed by Sarin et al. (1999) to solve the single-model, stochastic assembly line balancing problem. In assembly line system design, flexible assembly systems can be used to increase the flexibility of the system. 
O	Detailed reviews on workforce planning incorporating skills have been reported by De Bruecker et al. (2015). 
O	In order to achieve a high productivity in an assembly line design, there are several alternative ways, e.g. use of overtime, another production line, subcontracting, buffer stocks, and paralleling (Akagi et al. 1983).
O	Akagi et al. (1983) also expresses that these ways, except paralleling, are based on the results of serial line balancing with a lower production rate.
O	According to Gökçen et al. (2006), when the demand is high enough, a possible way of increasing the productivity of an assembly line is to use two or more assembly lines located in parallel to each other called as parallel assembly lines.
O	It increases the level of complexity of the problem (Özcan et al. 2010).
O	In Table 1, in all studies, except Baykasoğlu et al. (2012), the task processing times were considered in deterministic environment. 
O	Baykasoğlu et al. (2012) considered the problem with fuzzy cycle and task times.
P	For calculation of C, the least common multiple (LCM) approach proposed by Gökçen et al. (2006) can be used and modified to stochastic environment. 
P	Gökçen et al. (2006) proposed a binary linear programming model to deterministic version of the problem. 
P	In Equations (11-17), an extension of the mathematical model of Gökçen et al. (2006) as a chance-constrained program developed by Çerçioğlu (2009) is given. 
O	The example problem is taken from Gökçen et al. (2006).
O	In the ALBPs literature, TS algorithm is a widely used meta-heuristic approach (Scholl and Voß 1996; Chiang 1998; Lapierre et al. 2006, Suwannarongsri and Puangdownreong 2008, Özcan and Toklu 2008, Özcan et al. 2009b, Özcan et al. 2010b).
O	Tang et al. (2017) proposed seven perturbation schemes for generating neighbourhood solutions; backward-insert, forward-insert, neighbour-swap, swap, inverse, multi- insert and multi-swap. 
O	The task times of the original problems are used as the means of the tasks, and the task variances are randomly generated by using the approach used by Carraway (1989), Urban and Chiang (2006), Özcan (2010) and Chiang et al. (2016).
O	BD has been characterized by 5Vs: volume, variety, velocity, veracity, and value (Wamba et al., 2015; Assunção et al., 2015; Emani et al., 2015). 
O	Volume refers to the magnitude of data, which has exponentially increased, posing a challenge to the capacity of existing storage devices (Chen and Zhang, 2014). 
O	Variety refers to the fact that data can be generated from heterogeneous sources, for example sensors, Internet of things (IoT), mobile devices, online social networks, etc., in structured, semi-structured, and unstructured formats (Tan et al., 2015). 
O	Velocity refers to the speed of data generation and delivery, which can be processed in batch, real-time, nearly real-time, or streamlines (Assunção et al., 2015).
O	Among those 5Vs, veracity and value, which represent the rigorousness of Big Data Analytics (BDA), are particularly important because without data analysis, other BD processing aspects such as collection, storage, and management would not create much value (Huang et al., 2015; Chen and Zhang, 2014; Babiceanu and Seker, 2016). 
O	BDA involves the use of advanced analytics techniques to extract valuable knowledge from vast amounts of data, facilitating data-driven decision-making (Tsai et al., 2015). 
N	Therefore, not surprisingly, supply chains (SCs) have been revolutionized by BDA and its application in SCM has been reported in a number of special issues (Wamba et al., 2015; Gunasekaran et al., 2016; Wamba et al., 2017).
O	Indeed, BDA is reported to be an emerging SC game changer (Fawcett and Waller, 2014; Dubey et al., 2016), enabling companies to excel in the current fast-paced and ever-changing market environment.
O	Empirical evidence demonstrates multiple advantages of BDA in SCM including reduced operational costs, improved SC agility, and increased customer satisfaction (Sheffi, 2015; Ramanathan et al., 2017) and consequently, there is an increasing interest in identifying a specific skill set for SCM data scientists (Waller and Fawcett, 2013; Schoenherr and Speier-Pero, 2015).
O	Although the expectation of BDA adoption to enhance SC performance is rather high, a recent report found that only 17% of enterprises have implemented BDA in one or more SC function (Wang et al., 2016).
O	The main reasons for low uptake are the lack of understanding of how it can be implemented, the inability to identify suitable data (Schoenherr and Speier-Pero, 2015), low acceptance, routinization and assimilation of BDA by organizations and SC partners (Gunasekaran et al., 2017), and data security issues (Fawcett and Waller, 2014; Dubey et al., 2016).
O	For instance, O‘Donovan et al. (2015), Dutta and Bose (2015), and Babiceanu and Seker (2016) conducted literature reviews on material flow in manufacturing operations while Wamba et al. (2015) focused on logistics applications.
O	A literature review that takes a broad perspective of SC as a whole and cross-maps with BDA techniques in SCM is yet scarce (Olson, 2015; Addo-Tenkorang and Helo, 2016; Hazen et al., 2016; Wang et al., 2016; Mishra et al., 2016).
P	This approach has been adopted by a number of highly cited review papers in SCM literature, such as Seuring and Muller (2008), Seuring (2013), and Govindan et al. (2015). 
O	The research before 2011 was rather insignificant in terms of volume and lacked considerable contributions to theory and practices (Manyika et al., 2011).
P	These criteria were developed and justified by the authors in order to minimize the impact of the subjective bias, as suggested by Tranfield et al. (2003).
P	This taxonomy has been widely adopted in BDA studies as it reflects complexity of both BDA-applied problems and data analytics techniques (Delen and Demirkan, 2013; Duan and Xiong, 2015).
O	Descriptive analytics is the simplest form of BDA, describing what happened in the past, while predictive analytics predicts future events, and prescriptive analytics refers to decision-making mechanism and tools (Rehman et al., 2016).
O	The third layer comprises the nine most common types of BDA model (Erl et al., 2016, p.181). 
O	The model aims to classify a huge set of data objects into predefined categories, thereby generating predictions with high levels of accuracy (Mastrogiannis et al., 2009). 
O	It is noteworthy that the BDA-driven routing problem is mainly studied in static environments based on historical databases (Ehmke et al., 2016; Zhang et al., 2016), while the use of BDA for dynamic routing optimization in real-time contexts is only conceptualized in some theoretical platform-based papers such as Sivamani et al. (2014) and Hsu et al. (2015).
O	Moreover, the application of BDA on logistics network planning has recently gained more attention, but is still under-examined in both strategic and operational levels (Zhao et al., 2016; Mehmood et al., 2017). 
O	Finally, the monitoring and controlling of product conditions through sensors during the in-transit process is seldom addressed (Delen et al., 2011; Ting et al., 2014).
O	Production planning and control is currently receiving the most research interest, and the application of BDA theories and tools on this topic is at a relatively mature stage (Wang and Zhang, 2016; Zhong et al., 2015).
O	Although BDA adoption in product R&D and equipment diagnosis and maintenance is less often studied, papers in this area make a significant contribution to predictive and prescriptive analytics in manufacturing research (Lei and Moon, 2015; Wang et al., 2015; Wang and Zhang, 2016; Zhang et al., 2017).
N	Moreover, research on BDA-enabled quality control during manufacturing processes is rather limited (Krumeich et al., 2016; Zhang et al., 2015).
O	Furthermore, only few studies address order-picking problems in BDA-enabled warehousing (Ballestín et al., 2013; Chuang et al., 2014).
P	BDA has been widely adopted to facilitate the supplier selection process and recent efforts have been made to integrate this activity with order allocation problems and to reduce sourcing costs (Kuo et al., 2015).
O	In terms of sourcing risk management, most studies have only exploited the benefit of BDA in accurately detecting procurement risk based on a sizable supplier database, while models and decision support systems (DSS) that provide proactive prevention actions are still lacking (Ghedini Ralha and Sarmento Silva, 2012; Miroslav et al., 2014).
P	BDA can help in sensing demand behaviours to increase the agility and accuracy of demand forecasting (Fang and Zhan, 2015; Salehan and Kim, 2016; Wang et al., 2014).
N	However, current studies on this issue have taken a marketing intelligence perspective rather than an operational SCM perspective (Marine-Roig and Anton Clavé, 2015; Schmidt et al., 2015). 
O	Most of those studies examined the SC integration in the context of SC resilience (Papadopoulos et al., 2017; Sheffi, 2015), sustainability (Papadopoulos et al., 2017; Wu et al., 2017), risk management (Ong et al., 2015), and agility (Giannakis and Louis, 2016).
P	To be more specific, logistics/transportation, manufacturing, and warehousing domains are the major contributors of prescriptive analytics, thanks to the increasing adoption of various state-of-the-art systems such as CPS in Industry 4.0 (Wang et al., 2016; Helo and Hao, 2017) and ITS (Wang et al., 2015).
O	On the other hand, predictive analytics is still the primary actor in demand management and procurement, especially for demand forecasting and sourcing risk management, while prescriptive analytics is still rarely discussed (Ghedini Ralha and Sarmento Silva, 2012).
O	On the other hand, the study of real-time optimization appears to be quite mature in the manufacturing domain with the use of modelling and simulation to develop a real-time production control system, based on streamline context-aware data, generated from tracking devices such as RFID (Babiceanu and Seker, 2016; Kumar et al., 2016).
O	It is highly possible for transportation controllers and warehouse operators to adapt a similar approach of modelling and simulation to optimize routing problem in real-time, as suggested in (Wang et al., 2016). 
P	Classification is the most common approach at the predictive analytics level and has been widely applied in manufacturing to support production planning and control (Chien et al., 2014; Wang and Zhang, 2016) and equipment maintenance and diagnosis (Kumar et al., 2016; Shu et al., 2016; Wang et al., 2016).
N	This type of BDA model also plays a key role in logistics/transportation (Li et al., 2014; Yu and AbdelAty, 2014; Zangenehpour et al., 2015) and procurement research (Ling Ho and Wen Shih, 2014; Mori et al., 2012) but apparently, current studies in those areas have not fully exploited the advantages of classification. 
P	It could be beneficial for future research to extend the application of this approach to more SC and operational management such as fraud detection (Miroslav et al., 2014) and behaviour-based safety analysis (Guo et al., 2016). 
O	For descriptive analytics, association is the most widespread as it has been applied throughout every stage of the SC process, from procurement (Ghedini Ralha and Sarmento Silva, 2012; Jain et al., 2014), manufacturing (Bae and Kim, 2011), warehousing (Chiang et al., 2011, 2014; Chuang et al., 2014), and logistics/transportation (Cui et al., 2016), to demand management (Jin et al., 2016). 
O	Notably, the visualization model is rarely considered as the main focus of a study (Zhong et al., 2016; Zhong et al., 2015), but is commonly used as a complement to other advanced data-mining models (Shan and Zhu, 2015; Zhang et al., 2016).
O	Most of those papers focus on the intelligent DSS that enables real-time control of the entire operational process in manufacturing (Dai et al., 2012; Krumeich et al., 2016; Zhang et al., 2017; Zhong et al., 2016), logistics/transportation (Delen et al., 2011; Dobre and Xhafa, 2014; Hsu, Lin, et al., 2015), and SC agility (Ong et al., 2015; Papadopoulos et al., 2017).
O	For instance, Kmeans clustering algorithm is among the most adaptable techniques as it can be adopted in clustering (St-Aubin et al., 2015; Tan and Lee, 2015), classification (Chien et al., 2014), forecasting (Stefanovic, 2015), and modelling and simulation (Lei and Moon, 2015). 
O	Studies find that advanced data-mining techniques such as decision trees and neural network would develop more accurate predictive models by leveraging the result of cluster analysis (Krumeich et al., 2016; Lei and Moon, 2015; Stefanovic, 2015). 
O	For example, Li et al. (2016) use ARM and Generic algorithm (GA) to optimize storage assignments, thus enhancing order-picking processes. 
O	Finally, ARM can also be used in the hybrid optimization problems of supplier selection and order allocation (Kuo et al., 2015).
O	The BDA that has already been applied in product lifecycle design and assessment (Ma et al., 2014; Song et al., 2016) would be useful for predicting product returns and estimating the return quality.
O	Coordinating a distributed SC and managing the complex procedures of different BDA are challenging (Li et al., 2016). 
O	We call for research on developing SC system-wide feedback and coordination based on BDA to optimize system performance (Wang et al., 2016).
P	M¨onch et al. (2011) characterizes several scheduling prob13 lems found in the different work areas in wafer fabrication facilities (wafer 14 fabs). 
O	M¨onch et al. (2011) identifies a s-batch environment in the photolithography work area but states that the p-batch environment is more important in this industry since it appears in the oxidation, deposition and diffusion work areas, as well as in the burn-in ovens in the chip testing facilities.
P	Single machines are not only interesting on their own but, as M¨onch et al. (2011) states, they can also form subproblems in decomposition schemes of more complex scheduling problems.
O	In case the batch has a limited size, this can be restricted by the number of jobs that the batch can process or by the existence of a size capacity for the batch, as explained in Matin et al. (2017).
P	This work relates to the problem of bounding the batches by the number of jobs as most articles do in the literature: Wang and Uzsoy (2002), He et al. (2015), Sabouni and Jolai (2010) and Aloulou et al. (2014) among others.
O	The length of these operations can be as short as a few hours, or as long as 48 hours, although some military applications require burn-in cycles with a total time of 240 hours (Mathirajan et al., 2004).
O	Also, in earlier operations like oxidation, deposition and diffusion there is no need to consider set-up times, as stated in M¨onch et al. (2011).
O	Following the standard three-field notation for sche55 duling problems (Graham et al., 1979), this problem can be denoted as 1|p − batch, b < n|(#batch, Lmax).
O	The single objective problem 1|p − batch, b < n|Lmax is shown to be unary (or strongly) NP-hard in Brucker et al. (1998).
O	Cabo et al. (2015) address the single-objective 1|p − batch, b < n|Lmax problem.
O	Hence, He et al. (2013) can obtain the Pareto-optimal schedules in polynomial time when studying Cmax together with a general cost function fmax.
O	More recently, Aloulou et al. (2014) and He et al. (2015) consider bounded machines.
O	Aloulou et al. (2014) present the problem of a 2-machine flow-shop to minimize the number of batches (#batch) and Cmax, and study its computational complexity, proposing approximation algorithms with guaranteed worst-case performance.
P	He et al. (2015) consider a single batching machine with the objective of minimizing Cmax and Lmax. 
P	Fan et al. (2012) study two settings, both minimizing Cmax but one focusing on the total weighted completion time (P 107 wjCj ), and the other on the total weighted number of tardy jobs (P 108 wjUj ).
P	Feng et al. (2013) present a 2-customer setting, where the first customer is interested in minimizing the makespan C A max, while the second requires minimizing the lateness L B max.
O	In Section 3, we extend the model proposed by Cabo et al. (2015) for the 1|p − batch, b < n|Lmax problem but that can be easily generalized for our bi-objective function.
P	The following definition characterizes a type of schedules that are optimal for any regular function when considering the unbounded batching-machine case (b ≥ n), as proved by Brucker et al. (1998).
O	Obtaining such an optimal schedule may be done in O(n log n) time or as stated in Brucker et al. (1998) in O(n2/b) time.
P	In Section 3.1, we propose a mixed-integer linear programming (MILP) for our problem based on the model by Cabo et al. (2015).
O	CMILP: Bi-objective MILP based on the model by Cabo et al. (2015).
N	We extend the model proposed by Cabo et al. (2015) that is intended for the 1|p−batch, b < n|Lmax problem but that can be easily generalized for our bi-objective function.
O	Like Cabo et al. (2015), we also define a big-constant Mk for each k ≤ n calculated in the following manner: find π the longest processing time order of jobs, such that pπ(1) ≥ pπ(2) ≥ . . . ≥ pπ(n) , then Mk = Pk j=1 pπ(j) the sum of the first k jobs with longest processing times.
O	It has proven to be competitive in solving large instances of problems (Damm et al., 2016; Ruiz et al., 2015). 
O	A general framework for the BRKGA is guided by the following steps (Toso and Resende, 2015; Mor´an-Mirabal et al., 2014).
N	Our BRKGA has a hybrid decoder (Tangpattanakul et al., 2013), that is, from a random-key vector we derive three different solutions and keep the one with the best fitness.
O	Initial tests with data from Cabo et al. (2015) yield a very flat Pareto-frontier, which did not help to highlight the difficulties and peculiarities of this new setting.
P	The benefits in reducing traffic congestion of system optimum with respect to user equilibrium traffic assignments are well-known (see, for example, Mahmassani and Peeta (1993) and Roughgarden and Tardos (2002)).
O	While system optimum approaches are used to coordinate traffic and possibly eliminate congestion when automated guided vehicles are involved (see Kaspi and Tanchoco (1990) and Bartlett et al. (2014)), the application of a system optimum approach to road networks has been considered to be not realistic due to fact that users take their own decisions.
O	User fairness was first considered in Jahn et al. (2005), where the arc travel times were fixed to the travel times experienced under user equilibrium assignment and only the paths with duration smaller than a fixed percentage of the least duration obtained under user equilibrium were considered as feasible paths.
O	In addition, a number of fairness measures were proposed to test the model such as the loaded unfairness (depending on the actual flow on arcs), the normal unfairness (depending on the used arc measure), the UE unfairness (depending on the experienced travel time and the travel time experienced under user equilibrium) and free flow unfairness (depending on the free flow travel time User fairness was also considered in Lujak et al. (2015), where a mathematical programming model was proposed based on Nash welfare optimization.
P	A linear programming based approach to control the trade-off between system and user perspectives, called proactive route guidance, was recently proposed in Angelelli et al. (2016).
P	In Ramming (2001) a literature review on the main path generation algorithms is provided and, according to Prato (2009), most of the path generation algorithms are based on repeated fastest path searches over the network. 
P	The well-known k-fastest path algorithm proposed by Yen (1971) is an example of generation methods based on the fastest path calculation.
P	Since then, several variants of the k-fastest path algorithm have been developed (see van der Zijpp and Catalano (2005) for details).
P	In de la Barra et al. (1993) a penalty algorithm in which impedance on links belonging to the fastest path is increased by a fixed percentage and a new fastest path search is performed.
O	Other variants are presented in Park and Rilett (1997), Bekhor and Prato (2006) and Bekhor et al. (2006).
O	The arc elimination algorithm was first presented in Azevedo et al. (1993) where all fastest path arcs are removed from the network at each iteration. 
O	Variants in which only some of the fastest path arcs are eliminated from the network are described in Prato and Bekhor (2006), Prato and Bekhor (2007) and Frejinger and Bierlaire (2007).
P	This method reflects the dynamics of traffic gradually entering the road network and it can be embedded into more sophisticated frameworks as in (see Hu et al. (2017), Hu et al. (2016) and references therein).
O	A large literature review on these topics can be found in Pugliese and Guerriero (2013).
O	Congestion problems are also studied from the point of view of congestion games (main references can be found in Tekin et al. (2012)).
O	In order to make the paper self-contained, we recall in this section the proactive route guidance approach of Angelelli et al. (2016).
O	This measure is similar to the one used in Jahn et al. (2005) to constraint paths but the free flow travel time is used instead of the UE travel time.
O	Moreover, even considering the impact of flow on experienced travel time through the so-called latency function (see Jahn et al. (2005) for details), the travel time rapidly grows when the arc congestion grows and, hence, limiting the maximum arc congestion results can be very useful.
O	In Angelelli et al. (2016) all the feasible paths from origin to destination for each OD pair are generated.
O	In Angelelli et al. (2016) it was observed that in the optimal solution of the proactive route guidance approach only few paths were used for each OD pair.
O	The comparison between the CE method and the HPG algorithm has been done also on a real-world instance from the Bar-Gera repository (Stabler (2018)).
O	Instances were generated taking into account different demand patterns and point attractiveness distributions as explained thoroughly in Angelelli et al. (2016) and are available at http://or-brescia.unibs.it/instances.
O	We also tested the HPG algorithm on a real-world benchmark instance from the Bar-Gera repository (see Stabler (2018)).
O	For τ > 15% we do not have results for the CE method but, since τ = 10% and τ = 15% are producing very similar results, we conjecture that, for τ > 10%, the proactive route guidance model is converging to a situation where all possible paths are eligible and, hence, ρCE becomes stable when increasing τ value as suggested by computational results in Angelelli et al. (2016).
O	On an operational level, supply chain integration is likely to reduce the waste of valuable resources and to increase profitability of the business (Leuschner et al., 2013).
O	Examples of such practice include inventory pooling (Berman et al., 2011; Zhong et al., 2017) and flexibly allocating workforce to multiple sites (Bhandari et al., 2008).
P	We, thereby, generalize existing routing and scheduling problems in this area (e.g., Dai et al. (2015), Stålhane et al. (2015), Irawan et al. (2017)), and show that a flexible workforce policy is a natural way to improve the efficiency of the supply chain.
O	Therefore, a coordinated approach in which the available technicians are jointly utilized between wind farms may increase the overall ef20 ficiency of the planning, for example, by reducing idle time of technicians (Shafiee, 2015; Irawan et al., 2017).
P	Ciancio et al. (2017) developed a heuristic solution approach for scheduling bus lines, and afterwards assigning crew to these lines.
O	Technician allocation touches the field of workforce management, of which its influence in routing problems is studied by Smilowitz et al. (2013) as well.
P	For example, Detti et al. (2017) have developed a hybrid heuristic combining variable neighborhood search and tabu search for a healthcare routing problem with many side constraints.
O	By incorporating the technician allocation, the TARP is a generalization of the short-term maintenance planning and routing problem at offshore wind farms, see Dai et al. (2015); Stålhane et al. (2015); Irawan et al. (2017).
P	Pickup and delivery problems (Dumas et al., 1991; Savelsbergh and Sol, 1995) consider the fulfillment of transportation requests by developing cost-minimizing vehicle routes. 
O	Following the classifications by Berbeglia et al. (2007) and Parragh et al. (2008), the TARP combines a one-to-one with a many-to-many pickup and delivery structure. 
P	Sophisticated exact solution approaches have been proposed for the one-to-one PDP with time windows, e.g., see Ropke et al. (2007); Ropke and Cordeau (2009); Baldacci et al. (2011).
P	We refer to Hernandez-P ´ erez ´ and Salazar-Gonzalez (2004) and Hern ´ andez-P ´ erez et al. (2009) for the single vehicle case, and Hern ´ andez-P ´ erez ´ and Salazar-Gonzalez (2014) and Hern ´ andez-P ´ erez et al. (2016) for the multi-commodity case.
O	One of the main areas of application is the so-called bike repositioning problem; a many-to-many pickup and delivery problem where one needs to find cost-minimizing routes for multiple vehicles that re-balance the inventory of bike rental systems (Dell et al., 2016; Bulhoes et al., 2017).
P	Maintenance routing and scheduling at offshore wind farms, with a given set of technicians to a single O&M base, is first introduced by Dai et al. (2015): A mixed integer programming model is presented and solved with standard commercial solvers.
P	An exact decomposition method for a single period presented by Stålhane et al. (2015).
O	Recently, an extension to multiple wind farms and multiple depots (TARP-G) is discussed by Irawan et al. (2017).
O	A similar simulation approach is taken by Post et al. (2017), incorporating routing heuristics to provide an in-depth analysis of an LNG transportation network.
P	The TARP-G is referred to as the Maintenance Routing and Scheduling Problem at offshore wind farms, and an exact solution approach based on a set partitioning formulation has been proposed by Irawan et al. (2017).
P	Adaptive Large Neighborhood Search (ALNS), as first introduced by Ropke and Pisinger (2006), is a metaheuristic often used to solve complex routing problems, see, for example, Mancini (2016) and Ghilas et al. (2016).
N	We evaluate the performance of the 2-ALNS by solving benchmark instances from the literature, obtained from Irawan et al. (2017).
O	Two existing sets of benchmark instances (G1 and G2) are obtained from Irawan et al. (2017). 
O	Since their problem is equivalent to the TARP-G in our setting, we compared the performance of the 2-ALNS with the exact approach by Irawan et al. (2017).
N	Although the objective values of the instances of benchmark set G2 are not proven to be optimal by Irawan et al. (2017), the 2-ALNS obtaines similar results.
O	The results indicate that often the complete symmetric allocation of technicians is best for the instances of Irawan et al. (2017), as solving the instances as TARP and TARP-F only leads to slight improvements.
O	Vanalle et al. (2017) explored the GSCM pressures, practices, and performance observed in suppliers of a Brazilian automotive supply chain.
O	Conventional location allocation models have been dealt extensively, on the other hand its integration with supply chain has not received the same attention in terms of aligning properties of conventional model with supply chain (Melo et al., 2009).
O	Interesting research gaps have been identified by the extensive literature review by Melo et al., (2009) on facility location and supply chain management and a few of them are as follows: 91% of studies focused either on cost or profit as objectives and only 9% discussed the inclusion of multiple objectives, in addition, only heuristics algorithms either general or specific have been proposed to single objective problems and so far only three studies have dealt with the automotive context. 
O	Furthermore recent studies envisage the need of environmental pressures, hence location allocation models have to consider carbon di oxide emissions in the location allocation decisions (Fahimnia et al. 2015; Soleimani et al. 2017). 
P	Data driven approaches are gaining popularity and are able to offer valuable solution in shorter time frame, increases the likelihood of transformations that are significant and provides greater focus in achieving the goals (LaValle et al., 2011).
O	Hence, this study extends the bi-objectives model proposed by Shankar et al. (2013) to an allocation decision situation which constitutes triple -objectives in a multi-echelon supply chain scenario that simultaneously optimizes total supply chain cost, maximizes fill rate and minimizes transportation CO2 emissions..
O	On the supply chain context Melo et al. (2009) did a comprehensive review for a decade to understand the gaps in supply chain decision in addition to typical location allocation decisions such as capacity, inventory, procurement, production, routing and transportation modes.
P	Moreno et al. (2016) proposed two facility location and transportation models for emergency logistics under uncertainty in a multi-product, multi-period and multi-model context.
O	On the multiple objective front an attempt by Fahimnia et al. (2015) to solve a supply chain planning model resulted to balance cost and emissions performance of the supply chain. 
O	Rohaninejad et al. (2017) presented mathematical formulations and an approximation algorithm to solve the facility location problem under competitive conditions in which all players (investors) compete with each other in obtaining the best clients and locations.
O	In the period of data deluge in the magnitude of 40 Zettabytes ranging from textual content, multimedia content and in different platform pressurizes supply chain manager to accommodate multiple perspectives before making a decision (Sivarajah et al., 2017).
P	In this context, Mousavi et al. (2013) proposed a new fuzzy mathematical programming-based probabilistic approach to solve a two phase mixed-integer programming (MIP) model for cross-docking systems design under a fuzzy environment.
P	In this regard, Jamshidi et al. (2012) developed a supply chain model with cost minimization along with environmental effects.
P	Following this, Harris et al. (2014) developed a multi-objective model including financial costs and CO2 emissions.
P	In this regard, Falcone et al. (2008) made an attempt to investigate the use of evolutionary algorithms (EA) to solve supply chain optimization problem with multiple objectives.
O	A study by Yank et al. (2016) included genetic algorithm with ILOG CPLEX solver to match energy requirements and availability of green energy in the same cluster using a location-allocation modelling context.
O	However in the circular economy context, Soleimani et al. (2017) dealt with profitability and customer satisfaction with an emphasis of environmental and societal responsibilities to design a closed loop supply chain including multiple levels, multiple products and multiple periods.
P	To overcome this issues Deb et al. (2002) then introduced the concept of NSGA-II, where they addressed the shortcomings of NSGA.
O	In this regard, Mallipeddi et al. (2011) identified a sensitive issue in the performance of DE which could be dealt with mutation and crossover strategies and their associated control parameters.
P	Hence, Deng et al. (2011) further enhanced DE with binary coding that includes new mutation rules
P	With several experiments, Deng et al. (2011) proved the effectiveness and validity of the new operator with two different combinatorial optimization problems.
P	In terms of heuristics or algorithms Murat et al. (2016) presented a global shooting algorithm for the facility location and capacity acquisition problem on a line with dense demand.
P	The model presented by Shankar et al. (2013) is extended to a three-objective model.
O	Non-dominated Sorting Genetic Algorithm (NSGA II) is a well-established multiple objective evolutionary algorithm that has potential to deal with complex model and applied in many combinatorial optimization context (Kadambala et al., 
O	The data regarding the first two objectives are taken from Shankar et al. (2013).
O	This study compares the results of NSDEA variants and MOHPSO (Shankar et al., 2013) for the two-objective problem, simultaneously minimizing TSCC and maximizing fill rate.
P	A study by Garcia et al. (20090 recommend to observe the behavior of evolutionary algorithms using a through statistical analysis.
P	This study conducted statistical analysis based TSCC generated by the five NSDEA variants and the MOHPSO values reported by Shankar et al. (2013).
O	Hence, simulation studies have to be used to compare the performance of dynamic pricing strategies and to analyze their strategic interaction in different oligopoly settings, see, e.g., Kephart et al. (2000) or Serth et al. (2017).
O	The demand coefficients defined in Example 4.1 are based on a large data set (>20 M observations/month) from the Amazon market for used books including 100 000 different products and up to K = 20 competitors, cf. Schlosser et al. (2016).
O	Nevertheless, our model also allows to calibrate sales probabilities using other approaches (e.g., using decision trees, see Quinlan (1986), or gradient-boosted decision trees, see Chen, Guestrin (2016), etc.).
O	We observe that the interplay of the strategic competitor’s and the price adjustments of our heuristic lead to specific cyclical price patterns, cf. Figure 4d, which have similarities to Edgeworth-price cycles, see, e.g., Maskin, Tirole (1988) or Noel (2007).
O	Analyzing the relation between realized sales, offer prices, and the underlying market situations that have been observed at the time of the price adjustment, cf. Section 4.1, makes it possible to estimate the conditional sales probabilities P˜ (1) t (i, a|p), see also, e.g., Vulcano et al. (2012), Abdallah, Vulcano (2016), and Fisher et al. (2017).
O	The application of the optimal and both heuristic strategies leads to cyclic price patterns over time, cf. Edgeworth cycles, see, e.g., Maskin, Tirole (1988) or Noel (2007).
O	Mutual best response strategies might use observations of offer prices to estimate probability distributions for the competitors’ remaining inventory (Hidden Markov Model), cf. Schlosser, Richly (2018).
P	The rank aggregation problem has been studied in various fields with different names such as median rank problem, social choice problem, Kemeny problem, etc. (Arrow, 1951; Kemeny, 1959; Kemeny and Snell, 1962; Dwork et al., 2001).
O	Determination of consensus ranking is an essential component of various models involving rank data, for instance, distance-based models (Mallows, 1957) and rank clustering (Heiser and D’Ambrosio, 2013).
O	One of the earliest attempts to formally conceive the idea of aggregating ranks traces to Nicholas of Cusa in 1435 (Sigmund, 1963).
O	On similar lines, de Borda (1781) offered a rather scientific analysis for deciding the election winners. 
P	Around the same period, de Condorcet (1785) proposed a method for pairwise comparison of alternatives and noted the intransitivity of the method, also known as the Condorcet’s paradox.
P	Kendall (1938) proposed a method to aggregate ranks based on the rank aggregation coefficient.
P	On similar lines, Kemeny (1959) defined a distance measure to find the best fit of the rankings.
P	Further, Kemeny and Snell (1962) proposed a set of three axioms to define a general distance metric.
O	Finding a consensus ranking under Kemeny framework, just like the traveling salesman problem, is an NP-hard problem (Bartholdi et al., 1989). 
O	With the work of Cohen and Mallows (1980) and Thompson (1993), statistical analysis of ranking was revived in the eighties and nineties (Heiser, 2004). 
P	Thompson (1993) and Heiser (2004), under a geometric configuration, established that the vertices of the permutation polytope form the sample space of rankings.
P	Other distance-based models under axiomatic frameworks have also been developed (Cook and Saipe, 1976; Cook et al., 1986; Cook, 2006; Cook et al., 2007).
N	However, due to differences in distance measure and axioms, similar to D’Ambrosio et al. (2017), the present paper is not comparable to those models.
P	Emond and Mason (2002) proposed a Branch-and-Bound algorithm to find a consensus ranking. 
O	Subsequently, Branch-and-Bound was employed by D’Ambrosio and Heiser (2009) and D’Ambrosio and Heiser (2016) for recursive partitioning algorithms for preference ranking.
N	The drawback of the algorithm is that its run-time becomes unmanageable when the number of objects is more than 25 (D’Ambrosio et al., 2015).
P	Ali and Meil˘a (2012) surveyed various approaches and performed empirical comparisons to determine algorithms that work well in practice
P	To overcome drawbacks in the Branch-and-Bound algorithm, D’Ambrosio et al. (2015) and Amodio et al. (2016) developed an algorithm, called QUICK, which, though similar to the Branch-and-Bound algorithm, produces a remarkable saving in the computational time.
P	D’Ambrosio et al. (2015) also proposed another algorithm, called FAST, which takes much longer than QUICK but tries to achieve an improved minimum in terms of Kemeny distance.
P	Recently, D’Ambrosio et al. (2017) proposed a differential evolution algorithm, called DECoR, which is able to arrive at solutions in reasonable computing time even for large n (up to 200).
O	Following Kemeny and Snell (1962), for a given ranking α of n objects, the diagonal elements of its corresponding n × n score matrix (aij ) are 0.
O	Kemeny (1959) introduced the ranking problem where the consensus ranking σ (also called Kemeny-optimal solution) is defined as σ = arg min ρ∈Zn totK(ρ) = arg min ρ∈Zn X k u=1 d(αu, ρ). 
O	Emond and Mason (2002), based on Kendall’s correlation coefficient, had defined an extended correlation coefficient τX between any two rankings α and β, which is related to d as follows:
O	Emond and Mason (2002) showed that minimizing total Kemeny distance is equivalent to maximizing average τX.
O	In the literature, it has been suggested that two or more rankings that achieve the same value of mintotK = min ρ∈Zn totK(ρ) are equivalent (Kemeny and Snell, 1962).
P	To overcome the issue, researchers have used multiple repetitions of algorithms with random starting points to pick the best solution (Hartigan and Wong, 1979; Hastie et al., 2009).
O	By and large, algorithms proposed in the literature tend to look out for a consensus ranking directly (Emond and Mason, 2002; D’Ambrosio et al., 2015, 2017).
P	For the comparison, we specifically consider three algorithms— QUICK, FAST, and DECoR (D’Ambrosio et al., 2015, 2017). 
P	The algorithm DECoR proposed by D’Ambrosio et al. (2017) is a heuristic algorithm that can deal with n up to 200 in reasonable computing time.
P	After this threshold of 50, DECoR is much faster than QUICK (D’Ambrosio et al., 2017).
O	To test the robustness of the algorithms, QUICK and FAST, D’Ambrosio et al. (2015) generated a random data of 100 objects with 15 attributes.
O	PrefLib repository (Mattei and Walsh, 2013) is a comprehensive resource for preference datasets.
O	#Kemeny distance for DECoR is reverse calculated from the average τX = 0.74215, as reported in D’Ambrosio et al. (2017).
P	In fact, their efficiency is considered a determining factor in measuring the overall terminal productivity according to Chung and Chan (2013).
P	For a comprehensive review of the different models and approaches used to address quay problems, the reader is referred to the surveys of Bierwirth and Meisel (2010, 2015). 
O	Although some research addressed the integrated operations for seaside problems (e.g. Diabat and Theodorou (2014); Fu et al. (2014); Msakni et al. (2016); Agra and Oliveira (2018)) or landside problems (e.g. Saini et al. (2017)), we focus in this review on the work exclusively related to the QCSP.
O	Kim and Park (2004) initiated a stream of works for the single ship QCSP where a task is defined as a container group.
O	Later, this model was subject to many improvements in the literature, e.g. Moccia et al. (2005), Bierwirth and Meisel (2009).
P	Sammarra et al. (2007) proposed a two-stage problem decomposition that was solved using a Tabu Search algorithm enhanced by a local search procedure.
O	Chung and Choy (2012); Kaveshgar et al. (2012); Chung and Chan (2013) implemented different versions of a Genetic Algorithm for the problem. 
P	Nguyen et al. (2013) developed an evolutionary heuristic based on Genetic Algorithm and Genetic Programming. 
P	Izquierdo et al. (2011) proposed another variant of evolutionary heuristic, called Estimation of Distribution Algorithms, which generates new solutions using a probabilistic model that is constantly updated with the statistical information of the individual solutions. 
P	Many techniques have been proposed to solve this problem variant: Meisel (2011) proposed a tree-search solution method, Monaco and Sammarra (2011) implemented a Tabu Search algorithm, Guo et al. (2013) presented a modified generalized extremal optimization (MGEO), Chen et al. (2014) developed a compact mathematical model, and Legato et al. (2012) proposed a solution method based on Timed Petri Net model.
O	In Zhang et al. (2008), the on-line bay QCSP was studied where the objective was to minimize the makespan subject to non-crossing constraints.
O	Some other works addressed the bay QCSP with only non-crossing constraints (e.g. Wang and Kim (2009), Lee and Chen (2010) and Liu et al. (2014)).
O	Recently, Zhang et al. (2017) developed approximate algorithms for multi QCs with a uniform work rate, and two QCs with different work rates.
P	In Hakam et al. (2012), the safety margin was included and the problem was solved using a genetic algorithm. 
P	Conversely, Wang et al. (2012) included the traveling time of QCs and proposed a Particle Swarm Optimization. 
P	Lee et al. (2011) developed a solution method for the bay QCSP in which the QCs are placed in an indented berth and subject to non-crossing andsafety constraints. 
P	Lu et al. (2012) included the concept of contiguous bay operations, known as area bay problem, and developed a polynomial time heuristic. 
P	Wang et al. (2013) took into consideration the vessel stability while building a solution for the bay QCSP.
O	Choo et al. (2010) tackled the single ship QCSP, where the QCs were subject to safety and non-crossing constraints, and the traveling time of QCs between bays was assumed negligible.
O	Al-Dhaheri et al. (2016a) considered the vessel stability constraints during loading/unloading containers on/off the vessel, along with nonzero traveling time of QCs between bays.
P	Few papers developed exact approaches to solve this problem, e.g. a branch-and-cut presented in Moccia et al. (2005), a branch-and price proposed by Choo et al. (2010) and a constraint programming model of Unsal and Oguz (2013).
O	Finally in the case of single container QCSP, the MIP models use decision variables based on QC-to-bay assignment over time to locate the position of each QC at each segment of the planning horizon, e.g. Choo et al. (2010), Al-Dhaheri and Diabat (2015), and Al-Dhaheri et al. (2016b).
O	Lee et al. (2008b) showed that the restricted version of the QCSP with non-crossing constraint is NP-complete, which means the more general problem addressed in this paper is also NP-complete.
P	In this regard, we used the lower bound described in AlDhaheri et al. (2016a), which is adapted from the work of Guan et al. (2013).
O	The experiments are based on the problem instances of Al-Dhaheri et al. (2016a) that include small-, medium-, and large-sized instances.
O	Since these methods depend on bound values, the heuristic presented in Section 4.4 as an upper bound and the lower bound of Guan et al. (2013) are used as follows.
O	Flow shop scheduling is a very common manufacturing layout, with a wide range of applications in production and other areas, e.g. in database applications (Allahverdi and Al-Anzi, 2002) or parallel computing (Sahni, 1995).
P	Flow shop scheduling is also the basis for scheduling robotic cell flowshops, which are widely used in modern manufacturing systems (Dawande et al., 2007).
P	There are n!max{m−2,1} candidates for the optimal schedule, since there always exists an optimal schedule with the same job sequence on the first two and on the last two machines (Conway et al., 1967).
O	Non-permutation schedules can be up to a factor O( √m) shorter than permutation schedules, and are often significantly shorter in practice (Liao et al., 2006; Potts et al., 1991; Tandon et al., 1991).
N	Finally, we provide a comprehensive experimental evaluation, which includes the large, hard instances of Vallada et al. (2015).
P	The constructive heuristic NEH (Nawaz et al., 1983; Taillard, 1990) is considered to be the best heuristic for the PFSSP (Dong et al., 2008; FarahmandRad et al., 2009; Fernandez-Viagas and Framinan, 2014; Kalczynski and Kamburowski, 2009; Rossi et al., 2016; Vasiljevic and Danilovic, 2015).
P	Many researchers have proposed improvements of the NEH heuristic (Dong et al., 2008; Fernandez-Viagas and Framinan, 2014; Kalczynski and Kamburowski, 2009; Vasiljevic and Danilovic, 2015).
P	The current best constructive heuristic with complexity O(n2m) was proposed by Fernandez-Viagas and Framinan (2014).
P	FarahmandRad et al. (2009) proposed NEH-based constructive methods that evaluate the relocation of a previously inserted job.
P	Rossi et al. (2016) proposed a similar approach of complexity O(n3m).
O	It reinserts pairs of consecutive jobs (π(k),π(k + 1)), visiting the pairs with odd k first and then those with even k (G8 order), and uses the initial order and the tie-breaking mechanism of NEH-KK2 (Kalczynski and Kamburowski, 2009) (F13 tie-breaking).
O	Among the metaheuristics applied to the PFSSP are tabu search (Eks¸ ioglu ˘ et al., 2008; Grabowski and Wodecki, 2004), scatter search (Haq et al., 2007; Nowicki and Smutnicki, 2006), genetic algorithms (Chang et al., 2013; Chen et al., 2012; Nagano et al., 2008; Ruiz et al., 2006), ant colony optimization (Ahmadizar, 2012; Rajendran and Ziegler, 2004), artificial bee colonies (Liu and Liu, 2013; Pan et al., 2014), differential evolution (Liu et al., 2014; Onwubolu and Davendra, 2006; Shao and Pi, 2016), iterated greedy algorithms (Fernandez-Viagas and Framinan, 2014; Ruiz and Stützle, 2007; Vallada and Ruiz, 2009), and hybrid methods (Lin et al., 2015; Lin and Ying, 2009; Tseng and Lin, 2009; Xie et al., 2014; Zobolas et al., 2009). Fernandez-Viagas et al. (2017) reimplemented and compared under the same conditions many state-of-the-art methods for the PFSSP and concluded that the iterated greedy algorithm of Fernandez-Viagas and Framinan (2014) is currently the best.
O	Rossit et al. (2017) have reviewed exhaustively the literature on the non-permutation FSSP, classifying the 72 articles published on this topic.
O	The best heuristics include the hybrid simulated annealing and tabu search of Lin and Ying (2009), and the ant colony algorithms of Yagmahan and Yenisey (2010), and Rossi and Lanzetta (2013, 2014).
P	To the best of our knowledge, the state-ofthe-art methods for the non-permutation FSSP are the constructive heuristic NFS and an iterated greedy heuristic which we have proposed in a previous article (Benavides and Ritt, 2016).
O	NFS achieves better results than other variations of NEH (Fernandez-Viagas and Framinan, 2014; Kalczynski and Kamburowski, 2009; Nawaz et al., 1983) for the PFSSP which take time O(n2m), and than the HFC heuristic (Koulamas, 1998) for the FSSP which needs time O(n2m2), but also takes more time.
O	The iterated greedy heuristic of Benavides and Ritt (2016) uses NFS to produce an initial solution, and then repeatedly perturbs the solutions by deconstruction and greedy reconstruction followed by a local search.
O	This neighbourhood extends the one of Nowicki and Smutnicki (1996) for the job shop scheduling problem, which swaps only two critical operations at the beginning or the end of a critical block.
P	This heuristic improved several previous results for the non-permutation FSSP, and also obtained better results than those of Fernandez-Viagas and Framinan (2014) for the PFSSP.
O	It is equivalent to NEH (Nawaz et al., 1983; Taillard, 1990) when the percentage of non-permutation insertions is p = 0%. It has time complexity O(n2m).
N	It breaks ties among straight insertion positions of equal makespan using the method of Fernandez-Viagas and Framinan (2014). 
O	It is equivalent to NEH (Nawaz et al., 1983; Taillard, 1990) when the percentage of non-permutation insertions is p = 0%.
O	It is equivalent to FRB5 (FarahmandRad et al., 2009) when the percentage of nonpermutation insertions is p = 0%.
P	It is equivalent to the heuristic G8 proposed by Rossi et al. (2016) when the percentage of non-permutation insertions is p = 0%.
P	The pair reinsertion local search proposed for the permutation and for the non-permutation FSSP is based on the improvement phase proposed by Rossi et al. (2016).
O	We set the number of jobs to d = 4 and the temperature factor to α = 0.4 in our implementations, following the calibration of Ruiz and Stützle (2007).
O	IG0, Ins is our implementation of the heuristic IG_RSLS (Ruiz and Stützle, 2007) for the PFSSP that uses the NEH heuristic and the insertion local search with the acceleration technique of Taillard.
N	We also have implemented a variant IGFF,Ins + TBFF that uses the tie breaker of Fernandez-Viagas and Framinan (2014) for permutation insertions in the reconstruction and local search phases. 
O	We have tested our algorithms on the 120 instances proposed by Taillard (1993) and the 480 instances proposed by Vallada et al. (2015).
P	The best known values are those reported by Taillard (2004) and Vallada et al. (2015) for the PFSSP.
O	In this case we apply the non-parametric test of Mack and Skillings (1980) for a two-way layout with an equal number of replications and the corresponding post-hoc test, as described in Hollander et al. (2014, chs. 7.9, 7.10).
O	The left part of Table 7 shows the ARDs for each group of ten instances of Taillard’s benchmark of the same size and the average ARD for the permutation heuristics (with p = 0% non-permutation insertions) for six different heuristics heuristics based on NEH: the original version of Nawaz et al. (1983), the variant NEHFF which uses the job priority order of Dong et al. (2008) and the tie-breaking proposed by Fernandez-Viagas and Framinan (2014), the variant G8 proposed by Rossi et al. (2016), the variant FRB5 proposed by FarahmandRad et al. (2009), and the heuristics BRPa and BRPc proposed here.
O	The right parts of Tables 7 and 8 show the results for the seven non-permutation heuristics BR and for the NFS heuristic of Benavides and Ritt (2016).
P	We have selected for the permutation FSSP the iterated greedy algorithm with an insertion local search IG_RSLS of Ruiz and Stützle (2007), and the iterated greedy algorithm IG+TBFF proposed by Fernandez-Viagas and Framinan (2014).
O	To relate these results to the literature and provide a perspective on the evolution of solution quality column “IG_RSLS” reports the values from Ruiz and Stützle (2007).
O	For the non-permutation case we compare the iterated greedy algorithm for non-permutation flow shop NFS+IGA(LS) of Benavides and Ritt (2016), to the nonpermutation version IGR, RNB.
O	In both cases we chose the second best construction method BRR, since method BRPa takes too long on the larger instances of Vallada et al. (2015).
P	The ARDs produced by IGBRFF,Ins + TBFF are a third better than those reported by Fernandez-Viagas and Framinan (2014) for the IG+TBFF for the same time limits.
O	We had access to the implementation of Fernandez-Viagas and Framinan (2014) and could verify that the methods are exactly the same and these differences come from different environments (language, compiler, and machine).
N	Fernandez-Viagas and Framinan (2014) have shown that their tie breaker improves over IG_RSLS.
O	In the permutation case, the average number of iterations for IG0, Ins and IGBRFF,Ins + TBFF confirm that the random tie breaker and the tie breaker of FernandezViagas and Framinan (2014) have similar performance.
N	To achieve these results, we propose a permutation representation for non-permutation flow shop schedules that uses pseudojobs to represent blocks of consecutive operations of jobs, and show that the acceleration technique of Taillard (1990) for the PFSSP can be extended to non-permutation flow shops using this representation.
P	In the location literature, they are mostly applied on p-median problems (see Mladenovi´c et al., 2007, for an overview).
O	Farham et al. (2015) considered the RLocP with congested regions having entry and exit points, called RLocP-CR-EE.
O	There are also a few studies in the literature considering the RLocP under rectilinear distance metric (see, for example, (Larson and Sadiq, 1983; Batta et al., 1989) for the barrier case and (Butt and Cavalier, 1997; Sarkar et al., 2004, 2009) for the congested region case).
O	This algorithm is similar to Floyd-Warshall algorithm (Cormen et al., 2009).
P	Farham et al. (2015) explained how a congested region with entry and exit points can be decomposed into several disjoint regions composed of barriers and congested regions as defined in this study.
P	Aneja and Parlar (1994) provided variants of the basic AP70 by removing some of its regions and reported their heuristic solutions.
O	The same approach was used by Bischoff and Klamroth (2007) who reported heuristic solutions for basic KC5 and KC10 instances in Katz and Cooper (1981) by approximating the circular regions with different regular polygons both from outside and inside. 
O	Similar strategy is used by Farham et al. (2015) to generate test instances including congested regions with passages.
P	Xb s of the RLocP-FR are found using the exact approach proposed in Aneja and Parlar (1994).
P	We used the Golden Section search technique given in Kiefer (1953).
O	To solve RLocP with convex polygonal barriers, Butt and Cavalier (1996) proposed a heuristic approach, called FORBID, which starts with an initial solution and iteratively solves unrestricted Weber problems with respect to the intermediate points on facility-demand paths corresponding to the current solution.
O	This subset is chosen due to the hypervolume indicator that measures the dominated region in the objective space bounded by some reference point (Zitzler and Thiele, 1998).
O	This problem arises in population-based heuristic methods and is used in the selection step of several multiobjective evolutionary algorithms (Fleischer, 2003; Knowles et al., 2003; Knowles and Corne, 2003; Zitzler and K¨unzli, 2004; Beume et al., 2007; Bader and Zitzler, 2011).
O	Moreover, algorithms for this problem can be used to support the decision makers in finding a representative subset with desired cardinality while fulfilling the quality guarantee that this set maximizes the hypervolume indicator; see discussion about the representation problem in Sayın (2000); Vaz et al. (2015) and references therein. 
O	This allows a constant ratio approximation with the classical greedy approach discussed in Nemhauser et al. (1978); see the application of this approach to the HSSP in Huband et al. (2007) and Guerreiro et al. (2016).
P	For more than two dimensions, explicit enumeration techniques are discussed in Bringmann and Friedrich (2010); only a dynamic programming approach has recently been proposed for the three-dimensional case (Bringmann et al., 2017).
N	In particular, we propose an integer programming formulation that extends the work in Kuhn et al. (2016) for the twodimensional case and that can be solved with any integer programming solver.
N	We introduce the following usual definitions of multiobjective optimization (Ehrgott, 2005) and hypervolume indicator (Guerreiro et al., 2016; Kuhn et al., 2016).
O	Bringmann and Friedrich (2008) have shown that the calculation of the hypervolume indicator is #P-hard.
P	In addition, the most efficient algorithm available for a general d has a time complexity of O(n d/3 polylog n) (Chan, 2013), except for the case of d = 3, which can be solved in O(n log n) (Beume et al., 2009).
P	Bader (2009) has conjectured that the HSSP is NP-hard for three and more dimensions and, only very recently, Bringmann et al. (2017) have shown that this is indeed true.
P	Later, Bringmann et al. (2014) proposed an algorithm with O(n(k + log n)) time complexity.
P	Very recently, Kuhn et al. (2016) proposed a different approach with O(k(n − k) + n log n) time complexity, by reducing the HSSP to a k-link shortest path formulation that can be solved with dynamic programming.
P	Very recently, Bringmann et al. (2017) proposed a dynamic programming algorithm with n O( √ k) time complexity for the particular case of d = 3, which improves upon the known time complexity bound for this particular case.
P	Heuristic approaches have also been proposed to solve the HSSP, e.g., Huband et al. (2003, 2007); Beume et al. (2007); Igel et al. (2007); Guerreiro et al. (2016);
P	Basseur et al. (2016). Of particular relevance for this work is the greedy algorithm proposed in Huband et al. (2007) and improved in Guerreiro et al. (2016).
O	This approach is based on the classical greedy algorithm in Nemhauser et al. (1978); it starts with an empty subset and at each of the k iterations, the point that has the largest hypervolume contribution to the subset is added to it. 
O	Guerreiro et al. (2016) discuss a speed-up technique for the three-dimensional case to efficiently update all the hypervolume contributions of the points not in the subset every time a new point is added to it, which runs in amortized linear time per point.
O	Noteworthy, the empirical approximation quality was observed to stay within 0.89 of the optimal values for low dimensions (Guerreiro et al., 2016).
P	Recently, Bringmann et al. (2017) have proposed an efficient polynomial-time approximation scheme for any dimension by solving subproblems that arise from the partitioning of the dominated region with an exponentially-sized grid.
N	We state the Integer Programming (IP) formulation for the HSSP with d = 3, which extends the formulation given in Kuhn et al. (2016).
N	We subdivide D(N) with respect to all given coordinate values from the set N of nondominated points (see other slightly different decomposition of D(N) in Lacour et al. (2017)).
O	This bound takes into account the approximation result of 1 − 1/e from the greedy algorithm (Guerreiro et al., 2016). 
O	Besides the advancements in technology, such as electric vehicles and energy-efficient appliances (Rezvani et al., 2015; Whitehead et al., 2015), policies also play a vital role in stimulating commuters’ behavioral changes to achieve sustainable urban environment.
O	Such a trading scheme is rigorously analyzed in transportation sector to be an alternative to the toll on road links (Yang and Wang, 2011; Wang et al., 2012; Bao et al., 2014; Akamatsu and Wada, 2017).
O	Xiao et al. (2013) demonstrated the effectiveness of an optimal credit charge scheme in eliminating the bottleneck queue. 
O	Bao et al. (2014) further investigated tradable credit scheme accounting for travelers’ loss aversion behavior.
P	It is similar to Personal Mobility Carbon Allowance Scheme which was proposed by Aziz et al. (2015).
P	Emerging technologies, such as ICTs, mobile sensing and automated vehicles, make it possible for the road manager to charge the credits anywhere and anytime if needed (Klein et al., 2016; Akamatsu and Kawa, 2017).  
O	Aziz et al. (2016) investigated the effect of personal mobility carbon credit scheme on the transportation networks. 
P	Xu et al. (2015) proposed the environmentally constrained traffic equilibrium problem and reformed it using the gap function for convenience of solvability.
P	Han et al. (2011) proposed complementarity formulations for dynamic user equilibrium under CTM and investigated the simultaneous path and departure time choices accounting for user heterogeneity. 
P	Ukkusuri et al. (2012) built upon the previous work and extended the complementarity formulation to multiple O-D pairs.
O	The first one is microscopic emission modeling (Barth et al., 1996; Panis et al., 2006).
O	Macroscopic modeling is based on the system indicators, such as flows and speed, which is commonly used in the network analysis (Aziz and Ukkusuri, 2013; Friesz et al., 2013; Aziz et al., 2016).
O	Joksimovic et al. (2005) addressed the optimal toll pricing problem in alleviating congestion by considering joint path and departure time choices and employed the grid search method to solve the bi-level problem under the uniform toll scenario and the dynamic toll scenario.  
P	Do Chung et al. (2012) considered the dynamic toll pricing under demand uncertainty and applied a robust optimization approach based on a bi-level cellular particle swarm optimization (BCPSO) to solve the congestion problem. 
O	Wang et al. (2014) formulated a bi-level programming to describe the network design problem (NDP) under tradable credit design with equity constraints and applied a relaxation method to solve the NDP problem.
O	Di et al. (2016) studied the bi-level second best toll pricing problem while considered the users in the lower level are of bounded rationality in path choices.
N	This research differs from previous researches of Aziz and Ukkusuri (2013) and Aziz et al. (2016) in the following ways. 
O	Aziz et al. (2016) focused on the flow redistribution under a given personal mobility carbon allowance scheme.
N	This work extends Aziz et al. (2016) by formulating bi-level model to investigate how to apply an optimal carbon credit charge scheme to attain mobility and emission goals. 
O	This section introduces the heterogeneous users’ simultaneous path and departure time choices in dynamic user equilibrium, which is based on Han et al. (2011) and Ukkusuri et al. (2012).
O	This section introduces the heterogeneous users’ simultaneous path and departure time choices in dynamic user equilibrium, which is based on Han et al. (2011) and Ukkusuri et al. (2012).
O	Several studies, such as Ukkusuri et al. (2012) and Aziz et al. (2016), show the applicability of this algorithm as an efficient heuristic method in obtaining converged local solutions. 
O	Other triangular-shaped tolls can also be seen in Wie et al. (2008) and Do Chung et al. (2012). 
O	Note that we have chosen to formulate the problem using the so called aggregate commodity definition (see Bienstock et. al. (1998)), as opposed to disaggregate model where each origin-destination pair is defined as a separate commodity.
O	The well-known 3-partition inequality (Magnanti et. al. (1993)), and the 4-partition inequality R1 (Agarwal (2006)) are special cases of this inequality. 
O	In addition to these inequalities, our implementation also uses the rounded metric inequalities described in Avella et. al. (2007).
P	Moreover, we demonstrate that a simple modification in the implementation of capacity formulation, which has been independently proposed, first by Fortz and Poss (2009), and later by Agarwal (2013), for other versions of network design problems, but has never been used for NDP, can lead to enormous computational gains compared to the other implementations of capacity formulation reported earlier (e.g. Avella et. al. (2007) and Bienstock et. al. (1998)).
P	Magnanti et. al. (1993) were the first to address this problem under the name of Network Loading Problem.
O	In a different work, Magnanti et. al. (1995) have addressed the two-facility version of the problem, for which they have shown that the cut-set inequalities are facet-defining under certain conditions.
O	Bienstock et. al. (1998), in a different work for the directed version of the problem, have developed partition-based metric inequalities and total-capacity inequalities, and have reported computational results with the same.
O	Avella et. al. (2007) discuss rounded and tight metric inequalities for this problem, and report computational results based on them. 
O	More recently, several authors have used Benders Partitioning based approaches (Babonneau and Vial (2010), Fortz and Poss (2009), and Costa et. al. (2009)) for solving this problem.
O	Avella et. al. (2007) by using bi-level programming to generate violated tight metric inequalities, and reports improved computational results.
O	Apart from Avella et. al. (2007), several authors (see Fortz and Poss (2009), Mattia (2011,2012) and Bienstock et. al. (1998), and Agarwal (2013)) have also use the capacity formulation for solving other versions of the problem.
O	It is well known that the separation problem of finding a violated metric inequality for a given capacity solution is a linear program (see Avella et. al. (2007)).
P	In some of these implementations (specifically Avella et. al. (2007), Mattia (2011,2012) and Bienstock et. al. (1998)), the authors continue adding the violated metric cuts at each node of the branch and cut tree until the current fractional solution is feasible with respect to all metric inequalities. 
O	The main approaches for solving polynomial programming problems include RLT relaxations (Sherali and Tuncbilek, 1992; Sherali and Adams, 1999; Amaral et al., 2008), αBB (Androulakis et al., 1995; Adjiman et al., 1998b,a), convexification and branch-and-bound (Tawarmalani and Sahinidis, 2002; Belotti et al., 2009; Misener and Floudas, 2014), semidefinite programming (SDP) relaxations (Lasserre, 2001, 2002; Parrilo, 2003; Burer and Vandenbussche, 2008; Gorge et al., 2015), sum of squares (SOS) relaxations (Waki et al., 2006; Kim et al., 2005), DC (difference of convex) programming (Tuy, 1987; Le Thi Hoai and Tao, 1997; Anstreicher, 2012), and B-spline (Gawali et al., 2017).
O	Ryoo and Sahinidis (2001) demonstrate that convex relaxations via arithmetic interval are at least as tight as the one generated via recursive application of arithmetic interval (termed as recursive McCormick in Luedtke et al. (2012)).
P	Luedtke et al. (2012) extend these results for multilinear terms having variables with symmetric bounds around zero.
O	For a quadrilinear term, Cafieri et al. (2010) demonstrate that recursive McCormick relaxations can be tightened by employing convex envelope of a trilinear term by Meyer and Floudas (2004). 
O	The first quadrification scheme we present, namely Scheme 1 (or recursive application of arithmetic interval in Ryoo and Sahinidis (2001) and recursive McCormick in Luedtke et al. (2012)), is based on the quadrification approach implemented in BARON (Tawarmalani and Sahinidis, 2004).
O	We utilized the dual optimizer of Cplex 12.51 (IBM ILOG CPLEX, 2013) for solving LP relaxations and snopt Version 7 (Gill et al. (2005)) to update the upper bounds on the polynomial programs using the optimal solution to the LP relaxation as the starting point.
O	Our approach can be extended to more general factorable programming problems (Sherali and Wang, 2001), generalized linear multiplicative programming (Wang and Liu, 2011), as well as mixed-integer nonlinear programming problems (Borchers and Mitchell, 1994; Cafieri and Rey, 2017).
O	There are many contributions on TSCP for several variants to the case presented above, see the seminal works Cohen and Lee (1989), Escudero (1994) and Shapiro (1993), among others.
O	The problem’s formulation is so-called deterministic equivalent model (DEM), see in e.g., Birge and Louveaux (2011) the main concepts on stochastic optimization.
O	A recent review of decomposition algorithms is presented in Aldasoro et al. (2017), most of the algorithms are intended for problem solving with moderate model dimensions.
O	For bigger instances, some types of scenario cluster decomposition approaches can be used, such as Branch-and-Fix Coordination (Aldasoro et al., 2017; AlonsoAyuso et al., 2003), two-stage Lagrangean decomposition (Carœand Schultz, 1999), Progressive Hedging algorithm (Gade et al., 2016) and multistage cluster Lagrangean decomposition (Escudero et al., 2016b), among others.
O	For instances with very large dimensions, such as real-life STSCP instances, matheuristic approaches should be used, as the algorithms that belong to the stochastic nested decomposition methodology, see Aldasoro et al. (2015), Cristobal et al. (2009), Escudero et al. (2015) and Zou et al. (2016).
O	Some approaches that present risk reduction measures are as follows: scenario immunization, see Dembo (1991) and its treatment in Escudero (1995), semi-deviations (Ahmed, 2006; Ogryczak and Ruszczynski, ´ 1999), min-risk (i.e, excess probabilities) (Ahmed, 2006; Schultz and Tiedemann, 2003), value-and-risk (Gaivoronski and Plug, 2005), conditional value-at-risk (CVaR) (Ahmed, 2006; Pflug and Pichler, 2015b; Rockafellar and Uryasev, 2000; Schultz and Tiedemann, 2006) and stochastic dominance (SD) strategies (Escudero et al., 2016a; Gollmer et al., 2011; 2008). 
O	Recent state-of-the-art surveys on risk management, specifically dealing with supply chains, can be found in Esmaeilikia (2013), Esmaeilikia et al. (2016a); 2016b), Fahimnia et al. (2015), Heckmann et al. (2015) and Ho et al. (2015).
O	For computational comparison of the time inconsistent versions of those measures, see Alonso-Ayuso et al. (2014).
O	However, there are only a few recent STSCP works dealing with risk averse measures, mainly CVaR in Alem and Morabito (2013) and Nickel et al. (2012), and meanrisk and minmax for robust solutions in Govindan and Fattahi (2017) presenting real-life cases for validating the proposed approaches, among others.
O	So, ECSD belongs to the type of risk averse measures that have the time consistency property as presented in Homem-de Mello and Pagnoncelli (2016).
N	Introducing a specialization of our stochastic nested decomposition (SND) approach in Cristobal et al. (2009) based on stochastic dynamic programming (SDP) for dealing with the proposed time-consistent risk averse measure, given the high number of cross-scenario constraints and, as a consequence, the nice scenario node-based modeling structure is lost.
O	Let us point out that it is beyond the scope of this work to present a methodology for multi-period scenario tree generation and reduction; see e.g., Dupacová et ˇ al. (2000), Heitsch and Römisch (2009), Hoyland et al. (2003), Leövey and Römisch (2015) and Pflug and Pichler (2015a) for ways to perform both tasks.
O	An S2 set Beale and Forrest (1976) is an ordered set of nonnegative variables {β f i ∀f ∈ Fi}, such that the sum of the variables in the set must be 1 and no more than two members may be nonzero with the further condition that if there are as many as two they must be adjacent.
P	The concept of the expected cost excess of the objective value on satisfying a given threshold have its roots in the Integrated Chance Constraints concept introduced in Klein (1986), see also Klein and van der Vlerk (2006).
O	Following the rationale in Pflug (2000) for the CVaR measure, it can be shown that TSD is a coherent risk measure, according to the standards set in Artzner et al. (1999); 2007).
O	In addition, it can also be shown Homem-de Mello and Pagnoncelli (2016) that the time-consistency of TSD, as defined below, depends on the bounds ep and θ p.
P	To perform the TSCP cost risk reduction, the multi-period stochastic dominance risk averse measure proposed in Escudero et al. (2016a) is considered.
P	The concept of expected conditional risk measure (ECRM) was introduced in Homem-de Mello and Pagnoncelli (2016), where the time-consistency is defined and proved, see also Asamov and Ruszczynski (2015), Pflug and Pichler (2015b), Rudloff et al. (2014), Ruszczynski ´ (2010) and Shapiro (2009).
O	See also below and Cristobal et al. (2009), Escudero et al. (2013), Escudero et al. (2015) and Zou et al. (2016), among others.
O	Note: The SND variant where there is not any Markovian relationship between the uncertainty in successive periods was introduced in Pereira and Pinto (1985)
P	In 1999, Schuurman and Woeginger (1999) listed 10 of the most prominent open problems around polynomial-time approximation algorithms for NP-hard scheduling problems at that time.
O	The idea in fixed-parameter algorithms is to accept exponential running times, which are seemingly inevitable in solving NP-hard problems, but to restrict them to certain aspects of the problem, which are captured by parameters (Cygan et al., 2015; Flum and Grohe, 2006; Niedermeier, 2006).
P	Thus, fixed-parameter algorithms can solve even large instances of NP-hard scheduling problems if the parameter takes only small values, the function f grows only moderately, and the polynomial degree in n is small (van Bevern et al., 2015).
O	Moreover, problems that are even difficult to approximate can be approximated efficiently and well in real-world instances using fixed-parameter approximation algorithms that exploit small parameters of real-world data (van Bevern et al., 2017a).
P	While fixed-parameter algorithms are now a well-investigated area of algorithmics, their systematic application to scheduling problems has gained momentum only recently (van Bevern and Pyatkin, 2016; Chen et al., 2017; Cieliebak et al., 2004; Halldórsson and Karlsson, 2006; Hermelin et al., 2015, 2017a; Jansen et al., 2017; Marx and Schlotter, 2011, and more references below).
O	This already led to the transfer of proof techniques from parameterized complexity to the world of scheduling, such as n-fold integer programming (Knop and Koutecký, 2017), color coding, problem kernelization (van Bevern et al., 2015), and W-hardness (Bentert et al., 2017; Bodlaender and Fellows, 1995; van Bevern et al., 2016, 2017b; Mnich and Wiese, 2015; Fellows and McCartin, 2003; Hermelin et al., 2017b).
O	It also led to the transfer of techniques from mathematical programming to parameterized complexity, such as convex integer programming (Mnich and Wiese, 2015) or parameterizing by structural properties of the integer feasible polytope of linear programs (Jansen and Klein, 2017).
P	Throughout this work, we use the standard three-field notation of scheduling problems due to Graham et al. (1979).
O	When jobs are partitioned into batches and a sequence-dependent setup time spq is needed when switching from a job of batch p to a job of batch q, then, in accordance with Allahverdi et al. (2008), we add “STsd,b” to the β-field.
P	It has been successfully applied to obtain effective polynomial-time data reduction algorithms for many NP-hard problems (Guo and Niedermeier, 2007; Kratsch, 2014) and also led to techniques for proving lower bounds on the effectivity of polynomial-time data reduction (Misra et al., 2011; Bodlaender et al., 2014).
O	How to find parameters that are both small in applications and lead to fixed-parameter algorithms, is a research branch on its own (Fellows et al., 2013; Komusiewicz and Niedermeier, 2012; Niedermeier, 2010). 
O	It is known that any problem having an EPTAS is fixedparameter tractable parameterized by the cost of an optimal solution (Cesati and Trevisan, 1997, Lemma 11).
N	Moreover, strongly NP-hard optimization problems with polynomially bounded objective functions do not have FPTASes unless P = NP (Garey and Johnson, 1979).
P	The problem 1||ΣwjT j is strongly NP-hard (Lawler, 1977; Lenstra et al., 1977), yet has two easier special cases: 1|pj=p|ΣwjT j is solvable in polynomial time via a reduction to the classical linear assignment problem, whereas 1||ΣT j is solvable in pseudo-polynomial time (Lawler, 1977; Lenstra et al., 1977).
O	To be more exact, Lawler (1977)’s pseudo polynomial-time algorithm is a fixed-parameter algorithm for 1||ΣT j parameterized by the maximum processing time pmax that also works for 1||ΣwjT j with agreeable processing times and weights, that is, pi < pj implies wi ≥ wj.
O	Ambühl and Mastrolilli (2009) showed that 1|prec|ΣwjCj is a special case of Weighted Vertex Cover.
P	Vertex Cover is one of the most well-studied problems in parameterized complexity theory, in particular in terms of problem kernels; small kernels for Weighted Vertex Cover were established by Etscheid et al. (2017).
N	The question is complicated by the fact that each vertex in the Weighted Vertex Cover instance created by Ambühl and Mastrolilli (2009), and thus each variable in the corresponding ILP, corresponds to a pair of jobs in the 1|prec|ΣwjCj instance, such that known data reduction rules for Weighted Vertex Cover do not allow for a straightforward interpretation in terms of jobs.
O	In a survey on open questions in maximum throughput scheduling, Sgall (2012) asked whether there is a polynomial-time algorithm for 1|rj , pj≤c|ΣUj for constant c.
P	The special case 1||ΣUj is polynomial-time solvable, whereas 1||ΣwjUj is weakly NP-hard and solvable in pseudo-polynomial time (Lawler and Moore, 1969; Karp, 1972).
O	Hermelin et al. (2017a) gave fixed-parameter algorithms for 1||ΣwjUj simultaneously parameterized by any two out of the following three parameters: the number of distinct due dates, the number of distinct processing times, and the number of distinct job weights.
O	Fellows and McCartin (2003) showed that 1|prec, pj=1|ΣUi is W[1]-hard parameterized by the number of tardy jobs but fixed-parameter tractable with respect to this parameter if the partial order induced by the precedence constraints has constant width.
O	For makespan minimization on a single machine, Billaut and Sourd (2009) gave an algorithm that runs in n O(τ 2 ) time for τ forbidden start times and n jobs; this was improved by Rapine and Brauner (2013) to n O(τ) time.
O	Job shop scheduling problems (JSPs) are usually studied under the assumption that all machines are always available, which is the so-called deterministic model (Jain and Meeran, 1999; Pinedo, 2016; Calis and Bulkan, 2015). 
P	The schedule performance will change stochastically with RMBs, for which one feasible way is to select a schedule with the optimal expected schedule performance (Kasap, Aytug and Paul, 2006; Zandieh and Gholami, 2009; von Hoyningen-Huene and Kiesmueller, 2015).
O	Robust scheduling has been used to stabilize the schedule performance (Bertsimas and Sim, 2004; Goren and Sabuncuoglu, 2010), where the robustness of a schedule is generally measured by the expected deviation from its initial performance. 
O	Since the robustness measure can hardly be evaluated analytically, one has to resort to the Monte Carlo simulation (Jin and Branke, 2005) at the cost of intensive computation (Liu, Gu and Xi, 2007). 
O	For example, the LPT (Longest Processing Time first) rule can be used to optimize the expected makespan (Kasap, et al., 2006) and the total weighted completion time (Huo, Reznichenko and Zhao, 2014), while the SPT (Shortest Processing Time first) rule can minimize the expected total waiting time (Aytug and Paul, 2012). 
O	When the processing time is also variable, the SEPT (Shortest Expected Processing Times first) rule provides an optimal sequence for the case of job processing time with position-based learning effect (Zhang, Wu and Zhou, 2013), while the index policy can be further used for the case of job processing time with linear deterioration (Cai, Wu and Zhou, 2011), with respect to the expected makespan.
O	Furthermore, the optimal sequences, in both the stochastic single machine just in time scheduling (Tang, Zhao and Cheng, 2008) and the single machine scheduling with randomly compressible processing times (Qi, Yin and Birge, 2000), possess a V-shape property with respect to the due date-based measure.
P	In addition to the single machine scheduling, Allahverdi and Mittenthal (Allahverdi, et al., 1995) proposed a dominance rule based on Johnson’s rule to minimize the expected makespan for a two-machine flow shop.
O	Accordingly, Xiong, Xing and Chen (Xiong, et al., 2013) took the weighted total slack time of all operations as the surrogate robustness measure: where wij is the workload of the machine on which operation Oij is processed and wtot is the sum of workloads of all machines.
P	In our experiments, we apply the GA proposed by Della Croce, Tadei and Volta (Della Croce, et. al., 1995) to optimize the schedule risk.
P	In this paper we adapt the -Robustness paradigm developed by Bertsimas and Sim (2004) to the case where coefficients in constraints depend on data the variation of which impacts the coefficient in a non-linear way.
O	The framework developed in Bertsimas and Sim (2004) generates a robust counterpart for P that ensures feasibility of the robust solution whatever the variation of coefficients satisfying a given level of conservatism.
O	The -robustness approach (see Bertsimas and Sim, 2004 for the construction of the robust counterpart of P) extends the more conservative approach of Soyster (1973) and has the advantage to be more tractable than the non-linear approach of Ben-Tal and Nemirovski (1999) based on ellipsoidal uncertainty, as the robust model remains an LP.
O	Right-Hand Side (RHS) uncertainty (a recent survey on robust optimization can be found in Gabrel et al., 2014):
O	Bertsimas et al. (2013) and Lorca and Sun (2015) address the unit commitment problem in the energy sector.
P	Büsing and D’Andreagiovanni (2014) propose an extension on the distribution of the uncertainty set, which they call multiband uncertainty.
P	Büsing and D’Andreagiovanni (2014) propose to break the set into multiple narrower ”bands”, each with a customized  value, according to historical data.
P	Similarly, in Düzgün (2012); Düzgün and Thiele (2010, 2015), the authors propose the use of multiple ranges for the uncertain parameters, limiting overly conservative solutions when the uncertainty set is too wide.
O	Düzgün and Thiele (2015) bound as well the number of parameters that fall into each range (pessimistic view), but also bound the number of parameters that take the worst-case value within each range.
O	In Multinomial Logit (MNL) choice models (Mc Fadden, 1973), the probability pij of customer i choosing product j is expressed by pij = euij /(k euik ) which is a non-linear function of utilities uij that are known to be hard to calibrate precisely (see Espinoza García and Alfandari, 2015 for an application to the location of new housing developments).
O	Note that several papers deal with non-linear robust optimization, but their goal is more to study the robust counterpart of a non-linear problem, i.e. the nominal problem comprises non-linear functions of the variables (see for example Ben-Tal et al., 2017; Ben-Tal and Nemirovski, 1998; Ben-Tal et al., 2002; Diehl et al., 2006; Houska and Diehl, 2013; Kawas and Thiele, 2011; Takeda et al., 2008; Zhang, 2007).
O	A robust network design problem was studied in Pessoa and Poss (2015) which was a linear program with a quadratic dependency on uncertainty in a constraint.
O	As a case in point, the total costs of U.S. domestic air traffic delays and disruptions in year 2007 were estimated at approximately $31.2 billion (Ball et al., 2010).
P	Of these, $8.3 billion represents incremental operating costs due to delays to the airlines (including additional fuel, labor and material costs), $16.7 billion represents the estimated costs the passengers, and the remaining $6.2 billion represents indirect costs in the form of lost demand and impact on GDP (Ball et al., 2010).
P	Though robust planning adds value at every step of the airline scheduling process, we choose aircraft routing because of its high impact on schedule reliability and relatively low impact on flight operating costs, crew costs, and passenger revenues (Lan et al., 2006).
O	Sohoni et al. (2011) define two service-level metrics, flight service level and network service level, to capture airline performance and passenger connectivity respectively.
O	Arikan et al. (2013) build stochastic models of airline networks to identify bottleneck airports and flights, identify metrics for measuring robustness and make schedules more robust.
O	Burke et al. (2010) build a memetic approach for multi-objective improvement of robustness-influencing-features (called robustness objectives) in airline schedules.
P	Lan et al. (2006) introduce the term delay propagation and propose an aircraft routing model to reduce delay along the downstream flight legs.
O	AhmadBeygi et al. (2010) expand on this notion of propagated delay using the concept of propagation trees.
P	Dunbar et al. (2012) present a way to minimize propagated delay by making both aircraft routing and crew pairing decisions in an integrated fashion capturing associated dependencies.
O	The importance of effective lot-sizing decisions in production systems with setup times has been recognized for decades, beginning with the Economic Lot-Sizing model (Harris 1915; Cardenas-Barron et al. 2014).
O	In a previous paper (Kang et al. 2014) we present an exploratory analysis of these issues for a multiproduct single machine dynamic lot-sizing problem.
P	This paper presents an improved heuristic for the NIP model formulated in Kang et al. (2014).
O	We also compare our results to those of two benchmark heuristics: a version of the Feasibility Pump (Fischetti et al. 2005; D'Ambrosio et al. 2012) and the Surrogate Problem method of Gokbayrak and Cassandras (2001; 2002).
O	Most of the extensive literature on lot-sizing models (Brahimi et al. 2006; Quadt and Kuhn 2008; Buschkühl et al. 2010) focuses on the trade-off between setup cost and finished goods inventory holding cost, resulting in a fixed-charge problem structure.
O	Florian et al. (1980) and Bitran and Yanasse (1982) have shown that the single product lot-sizing problem with capacity constraints belongs to the class of NP-hard (Nondeterministic Polynomial Time Hard) problems, for which an exact procedure that runs in polynomial time is unlikely to exist (Garey and Johnson 1979).
O	Later dynamic programming models have focused on reducing computational time by exploiting special structure (Federgruen and Tzur 1991; Wagelmans et al. 1992; Chen et al. 1995), extending the assumptions of the problem (Lee et al. 2001) and exploring the optimality of dynamic programming algorithms in a rolling horizon context (Lee and Denardo 1986; Stadtler 2000; Fisher et al. 2001; Van den Heuvel and Wagelmans 2005). 
O	Capacitated versions of the problem, where the number of units that can be produced in a given period is limited, have been addressed by mixed integer programming models that are solved using polyhedral approaches (Pochet and Wolsey 2006), Lagrangian approaches (Billington et al. 1983; Karmarkar and Schrage 1985; Trigeiro et al. 1989; Tempelmeier and Derstroff 1996) and column generation (Manne 1958; Lasdon 1970; Bitran and Matsuo 1986; DeGraeve and Jans 2007).
O	Bruno et al. (2014) discuss logistics problems that can be formulated as capacitated lot-sizing problems.
O	A number of authors (Karmarkar et al. 1992; Lambrecht et al. 1998; Kuik and Tielemans 1999; Choi and Enns 2004; Van Nyen et al. 2005; Vaughan 2006; Koo et al. 2007; Koo et al. 2011; Rabta and Reiner 2012) have used different steady-state queueing models to obtain optimal lot sizes and production rates.
O	In Europe, a convoy of more than 12 ATs completed a European cross-border journey in April 2016 (The Guardian, 2016).
O	In such a setting, fuel savings of around 5% can be achieved by the lead truck, and between 10–15% for the follower trucks (Tsugawa, 2012).
O	In the US, the first freight shipment with an AT was made on the 25th of October 2016 (Wired.com, 2016).
O	In the US, there was a shortage of 48,000 truck drivers in 2015, which is predicted to be 175,000 by 2024 under the current trend (Costello and Suarez, 2015).
O	Traffic crashes including trucks represent 10% of all crashes in the US, and more than 90% of accident causes are due to driver errors (Singh, 2015).
P	There have been several strategies proposed for the implementation of ATs, including lane reservation (Fang et al., 2013) and exclusive assignment of an existing infrastructure (Wu et al., 2017), although both are deemed to be expensive and restrictive with respect to the applicability of this new technology (Vanholme et al., 2013).
O	A preferred feasible strategy, at least for the foreseeable future, is for ATs to share the same infrastructure with conventional vehicles (driven by humans) (Vanholme et al., 2013).
P	The problem we consider is similar to the Pollution-Routing Problem (PRP) introduced by Bektas¸ and Laporte (2011), in that we treat the choice of speed at which the vehicle will want to travel at on a given segment of road as a decision variable as part of devising the operational routing plans.
P	For a wider and comprehensive overview of the green transportation, including air and maritime transportation, we refer the reader to Psaraftis (2016).
P	For recent reviews of the literature on green road freight transportation, we refer the reader to Demir et al. (2014b) where the authors review gas emissions models, and to Lin et al. (2014), Eglese and Bektas¸ (2014) and Bektas¸ et al. (2016) which are surveys of green VRPs.
O	In the original paper of Bektas¸ and Laporte (2011), the PRP makes use of the comprehensive fuel consumption model proposed by Scora and Barth (2006), and is modeled through a non-linear mathematical formulation in which the decision variables correspond to the selection of routes and speeds.
O	Dabia et al. (2016) described a branch-and-price algorithm for the PRP based on a column generation mechanism in which the master problem is a set partitioning problem.
P	Maden et al. (2010) considered the VRP with time windows and time-dependent speeds, and developed a tabu search heuristic in which the objective is to minimize the total time spent by the vehicles.
O	The most common sources of uncertainty in stochastic vehicle routing are customer availability, customer demand, service times and travel times (Gendreau et al., 2016).
N	In these variants, some key concepts such as feasibility or optimality are not defined in the same way as in the deterministic VRP, which increases the difficulty of solving such problems (Cordeau et al., 2007).
P	The most relevant variant to our paper is the VRP with stochastic travel times, as described by Laporte et al. (1992), who modeled the problem as a two-stage stochastic program for the first time.
O	Hwang and Ouyang (2015) considered the full-truck delivery problem under uncertainty, defined on a road network where nodes represent major intersections and directed arcs represent route segments between them.
O	While this may seem a relatively straightforward modification of the objective function, it does make a difference since it has been shown that the largest component of the PRP objective function is the driver’s wage (Bektas¸ and Laporte, 2011). Table 1 summarizes the differences between the papers cited above and our paper.
P	Comprehensive reviews of earlier studies and computational approaches on cell formation can be found in Papaioannou and Wilson (2010) and Albadawi et al (2005).
O	Though a multi-objective form combining individual objective functions is possible (Yasuda et al, 2005; Vin et al, 2005), most methods seek a cell formation solution that is suited for a chosen criterion in the final objective function.
P	Manzini et al. studied the effect of a minimum similarity level so that a group may be formed (Manzini et al, 2010). 
O	With the rapidly growing demand for home services and the competitive pressure from market, the service providers face the challenge to make a set of complex decisions, in which service operations planning is a critical one (Hulshof et al. 2012).
N	To avoid delivery failure, the service providers normally quote an appointment time (planned service start time) to each customer in advance (Agatz et al. 2008).
O	If a team arrives later than the appointment time at a customer’s home, the customer has to wait, incurring waiting cost (inconvenience and anxiety due to lateness of the teams, see Braekers et al. 2016).
O	Taş et al. (2013, 2014) and Yuan et al. (2015) study the VRP with soft time windows (VRPSTW), where the service at each customer node should start within an associated time window, otherwise penalty cost incurs. 
P	Maintenance planning and scheduling has been widely studied in the electricity industry, particularly for thermal power plants; see Froger et al. (2016) for a comprehensive review.
O	Studies usually focus on a single turbine or a single wind farm and explore why failures happen, what should be done when they occur, and how they can be predicted or prevented. See Ding et al. (2013) for a survey.
O	Kov´acs et al. (2011) scheduled maintenance for onshore wind turbines over a one-day horizon.
P	For offshore wind farms, Irawan et al. (2017) solved a maintenance routing and scheduling problem minimizing labor, travel, and penalty costs.
P	We consider the wind turbine maintenance scheduling problem, focusing on onshore wind farms, introduced by Froger et al. (2017).
P	Froger et al. (2017) introduced several models based on ILP and constraint programming (CP).
P	In Section 4 we discuss experiments with the 160-instance testbed proposed by Froger et al. (2017).
N	By polynomially reducing the cumulative scheduling problem5 , which is known to be NP-complete in the strong sense (Baptiste et al. 1999), to WTMSPdec, we easily prove that WTMSP is strongly NP-hard.
P	This formulation is broadly inspired by the model introduced by Froger et al. (2017).
N	The fundamental difference between our model and that of Froger et al. (2017) is that our model does not assign technicians to plans.
O	This cut is similar to those defined for logic-based Benders decomposition (Hooker and Ottosson 2003) and have some similarities with the integer optimality cuts for the integer L-shaped method (Laporte and Louveaux 1993).
P	The B&C framework was introduced by Thorsteinsson (2001); it is primarily designed for LP and CP hybridization.
O	Thorsteinsson (2001) applied B&C to a planning and scheduling problem, while Sadykov (2008) used it for a complex scheduling problem on a single machine.
O	This approach is also referred to as a Benders-based branch-and-cut algorithm (Naoum-Sawaya and Elhedhli 2010) and as a branch-andBenders-cut method (Gendron et al. 2014).
O	It has been used to solve several types of problems: hub location (De Camargo et al. 2011), production routing under demand uncertainty (Adulyasak et al. 2015), location-design (Gendron et al. 2014), facility location and network design (NaoumSawaya and Elhedhli 2010), and hop-constrained survivable network design (Botton et al. 2013).
P	Botton et al. (2013) reported a significant improvement using this approach instead of the classical implementation of Benders decomposition, while Gendron et al. (2014) outlined the benefits in terms of solution quality, scalability, and robustness
P	We use the algorithms introduced by Osterg˚ard (2001) and ¨ Osterg˚ard (2002) to solve the maximum clique and ¨ MWC problems.
P	We report results for the 160-instance testbed proposed by Froger et al. (2017).
O	Froger et al. (2017) considered time horizons of different lengths (5 and 10 days with 2 or 4 periods per day), different numbers of tasks (20, 40, 80), and different numbers of skills (1 or 3). 
O	For a discussion of the instance generation process see Froger et al. (2017).
O	To hedge against demand uncertainty, we use a robust optimization framework: among all hub networks that are feasible for all possible demand realizations, we would like to find one that minimizes the worst case total cost (for more on robust optimization see, e.g., Atamtürk (2006); Ben-Tal et al. (2004); Ben-Tal and Nemirovski (1998); 1999); 2008); Bertsimas and Sim (2003); 2004); Mudchanatongsuk et al. (2008); Ordóñez and Zhao (2007); Yaman et al. (2001); 2007)).
P	The hose model was proposed by Duffield et al. (1999) and Fingerhut et al. (1997) to design virtual private networks.
O	Due to these advantages, the hose model has been used as an uncertainty model in many studies following its introduction (some examples are Altın et al. (2007); 2011); Chekuri et al. (2007); Italiano et al. (2006)).
O	Recently, Meraklı and Yaman (2016) study the uncapacitated multiple allocation p-hub median problem with polyhedral demand uncertainty.
P	Contreras et al. (2011a) propose a Benders decomposition in which they generate cuts for each candidate hub location instead of each origin-destination pair.
P	Camargo et al. (2009) propose two Benders decomposition algorithms to solve the variant of the problem where the cost is a piecewise-linear concave function.
N	Gelareh and Nickel (2011) study a problem with an incomplete hub network and solve this problem with a Benders decomposition algorithm.
P	The first mixed integer linear programming formulation for the capacitated multiple allocation hub location problem (CMAHLP) is proposed by Campbell (1992) using four indexed variables.
O	Sasaki and Fukushima (2003) consider a capacitated multiple allocation hub location problem where a capacity constraint is applied both on hubs and arcs and a flow can go through at most one hub on its way from origin to destination.
P	Marín (2005a) also provides new formulations and resolution techniques to obtain better computational results and succeeds to solve instances with up to 75 nodes.
P	In order to strengthen these formulations, Boland et al. (2004) propose preprocessing procedures and valid inequalities, which lead to a significant reduction in the computation times. 
O	Ebery et al. (2000) provide formulations with three indices and devise a heuristic algorithm to solve large instances.
N	RodríguezMartín and Salazar-González (2008) consider a capacitated hub location problem with multiple allocation on an incomplete hub network.
O	Contreras et al. (2012) also study a related capacitated hub location problem in which the capacities installed on each hub is not a parameter but a decision variable.
O	Here we limit ourselves to related studies and refer the reader to surveys in Campbell (1994b), and Alumur and Kara (2008); Campbell et al. (2002); Campbell and O’Kelly (2012); Klincewicz (1998); O’Kelly and Miller (1994) and Farahani et al. (2013) for further information.
O	Marianov and Serra (2003) study the problem in an air transportation network where hubs are M/D/c queues and the probability that the number of planes in the queue exceeds a certain number is bounded above. 
P	The multiple allocation hub location problem is first formulated by Campbell (1994a). Boland et al. (2004); Camargo et al. (2008); Cánovas et al. (2007); Ebery et al. (2000); Ernst and Krishnamoorthy (1998a); Hamacher et al. (2004); Klincewicz (1996); Marín (2005b); Mayer and Wagner (2002) and Contreras et al. (2011a) propose methods to solve this problem.
P	The version of the problem where there is no cost for opening hubs but the number of hubs is fixed to p is first formulated by Campbell (1992).
O	Alternative formulations are given by Campbell (1994a); Skorin-Kapov et al. (1996) and Ernst and Krishnamoorthy (1998a).
P	Campbell (1996) and Ernst and Krishnamoorthy (1998a); 1998b) propose exact and heuristic solution algorithms.
O	Yang (2009) decides on hub locations and flight routes under demand uncertainty using two-stage stochastic programming.
O	Sim et al. (2009) incorporate service level considerations using chance constraints when travel times are normally distributed.
O	Contreras et al. (2011b) consider the uncapacitated multiple allocation hub location problem under demand and transportation cost uncertainty.
O	Alumur et al. (2012) consider uncertainty both in fixed costs and demands.
P	Shahabi and Unnikrishnan (2014) propose mixed integer conic quadratic programming formulations for hub location problems with ellipsoidal demand uncertainty
O	Meraklı and Yaman (2016) study the uncapacitated multiple allocation p-hub median problem with hose demand uncertainty and present Benders decomposition based algorithms.
P	We use the hose model introduced by Duffield et al. (1999) and Fingerhut et al. (1997) which is commonly used in the telecommunications literature to represent the demand uncertainty.
P	We use the formulation proposed by Hamacher et al. (2004) as a starting point.
N	This formulation is devised for the uncapacitated version of the problem, hence we adjust it by adding a set of capacity constraints as proposed in Ebery et al. (2000).
P	To linearize it, we use a dual transformation, which is widely used in the robust optimization literature (see, e.g., Bertsimas and Sim, 2003 and Altın et al., 2011).
O	Nemhauser and Trotter Jr (1974) show that any extreme point ϑ of this LP satisfies ϑi ∈ {0, 1/2, 1} for all i ∈ N.
O	They are also NP-hard (Fowler et al., 1981; Nielsen and Odgaard, 2003) and combine the difficulty of cutting and packing problems with the geometric complexity of placing irregular convex and nonconvex pieces.
O	Consequently, a set of tools must be developed to deal with the geometry of the pieces, which is not a trivial task and may be a barrier to new research in the area (Bennell and Oliveira, 2008).
P	According to the typology proposed by Wäscher et al. (2007), this problem can be classified as open, two-dimensional and irregular.
P	Several authors have developed heuristic methods to solve the problem (see Dowsland and Dowsland (1995); Bennell and Oliveira (2009) for a review).
O	Most studies that employ constructive heuristics are based on bottom-left (Oliveira et al., 2000; Dowsland et al., 2002) and bottom-left-fill (Burke et al., 2006, 2010) algorithms, which sequentially insert each piece in the leftmost possible position on the board and in the bottommost position, in case of a tie.
O	Hybrid algorithms (Bennell and Dowsland, 2001; Gomes and Oliveira, 2006), guided local search (Egeblad et al., 2007; Umetani et al., 2009), iterated local search (Imamichi et al., 2009), beam search (Bennell and Song, 2010), extended local search (Leung et al., 2012), simulated annealing with collision free region (Sato et al., 2012), cuckoo search (Elkeran, 2013) and genetic algorithm (Pinheiro et al., 2015) are some of the algorithms applied to tackle the problem.
P	Ribeiro et al. (1999) developed a search strategy to solve the strip packing problem with convex pieces.
P	Carravilla et al. (2003) extended the previous study to deal with nonconvex irregular pieces and developed symmetry breaking constraints to handle pieces of same shape. 
P	Fischetti and Luzzi (2009) proposed the first mixed integer programming model for the problem based on Daniels et al. (1994) and Li (1994).
O	Álvarez-Valdés et al. (2013) defined a procedure to obtain the partitions described by Fischetti and Luzzi (2009) using horizontal slices.
P	They extended the linear compaction model proposed by Gomes and Oliveira (2006) to deal with the strip packing problem with convex pieces and reformulated the Fischetti and Luzzi (2009) model, lifting both lower and upper bounds of the continuous variables.
P	Toledo et al. (2013) proposed the dotted-board model, a mixed integer programming model in which the board is represented by a grid.
P	Section 2 defines the problem and reviews the dotted-board model proposed by Toledo et al. (2013);
O	This study is based on the dotted-board model (Toledo et al., 2013), a mixed integer programming model for the irregular strip packing problem, in which the board is represented by a grid.
P	They generated an initial clique covering, expanded each clique using a maximal clique generation algorithm and used the post-processing method proposed by Kou et al. (1978) to eliminate redundant cliques.
O	For surveys on graph coloring, see Pardalos et al. (1998) and Malaguti and Toth (2010) and for a specific review on local optimization methods, see Galinier and Hertz (2006).
O	The instances consist of 12 groups: three (Álvarez-Valdés et al., 2013), shapes (Oliveira and Ferreira, 1993), BLAZEWICZ (Błażewicz et al., 1993), RCO (Ribeiro et al., 1999), artif (Dowsland et al., 1998), shirts (Dowsland et al., 1998), dagli (Ratanapan and Dagli, 1997), fu (Fujita et al., 1993), J1 (Álvarez-Valdés et al., 2013), J2 (Álvarez-Valdés et al., 2013), poly (Hopper, 2000) and jakobs (Jakobs, 1996).
P	We ran the algorithm using eight sorting criteria described by Dowsland et al. (2002) and selected the best solution. 
O	We have proposed a clique covering MIP model for the irregular strip packing problem based on the dotted-board model (Toledo et al., 2013).
P	For further work, we suggest adding classical valid inequalities from the stable set polytope (Rebennack et al., 2012) to improve the linear relaxation of the clique covering model.
O	Due to the growing amount of road traffic and the limited capacity of the road network, traffic congestion has become a daily phenomenon, as stated in Kok et al. (2012).
O	In this paper we focus only on predictable causes, which have been proved to be major responsible of the travel time fluctuations. (see Skabardonis et al., 2003), and, therefore, need to be taken into account during the vehicle dispatching planning phase.
P	For a complete survey on the other version of the problem, the reader may refer to Gendreau et al. (2015).
P	The first to analyze the TDVRP was Beasley (1981), who adapted the savings algorithm, taking into account a planning horizon composed by two periods with different travel times.
O	Malandraki and Daskin (1992), addressed a TDVRP, with time windows and service time at nodes, in which travel times are computed by means of simple step functions.
P	They proposed a mixed integer linear programming formulation, a nearest-neighbor based heuristic and a branch-and-cut algorithm, while in Malandraki and Dial (1996), a dynamic programming algorithm is used to solve the timedependent traveling salesman problem (TDTSP), a special case of the TDVRP, in which a single vehicle is involved.
P	Horn (2000), proposed a model with linear piecewise travel speed and quadratic piecewise travel times. 
O	In Ichoua et al. (2003), the authors present a time-dependent travel speed model in which time is discretized into time-slots, that could be as small as necessary to correctly describe speed fluctuation, and travel speed is given by a different linear function for each slot, ensuring continuity on the border of adjacent timeslots.
P	To overcome this limitation, Mancini (2014), modeled travel times as polynomial functions, in order to better represent rush hour peaks and fluctuations.
O	Cordeau et al. (2014), in a study on the TDTSP, have pointed out that the more travel times fluctuation pattern of the arcs are similar among each others, the more the TSP solution obtained considering constant travel times is accurate also for the TDTSP.
O	Capacitated Vehicle Routing Problem, for which several efficient exact and heuristic algorithms, are available in the literature while Dabia et al. (2012), presented an arc-based formulation for the TDVRPTW and developed a Branch-and-Price in which the master problem results to be a Set Partitioning while the pricing problem becomes time-dependent shortest path with resource constraints.
O	In Crainic et al. (2012), the authors addressed different scenarios, in each one of which travel times on arcs are different but it is supposed to be constant along all the delivery phase.
P	In Jung and Haghani (2001), the authors proposed a solution method based on a genetic algorithm.
O	An application to fresh and perishable food delivery has been addressed in Osvald and Stirn (2008).
P	In Hashimoto et al. (2008) the author proposed an Iterated Local Search heuristic for the TDVRP with Time Windows.
P	The same problem has been addressed by Maden et al. (2009) in which the authors proposed a TS, and by Balseiro et al. (2011), where an Ant Colony System, (ACS), hybridized with an insertion heuristic, is presented.
P	Figliozzi (2012), proposed a fast iterative route construction and solution improvement method for the TDVRP with Time Windows and investigate the impact of the overlapping of the congested periods distribution and the customer time windows distribution, on the number of vehicles used.
P	In Harwood (2003), the author proposed a quickly travel time estimation technique which results in a neighborhood exploration.
P	Zhang et al. (2012), addressed the TDVRPTW with simultaneous pickup and delivery, proposing an hybrid ACS and TS heuristic.
O	Kuo et al. (2009), studied an interesting variant of the TDVRP in which the goal is to minimize the total fuel consumption.
O	In Jabali et al. (2012), the objective function considers the minimization of fuel consumption, CO2 emission and travel time.
O	Franceschetti et al. (2013), addressed the CO2 emission issue, but taking into account, as input data, speed reductions imposed by traffic congestion.
O	The values of the τ ijk are given as input, but they are chosen such that the FIFO property apply to the time-dependent travel time tij ∗Tij(), (Fleischmann et al., 2004).
P	Set Partitioning based refining heuristics have been proved to be very effective on Vehicle Routing Problems, (Kelly and Xu, 1999).
P	Multi-start heuristics have been introduced by Lin and Kernighan (1973) in 1973 and since then they have been broadly used in combinatorial optimization, (Marti, 2003).
P	One of the most used multi-start methods is the greedy random adaptive search procedure (GRASP), which was introduced by Feo and M. (1995).
P	In Bassi de Araujo and Armentano (2007) the authors propose a MRHC for the container loading problem, while in Fleurent and Glover (1999) an application of MRHC to the Quadratic Assignment Problem has been presented.
P	Computational results have been carried out on instances derived from those proposed for the VRP with Time Windows by Solomon (1987).
P	Finally, the third group contains the original 100 customers instances introduced by Solomon (1987): C101, C201, R101, RC101.
P	GRASP has been successfully applied to a large variety of combinatorial problems, (Gendreau and Potvin, 2010), both as a stand-alone optimization technique and combined with other heuristic techniques such as Path-Relinking, (Resende and Ribeiro, 2005), and Variable Neighborhood Search,(Festa and M., 2009).
O	Cordeau et al. (1998), Caprara et al. (2007), Lusby et al. (2011) and Harrod (2012) surveyed the optimization models for the train routing, scheduling and timetabling problem.
O	Caprara et. al (2002) concentrated on the problem of a single, one-way track linking two major stations, with a number of intermediate stations in between.
O	Caprara et. al (2006) extended the aforementioned model by considering several additional constraints for real-world applications.
P	Furthermore, Cacchiani et. al (2008) proposed heuristic and exact algorithms of the same problem based on the solution of the LP relaxation of an ILP formulation, in which the decision variables correspond to a full timetable for a train.
P	Zhou and Zhong (2007) proposed a generalized resource-constrained project scheduling formulation for a single-line track network.
O	Bersani et al. (2015) addressed the problem of scheduling trains on a single track from a robust optimization point of view.
P	Dessouky et. al (2006) developed a mathematical programming model to determine the optimal dispatching times for complex rail networks in densely populated metropolitan areas.
O	Mazzarello and Ottaviani (2007) deployed an alternative graph model, which was first introduced by Mascis and Pacciarelli (2002), to the implementation of a Traffic Management System.
O	Liu and Kozan (2009) modeled the train scheduling problem as a blocking parallel-machine job shop scheduling problem. 
P	In Corman et al. (2011), an innovative optimization framework is proposed for the multi-class rescheduling problem when perturbation occurs in the network.
P	The framework provides high quality schedules when combined with an advanced branch and bound algorithm in D’Ariano et al. (2007).
O	Sama et al. (2015) studied a filtering method for the routing selection and scheduling problem, and an ant colony optimization method is proposed in the solution procedure.
O	Sam et al. (201 ) stu ie the real-time train routing selection problem by revisiting the real-time Railway Traffic Management Problem (rtRTMP) and then introduced a new real-time Train Routing Selection Problem (rtTRSP) formulation.
P	Semrov et al. (201 ) introduce a train resche uling metho ase on reinforcement learning theory and solved for the scheduling problem of both single-track and complex railway networks.
P	For example, Brannlund et al. (1998) presented an optimization approach for the timetabling problem in which the single track problem with sidings is modeled as an integer problem.
O	Flier et al. (2009) studied adding an additional train to the timetable on a corridor with minimum possible delay.
O	Cacchiani et al. (2010) studied the freight train scheduling problem with a prescribed passenger train timetable.
P	In Sun et al. (2014), a multiobjective optimization model is proposed to minimize the deviation for train rerouting on a highspeed railway network.
P	Shafia et al. (2012) proposed a robust timetabling model for a single track railway line to compute buffer times.
O	In Veelenturf et al. (2016), a macroscopic level timetable rescheduling problem is studied for passenger trains.
O	Several other related studies can be found in Arenas et al. (2015), Karoonsoontawong et al. (2015), Goverde et al. (2016) an Bešinović et al. (2016).
O	In Cacchiani et al. (2016), a train timetabling problem is studied to determine the best schedule for a given set of trains under the constraints of track operational constraints.
P	For example, Corman et al. (2011) developed a microscopic level model of railway traffic to reschedule the prioritized trains when conflicts occur
P	We formulate the model as a mixed integer programming problem, which extends the model structure from Dessouky et al. (2006).
O	Another approach is to deploy the departure/arrival time decisions from our solution mo el, an use a greedy based algorithm to construct the routes an assign the priorities as the trains travel along the rail network, similar to the approach of Lu an Dessouky (2004).
O	This type of facility location problems are known as the site-generating problems (Love et al., 1988).
O	This coverage distance limitation of the facilities can be associated with the constraints on the voltage drop in the energy systems (due to the resistance on cables) as in Kocaman et al. (2012) or the pressure loss in the water systems (due to the friction in the pipes) as in Douglas et al. (1979) that are both linearly increasing with distance.
O	This problem is known to be an NP-hard problem (Garey and Johnson, 1979). 
O	Several exact (Balas and Carrera, 1996; Beasley, 1987; Beasley and K.Jörnsten, 1992; Fisher and Kedia, 1990) and heuristic (Beasley, 1990; Beasley and Chu, 1996; Caprara et al., 1999; Haddadi, 1997; Lorena and Lopes, 1994) methods are proposed to solve the SCP that have applications in fields such as crew scheduling (e.g., Caprara et al., 1999) and locating emergency facilities (e.g., Rajagopalan et al., 2008; Toregas et al., 1971). 
O	The algorithms for the SCP are compared in the survey paper (Caprara et al., 2000) by Caprara et al.
O	After the turn of the century, the work on the SCP concentrate on heuristic algorithms based on greedy randomized search (Bautista and Pereira, 2007; Haouari and Chaouachi, 2002; Lan et al., 2007), local search (Yagiura et al., 2006), genetic algorithm (Solar et al., 2002) and ant colony optimization (Ren et al., 2010).
P	To solve the PSCP exactly, Church (1984) defined the circle intersection points set (CIPS) as the locations of all demand points and the intersection points of all circles centered at the demand points with a radius of a predetermined coverage distance. 
O	It is possible to show that there exists at least one optimal solution to the PSCP in which all facilities are located in the CIPS (Eiselt and Sandblom, 2013).
P	The MWP is known to be an NP-hard problem (Megiddo and Supowit, 1984); therefore, several heuristic solution methods are proposed in the literature.
P	Cooper’s iterative locationallocation algorithm (Cooper, 1963; 1964) is a well-known algorithm developed for this problem.
O	In the allocation step, for fixed locations of the facilities, algorithm simply assigns each demand point to its nearest facility (breaking ties arbitrarily), and once the allocations are fixed, in the location step, the problem reduces to p independent single facility location problems that can be solved by the modified Weiszfeld’s method in Vardi and Zhang (2001).
O	As the final solution depends on the initial solution, a random multi-start version of this algorithm can be applied as in Drezner et al. (2016).
P	Based on the observation that the optimal solution of the continuous problem often has several facilities co-located with the demand points, in Hansen et al. (1998) proposed the p-median heuristic.
P	Recently, Brimberg and Drezner (2013) proposed to overlay the area containing the demand points with a grid. 
P	Brimberg et al. (2014) proposed an alternating solution procedure where a local search is conducted in the continuous space to obtain a local optimum.
P	Finally, Drezner et al. (2015) developed a distribution-based variable neighborhood search and a genetic algorithm, and a hybrid algorithm that combines these two approaches.
P	For other heuristic, metaheuristic and exact approaches for the MWP, readers can refer to a comprehensive review by Brimberg et al. (2008).
O	Krarup and Pruzan (1983) provided a highly cited survey on this problem.
P	Berman and Yang (1991) introduced the problem and proposed an iterative algorithm starting from the solution of the uncapacitated facility location problem. 
P	Krysta and Solis-Oba. (2001) and Weng (2013) presented integer programming (IP) formulations for the unweighted problem and proposed approximation algorithms.
P	Brimberg et al. (2004) introduced the fixed cost for facilities that is independent of the location.
O	The problem that we consider in this paper reduces to the problem considered in Brimberg et al. (2004) if the coverage distance limitation is removed.
O	Following the path in Hansen et al. (1998) of solving the discrete version to obtain an initial solution for the continuous problem, in the first stage of this heuristic, the SPLP is solved assuming that the demand points are the potential locations for facilities.
P	Brimberg and Salhi (2005) introduced zone-dependent fixed costs for the facilities, where they defined zones as polygons.
O	Drezner et al. (1991) introduced a Weber problem with limited distances.
O	In the distance-limited continuous location-allocation problem that we present, as opposed to the constant cost after the distance limit in Drezner et al. (1991), we assume an infinite cost after the distance limit, so our problem is quite different than other distance-limited problems considered in the literature (e.g. in Drezner et al., 2016; 1991; Fernandes et al., 2014).
O	A technological boom has taken place in the development of Unmanned Aerial Vehicles (UAV), also known as Drones, or Unmanned Aerial Systems (UAS) (van Blyenburgh, 1999; Gupta et al., 2013; Sundar and Rathinam, 2014).
O	This development dates back the last decade facing applications in the military sector (Enright et al., 2005; Gertler, 2012; Ground et al., 2002), mainly because the large amount of investment that used to be required for UAVs construction and control.
O	Sensors like accelerometers, gyroscopes, digital magnetometers, barometers and even antennas of Global Positioning System (GPS) (Nex and Remondino, 2014) were, initially, popularized by the smartphone industry, providing a new way of measuring the world we live in.
O	Drones emerging in the society is also related to the development of novel materials (Newcome, 2004; Spinka et al., 2011), lighter and more resistant, such as: advanced aluminum alloys, carbon fiber reinforced polymer and other composites.
O	This aforementioned eminent boom came on the scene mainly by hobbyists and the entertainment and photographic industries (providing the acquisition of aerial images) (Colomina and Molina, 2014).
O	These applications are becoming increasingly complex, involving other factors beyond simple images acquisition (Frew and Brown, 2009; Pohl and Lamont, 2008; Turner et al., 2016).
O	In this sense, the ability of UAV in collecting and delivering materials should be highlighted (Alighanbari et al., 2003; Enright et al., 2015; MEMP, 2013; Sundar and Rathinam, 2014; Weinstein and Schumacher, 2007), as well as measurement of air pollution in industrial areas, building painting, skyscrapers windows cleaning, city lights control and maintenance, among countless other applications that may arise from human creativity.
O	Patents are being registered (Berg et al., 2016), some of them mention the use of a package delivery apparatus that uses an UAV (Lisso, 2017).
O	The idea of the illustration was to show a Smart Cities (SC) (Mohammed et al., 2014) hub for commercial and personal deliveries that might be done by UAV.
O	Enright et al. (2015) surveyed algorithms on task assignment/allocation and UAVs scheduling, discussing dynamic time-dependent scenarios where targets arrive at random locations at random times.
O	This problem can fit the new class of Green Routing Problems (Erdogan and Miller-Hooks, 2012), which is being investigated for alternative fuel-powered vehicle fleets (Lin et al., 2014).
P	Sundar and Rathinam (2014) introduced a Mixed-Integer Linear Programing (MILP) for a so-called Fuel Constrained UAV Routing Problem (FCURP), considering a single UAV that should visit all targets at least once.
P	On the other hand, SC are now being able to take advantage of the new mesh of microgrid systems (Fathima and Palanisamy, 2015), which are composing the new electric grid paradigm, known as Smart Grids (SG).
O	This new bidirectional system, mainly composed of intermittent Renewable Energy Resources (RER) (Basak et al., 2012), can share energy with a hybrid environment composed of UAVs (Coelho et al., 2016b), which will be able to directly charge from collecting and delivering clients, as well as other independent charging stations.
O	The growth of the SC (Karnouskos et al., 2012) is surely aligned with energy development.
O	As mentioned, when UAVs are requested to visit several clients, vehicles might need to come to depot for refueling (Sundar and Rathinam, 2014).
O	In particular, Eqs. (26), (27) and (30), are inspired by the work of Coelho et al. (2016a), which focused on updating battery rates of Plug-in Electric Vehicles.
P	In order to obtain a set of non-dominated solutions for the proposed routing problem, in restricted computational time, the use of the Multi-Objective Smart Pool Search (MOSPOOLS) Matheuristic, introduced by Coelho et al. (2016a) and extended in Coelho et al. (2016b), is proposed.
P	Furthermore, we consider the version introduced by Coelho et al. (2016d), which brings a smart strategy for using MIP start solutions (line 4) in the beginning of the search done by the Black-Box solver.
O	In our case, each solution found in the branches of a Branch-and-Bound (BB) tree is considered by the addSolution procedure (Line 8), extracted from Lust and Teghem (2010) (described in Algorithm 2).
O	The core of the MOSPOOLS algorithm was implemented in C++ in the framework OptFrame 2.21 (Coelho et al., 2011), which has already been used for tackling other N P-Hard combinatorial optimization problems (Coelho et al., 2016c; 2016e; Souza et al., 2010).
O	The quality of the obtained Pareto Fronts was analyzed according to four different indicators: 1 – convergence, analyzed by the Hypervolume (HV) (Zitzler and Thiele, 1998) quality indicator (computed with the tool provided by Beume et al. (2009)); 2 – diversity, measured with the  metric (Deb et al., 2002). 3 – coverage (Zitzler and Thiele, 1998); and 4 – cardinality (Zitzler and Thiele, 1998).
O	For the parallel coordinates and the polar graphs, the order of the objective function in both graphs was chosen with assistance of Aggregation Trees (AT) (de Freitas et al., 2015), defining the order based on the concept of harmony and conflict between objectives
N	Because the PRP is a very complex problem, there are only a few exact algorithms (see Adulyasak et al., 2014b; Archetti et al., 2011; Ruokokoski et al., 2010) available in the literature and these algorithms can only solve small-scale instances to optimality. 
O	Therefore, many heuristics, e.g. an adaptive large neighborhood search (Adulyasak et al., 2014a), branch-and-price heuristics (Bard and Nananukul, 2009a, 2010), decomposition heuristics (Bertazzi et al., 2005; Boudia et al., 2008; Chandra, 1993; Chandra and Fisher, 1994; Lei et al., 2006), a GRASP (Boudia et al., 2007), Lagrangian heuristics (Fumero and Vercellis, 1999; Solyalı and S¨ural, 2009), matheuristics (Absi et al., 2015; Archetti et al., 2011), a memetic algorithm (Boudia and Prins, 2009), and tabu search algorithms (Armentano et al., 2011; Bard and Nananukul, 2009b) have been proposed for several variants of the PRP. 
O	The majority of these heuristics have been tested on the benchmark instances generated by Boudia et al. (2007) and Archetti et al. (2011) with the most successful ones being those proposed by Absi et al. (2015), Adulyasak et al. (2014a), and Archetti et al. (2011). 
P	For a recent review on the PRP, interested readers can refer to Adulyasak et al. (2015). 
N	The PRP is also very related to the inventory routing problem (IRP) in that the PRP reduces to the IRP when there are no production decisions at the plant/supplier level (i.e., production amounts are assumed to be known or unlimited in IRP).
O	See Coelho et al. (2014) for a recent review on the IRP.
P	The first two phases of the proposed heuristic is based on the a priori tour idea, which first appeared in Pınar and S¨ural (2006) and then in Solyalı and S¨ural (2011) for solving the single-vehicle inventory routing problems.
P	In the fourth phase, we try to obtain a feasible and improved solution exploiting an idea proposed in Archetti et al. (2012) and used in Coelho and Laporte (2013) for IRPs. 
O	Existing heuristics that involve solving a MIP like those of Absi et al. (2015), Adulyasak et al. (2014a), and Bard and Nananukul (2009b) consider approximating the distribution cost in the MIP by defining a visiting cost for retailers and associated variables.
P	The proposed heuristic has been tested on the benchmark instances generated by Boudia et al. (2007) and Archetti et al. (2011) and compared with the most successful heuristics mentioned above.
O	In particular, the heuristic managed to find new best solutions for 60 of 90 instances generated by Boudia et al. (2007) and 623 of 960 instances generated by Archetti et al. (2011).
O	Constraints (11) are the generalized fractional subtour elimination constraints (see Adulyasak et al., 2014b), which do not only prevent subtours but also ensure that individual vehicle capacities are not exceeded. 
P	We have performed our computational experiments on the benchmark instances introduced by Archetti et al. (2011) and Boudia et al. (2007) to assess the computational performance of the multi-phase heuristic (abbreviated as 5P).
O	We have solved the mathematical formulations in the second and fourth phases (i.e., F 0 and IF) using Cplex 12.5 with its default settings and a single thread, the CVRPs in the third phase using the heuristic of Groer et al. (2010), and the TSPs in the first, third and fifth phases using Concorde (Applegate et al., 2007).
O	The test instances as generated by Archetti et al. (2011) involve six periods and 14 (A1), 50 (A2), and 100 (A3) retailers in three sets, each of which has 480 test instances.
O	Being larger scale instances than those of Archetti et al. (2011), the instances generated by Boudia et al. (2007) involve 20 periods and respectively 50, 100, and 200 retailers in three sets of instances (B1, B2, and B3) with each set having 30 test instances.
P	We have compared our results with the results of the heuristics (IM-VRP and IM-MultiTSP) proposed by Absi et al. (2015) and the adaptive large neighborhood search heuristic (ALNS) proposed by Adulyasak et al. (2014a) on these benchmark instances.
N	The local search heuristic (H) proposed by Archetti et al. (2011) have not been included into the comparison, because H requires a correction in its implementation (Archetti, 2015).
P	The optimal solutions for the instances in set A1 (i.e., single vehicle instances) have been found using the branch-and-cut algorithm in Ruokokoski et al. (2010).
O	Note that IM-VRP and IM-MultiTSP are equivalent in the case of a single vehicle and their best performing variant is called the iterative heuristic with a multi-start scheme (IM-MS) in Absi et al. (2015).
O	The summary of the computational results on the instances introduced by Boudia et al. (2007) are presented in Table 9, where the first column indicates the name of the heuristic, the columns named as ‘Ave. Cost’ give the average of the objective function values obtained over 30 instances, and the columns named as ‘# Best’ give the number of the times a heuristic finds the smallest objective function value.
P	In this section, we provide the objective function values found by our heuristic on the instances introduced by Archetti et al. (2011) and Boudia et al. (2007).
O	In this paper, a general formulation of the DFLP is presented based on the zone-based FLP, which was first defined by Montreuil et al. (2002, 2004).
P	For example, the Flexible Bay Structure (FBS), which is a version of the zone-based FLP, is frequently used in the literature because it generates practical block layouts although it leads to higher costs (Meller, 1997; Chae and Peters, 2006; Konak et al., 2006).
O	The zone-based layout model can support different space partitioning of a facility in order to better represent specific real-life industrial cases (Montreuil et al., 2004).
O	For example, a bay configuration where departments are arranged along a central aisle is frequently used for the layout of semiconductor manufacturing plants (Peter and Yang, 1997).
O	In flexible manufacturing systems, manufacturing cells are also located around a central material handling system that interconnects all cells (Kusiak and Heragu, 1987). 
O	Only a few DFLP methods (Kulturel-Konak and Konak 2015; Lacksonen, 1997) can concurrently determine the relative locations and dimensions of the departments over multiple planning periods using an unrestricted FLP model where departments can be located anywhere within the facility
O	Koopmans and Beckmann (1957) modeled the FLP as a quadratic assignment problem (QAP) that aims to find the optimal assignment of n departments to n fixed and discrete locations to minimize the total material handling cost expressed as the product of material flow and travel distance.
O	The QAP is NP-Complete (Sahni and Gonzalez, 1976). 
O	There are earlier optimal procedures for solving the QAP with a small number of machines, such as Lawer (1963) and Burkar & Elshafer (1979).
O	Montreuil et al. (1993) strengthened the design skeleton-based approach by developing a linear programming model which efficiently generates a layout from a design skeleton.
O	The literature includes many papers in this direction, including Lee and Lee (2002), Dunker et al. (2005), Baykasoglu et al. (2006), Drira et al.(2007), Balakrishnan and Cheng (2009), McKendall and Hakobyan (2010), and McKendall and Liu (2012).
N	This version of the unequal area FLP is also categorized as the FLP on the continuous plane (Montreuil, 1990; Kulturel-Konak and Konak, 2013) and harder to solve due to the linearization of the non-linear constraint x y i i i a l l   for each department i.
O	Relevent research on this venue of the unequal area FLP in literature includes Montreuil (1990), Bozer and Meller (1997), Meller et al. (1999), Sherali et al. (2003), Konak et al. (2006), Ulutas and Kulturel-Konak (2012), Kulturel-Konak and Konak (2013, 2015), and Gonçalves and Resende (2015).
P	Meller et al. (1999) proposed a more accurate surrogate area constraint which reduced the maximum error to 14.3% for the aspect ratio of five.
P	Sherali et al. (2003) developed a polyhedral outer-approximation method based a set of tangential lines for supporting the non-linear function, y x i i i l a l  . 
P	Balakrishnan et al. (2003) developed a hybrid genetic algorithm for or the DFLP. 
O	Baykasoglu et al. (2006) studied the DFLP with budget limitations on the reconfiguration of facilities.
O	McKendall and Liu (2012) and Arostegui et al. (2006) summarize and compare several meta-heuristics for the DFLP.
O	The facility layout problem in multi-bay environments, another extension of the FLP, referred to as FLP with flexible bay structure (FLP-FBS), is concerned with determining the most efficient assignment of departments to parallel and flexible bays (Meller, 1997; Chae and Peters, 2006; Konak et al., 2006).
P	Konak et al. (2006) first formulated a linear MIP model for the unequal area FLP-FBS that can be optimally solved by MIP solvers up to twelve departments.
O	There are also several other variants of the FLP in literature for some other special cases of industries, such as the single row facility layout problem which arranges the departments on a straight line (Solimanpur et al., 2005; Hungerländer and Rendl, 2013), the grid-based facility layout problem in which the facility is divided into basic squares or rectangles having a unit area (Armour and Buffa, 1963), the multi-floor FLP which fulfills the layout optimization in a facility that has been separated into multiple sub-floors by inner structural walls, passages, or fixed split lines (Meller and Bozer, 1997; Lee et al., 2005), the multi-objective FLP which considers multiple objective functions in the facility layout design, such as the material flow, the number of material handling devices, and the average work-in-progress (Saraswat et al., 2015).
O	By restricting relative positions of zones along an axis direction (horizontal or vertical) such that all zones are arranged in multiple parallel arrays, the DFLP-FZ model reduces to the unequal-area FLP with FBS (Meller, 1997; Chae and Peters, 2006; Konak et al., 2006; Kulturel-Konak, 2012).
O	By defining some zones as fixed bars or lines, the DFLP-FZ model applies to the case of the multi-floor FLP that consider inner structure walls and un-occupiable passages in the facility (Meller and Bozer, 1997; Lee et al., 2005).
O	Meller et al. (1999) pointed out that even if the bounded perimeter constraint is satisfied, the actual area can be much smaller than ai for a large value of i (e.g., the error can be as high as 44% for i =5). 
P	Meller et al. (1999) proposed a more accurate surrogate area constraint, which is max 2( ) 3 , x y i i i i l l a f l i      , where max max{ , } x y i i i l l l  .
O	However, this piecewise linearization increases the problem complexity due to additional two binary variables for each department (Meller et al., 1999, Konak et al., 2006). 
P	Sherali et al. (2003) proposed a polyhedral outer-approximation that uses a number of tangential lines to support the non-linear function / y x i i i l a l  . 
N	However, the number of the support lines required to reduce the area approximation error to a target level is not defined in Sherali et al. (2003).
N	Like Sherali et al.’s (2003) polyhedral outer-approximation, our method does not require binary variables. 
O	Unlike Sherali et al.’s (2003) method which uses uniformly spaced tangential support lines, the end-points of secant lines are calculated to achieve a target maximum area approximation error in our method.
O	First, we carried out computational experiments on problem F10 to test the performance of the polyhedral inner-approximation method proposed in this paper and compared it to the outer-approximation method of Sherali et al. (2003) and Castillo and Westerlund (2005). 
O	In Table 9, column Fs indicates the number of feasible solutions found in 10 runs and column Dev.% indicates the percent deviation from the previous best-known solutions which can be found in Kulturel-Konak and Konak (2013 and 2015) and Gonçalves and Resende (2015).
O	The problem of finding active subnetworks has recently received considerable attention from the bionformatics community (see, e.g., Andreotti, 2015; Backes et al., 2011; Dittrich et al., 2008; El-Kebir, 2015; Hatem, 2014; Huang, 2011; Ideker et al., 2002; Yamamoto et al., 2009 and the references therein).
P	In Dittrich et al. (2008), this problem was formalized as the maximum weight connected subgraph problem (MWCS).
O	Aside from its importance in bioinformatics, the MWCS appears as a basic optimization problem in wildlife corridor design (Dilkina and Gomes, 2010), forestry planning (Carvajal et al., 2013), object and activity saliency detection (Adluru et al., 2014; Chen and Grauman, 2012; Vijayanarasimhan and Grauman, 2011), wireless network deployment planning (Kuo et al., 2015), among others.
P	Dittrich et al. (2008) showed that the MWCS can be transformed into the prize-collecting Steiner tree problem (PCSTP) and developed an exact integer linear programming (ILP)-based solution approach built on the PCSTP framework of Ljubic´ et al. (2006).
O	After Dittrich et al. (2008), further exact solution approaches based on ILPs have been proposed by Althaus and Blumenstock (2014), Álvarez-Miranda, M. Sinnl / Computers and Operations Research 87 (2017) 63–82 Álvarez-Miranda et al. (2013a), Álvarez-Miranda et al. (2013b), Backes et al. (2011), El-Kebir and Klau (2014) and Fischetti et al. (2016).
O	In contrast to Dittrich et al. (2008), where arc and node-variables are used, these latter approaches are based on formulations using only node-variables.
O	Complementary to these algorithms, polyhedral studies on the connected subgraph polytope are carried out in Wang et al. (2017).
O	In Backes et al. (2011), the cardinality-constrained counterpart of the MWCS was tackled via ILP considering an arc-based mode In a latter work, the same variant was approached in Álvarez-Miranda et al. (2013a) using a much more efficient node-based model.
P	Similarly, an arc-based ILP model was proposed by Dilkina and Gomes (2010) for the budget-constrained variant.
O	In a more recent work, considerably better computational results were obtained by means of a node-based model by Álvarez-Miranda et al. (2013b).
O	As a matter of fact, for this reason the R-package BioNet (see Beisser et al., 2010 in addition to Dittrich et al., 2008), also contains a heuristic for the MWCS, for users without access to an ILP solver.
P	Another example of a heuristic for an equivalent problem arising in Bioinformatics corresponds to the hybrid ILP-based heuristic proposed in Akhmedov et al. (2016); the approach is based on embedding the resolution of small size PCSTP instances within a clustering strategy in a divide-and-conquer scheme.
P	In contrast to the heuristics in Akhmedov et al. (2016) and Beisser et al. (2010), the approach proposed in this paper also provides a dual bound which allows to judge the quality of the attained (primal) solutions.
O	Sophisticated Lagrangian relaxation schemes (without cut generation) are designed by Haouari et al. (2008); 2010) for the PCSTP.
O	Complementary, R&C implementations are devised by Lucena (2005); 2006) for the Steiner tree problem, and by Cunha et al. (2009) for the PCSTP
O	A dual ascent algorithm for the PCSTP is designed in the current working paper (Leitner et al., 2016).
O	For the MWCS, the performance of the R&C is compared with that of the state-of-the-art B&C algorithm proposed in Fischetti et al. (2016).
O	Such a constraint may appear, e.g., in Bioinformatic settings where compact, i.e., cardinality-constrained, functional modules are preferred over large ones (see, e.g.,Yamamoto et al., 2009; Yosef et al., 2011); in this case c = 1
O	Likewise, in a wildlife corridor design setting although the aim is to find a connected reserve that maximizes the ecological suitability, it must respect an economical bound (Dilkina and Gomes, 2010).
P	Similarly, in the design of wireless networks, although the objective is to construct a mesh that maximizes the service coverage, there are construction budgets that must be satisfied (Kuo et al., 2015).
O	In this paper a node-based model, as proposed in ÁlvarezMiranda et al. (2013a), El-Kebir and Klau (2014) and Fischetti et al. (2016), is considered.
P	Compared to the formulation proposed by Álvarez-Miranda et al. (2013a) for the MWCS, the model based on (CONN.1) does not make use of an artificial root node for modeling connectivity.
O	Finally, this section is complemented with the following well-known observation (see, e.g., Fischetti et al., 2016).
O	This approach is known as Relax-and-Cut (Lucena, 2005), which is outlined in the next section.
O	Such hybrid scheme has been used, for instance, by Calheiros et al. (2003) for solving a partitioning problem.
P	The average direction strategy (ADS), which is proposed in Sherali and Ulular (1990).
O	The growing awareness for sustainability results in need to have decision support tools that can be used for making environmentally friendly decisions in logistics management (Soysal et al., 2012).
P	Studies on VRP considering transportation emission (e.g., Ubeda et al. (2011); Demir et al. (2012); Erdogan and MillerHooks (2012); Soysal et al. (2015)) have been correspondingly increasing as emission is one of the most prominent environmental issue.
O	The related literature indicates that contributions are necessary to provide green models that consider emissions for the CVRP with non-constant travel times (Bekta¸s and Laporte, 2011; Jabali et al., 2012).
O	The VRP with its variants have been extensively studied in the literature (Laporte, 1992; Eksioglu et al., 2009; Lin et al., 2014), although, relatively few attempts have incorporated fuel consumption and/or emission concerns into the routing decisions.
N	However, several studies (e.g., Bekta¸s and Laporte (2011); Kara et al. (2007)) reveal that having a set of vehicle schedules that minimizes total traveled distance or time does not always mean that the lowest total emission level is obtained.
O	These problems are pioneered by Malandraki and Daskin (1992) and Hill and Benton (1992).
P	Afterwards, several other studies respect the important issue of time dependent travel speeds especially for intra-city (urban) travel (e.g., Horn (2000); Ichoua et al. (2003); Fleischmann et al. (2004); Kok et al. (2012); Mancini (2014)).
P	The interested reader is referred to the recent review of Gendreau et al. (2015) on the topic.
O	Travel speed is not only important in basic VRPs but also regarded as among the main factors affecting fuel consumption and emissions (Demir et al., 2011; Goodyear-website; Ligterink et al., 2012).
O	Some of the studies that employ vehicle speed (e.g., Figliozzi (2011); Jabali et al. (2012); Franceschetti et al. (2013)) consider also non-constant travel times between nodes while calculating energy use
O	Due to the fact that VRP is an NP-hard problem and computational effort required for its solution increases exponentially with the problem size, various heuristic techniques (e.g., Hemmelmayr et al. (2012); Xiao et al. (2012)) are suggested in literature.
O	These heuristics can be categorized as constructive heuristics (e.g., Clarke and Wright (1964)) and improve feasible heuristics (e.g., Lin (1965)).
P	Gromicho et al. (2008) present the application of the classical RDP heuristic to different variants of VRPs.
O	They approach to the VRP as a TSP by means of a giant-tour representation, which was introduced by Funke et al. (2005).
O	The described approach, conversion to TSP and applying the classical RDP heuristic, has been also used in other VRP studies (Gromicho et al., 2012; Kok et al., 2010, 2012) by restricting the state space in the same way.
O	In this study, we also approach to the VRP as a TSP by means of a multiple TSP representation of the VRP introduced by Christofides and Eilon (1969).
O	We employ the COPERT2 methodology for the calculation of fuel consumption (Mellios et al., 2011).
P	For further details on these parameters, the reader is referred to Mellios et al. (2011).
P	We employ a DP algorithm methodology based on the DP formulation introduced by Bellman (1962) and Held and Karp (1962) for the TSP.
P	This approach has been also applied in other studies (see Gromicho et al. (2008, 2012); Kok et al. (2012)).
P	The FIFO property guarantees that if a vehicle leaves a node i for a node j at a given time, any identical vehicle leaving node i later will arrive node j later as well (Ichoua et al., 2003).
O	The parameters used for calculating the total fuel consumption cost are taken from Mellios et al. (2011) and are as follows: To be used in equation (1), a = 173.7871, b = 0.0685, c = 0.364001, d = −0.00025 and e = 0.00874. We use 2.63 kg/l as a fuel conversion factor to estimate CO2 emissions from transportation operations (Defra, 2007).
O	This subsection first presents the performance of the Simulation Based RDP on the base case, compared to the optimal policy produced by the DP algorithm and the policy produced by the classical RDP algorithm introduced by Gromicho et al. (2008).
P	In response, terminal operators, shipping lines and port authorities are investing in new technologies and smart decision rules to improve container handling, throughput, and operational efficiency (see Gharehgozli et al. [2015a]).
O	New automated container terminals, with stacks perpendicular to quay, use two or three yard cranes (YCs) per stack lane to retrieve and stack containers (Vis and Carlo [2010]; Li et al. [2012]).
O	A third and final configuration is a hybrid setting by having two non-passing cranes and one larger crane which is able to move over the other two (Gharehgozli et al. [2017]).
O	Likewise, Carlo et al. [2012] discuss twelve priority rules for twin automated stacking cranes that collaborate to transport containers.
P	Gharehgozli et al. [2015b] improve the stacking operational performance by better scheduling of the twin cranes in a container block.
O	Wu et al. [2015] also investigate the problem of scheduling multiple yard cranes within one yard block while considering practical operational constraints such as job target times, proper prioritization of yard retrieval jobs, crane availability, and spatial constraints.
P	For a thorough discussion on the applicability of profile minimization techniques to the solution of a system of linear equations, the reader is referred, for example, to the book of Tewarson (1973) and to the recent survey article by Davis et al. (2016).
O	These include an approach by Xu et al. (2013) for sparse matrix-vector multiplication optimizations using graphics processing units.
O	Higham (2003) considered the PMP in the context of small world networks.
O	Berry M. et al. (1996) dealt with the PMP in the information retrieval setting.
O	Mueller et al. (2007) reported on an application of profile reduction techniques in the graph visualization domain.
P	Meijer and de Pol (2015) suggested application of profile minimization algorithms in symbolic model checking.
O	Bolanos M. et al. (2012) considered the PMP in relation to a new approach for computing entropy rate for undirected graphs.
O	Lin and Yuan (1994) have shown the PMP to be NP-hard.
O	Therefore, polynomial-time exact algorithms for this problem have been proposed only for restricted classes of graphs, e.g., wheels, complete bipartite graphs (Lin and Yuan, 1994), trees (Kuo and Chang, 1994), and triangulated triangles (Guan and Williams, 2003).
P	One of the first constructive algorithms for the PMP was proposed by Cuthill and McKee (1969).
O	George (1971) observed that reversing the obtained permutation yielded better solutions.
P	Gibbs et al. (1976) presented another constructive technique for reducing the profile of a sparse matrix.
P	At about the same time, Gibbs (1976) proposed a variation of this heuristic, which is currently known as the Gibbs-King algorithm.
P	Sloan (1986) developed an algorithm consisting of two distinct stages.
P	The algorithm was shown to be superior to previous methods (Sloan, 1986).
P	Several enhancements to the Sloan algorithm have also been proposed (Duff et al., 1989; Kumfert and Pothen, 1997; Reid and Scott, 1999). Barnard et al. (1995) and Paulino et al. (1994a); 1994b) developed algorithms for the PMP which are based on spectral properties of the adjacency matrix of the graph.
P	Hu and Scott (2001) presented a multilevel algorithm for profile reduction.
O	The earliest such algorithm appeared in 1985 by Armstrong (1985).
P	Another SA algorithm for the PMP has been proposed by Lewis (1994).
P	Hager (2002) developed two exchange methods for improving a given solution to the PMP.
O	Reid and Scott (2002) presented a significantly improved implementation of the Hager’s exchange methods.
P	Kaveh and Sharafi (2012) proposed an algorithm for the PMP, which is based on the metaheuristic optimization method known as charged system search.
P	Koohestani and Poli (2014) presented a hyper-heuristic approach based on genetic programming for evolving an enhanced version of the Sloan algorithm.
P	More recently, Koohestani and Poli (2015) developed a genetic programming system for profile reduction of sparse matrices.
P	Sánchez-Oro et al. (2015) proposed a scatter search algorithm for solving the PMP.
O	However, considering direct matrix methods, Lewis (1994) as well as Reid and Scott (2002) have noticed that local search and metaheuristic approaches are useful in situations where either the same matrix A is used repeatedly or there are a large number of matrices having the same sparsity pattern (and represented by the same graph). 
O	Koohestani and Poli (2015) have remarked that an algorithm for the PMP could be used independently of direct methods like Gaussian elimination.
O	There are some other applications of the PMP where the computation time is not a major factor, and the development of metaheuristic algorithms for such applications is of significant practical as well as theoretical interest (Meijer and de Pol, 2015; Mueller et al., 2007)
P	The reader is referred to MafteiuScai LO (2014),Bernardes J. .A and de Oliveira SL (2015), and Davis et al. (2016).
O	Recent techniques include variable neighborhood search (Mladenovic et al., 2010), tabu search (Campos et al., 2011), scatter search (Campos et al., 2011), learning based evolutionary approach (Isazadeh et al., 2012), genetic algorithm (Pop et al., 2014), and simulated annealing (Torres-Jimenez et al., 2015).
P	Our choice of VNS and SA was inspired by the great success of these techniques for some other optimization problems on ermutations, e.g., the BMP on graphs (Mladenovic et al., 2010; Torres-Jimenez et al., 2015) and the single row facility layout problem (Palubeckis, 2015; 2017).
P	Variable neighborhood search (Hansen and Mladenovic, ´ 2001; Hansen et al., 2010) is a metaheuristic developed for the solution of optimization problems.
O	The basic element of the search mechanism in this method is the acceptance probability (Cerný, ˇ 1985; Kirkpatrick et al., 1983) exp(−(F (p ) − F (p))/T ), where p is the current permutation, p is a permutation in a neighborhood of p, and T is the temperature value. The difference δ(p, p ) = F (p ) − F (p) is called a move gain.
O	In the literature (see, for example, Rutenbar, 1989), this number often is chosen to be 100n.
O	Like in Hager (2002); Reid and Scott (2002); Sánchez-Oro et al. (2015), our LS implementation is based on the exploration of the insertion neighborhood.
O	The set we selected includes matrices of this collection used by Sánchez-Oro et al. (2015) and, in addition, 5 larger matrices used by Koohestani and Poli (2014); 2015).
O	As a testbed for experimental evaluation of algorithm performance, we considered a set of instances in the Harwell-Boeing Sparse Matrix Collection (Matrix Market, 2011)
O	In order to find good parameter settings for the VNS component, we examined the MSA-VNS algorithm’s performance on a training sample consisting of 32 symmetric matrices taken from the University of Florida Sparse Matrix Collection (Davis and Hu, 2011).
O	Most of these values are taken from Table 9 in SánchezOro et al. (2015).
P	For the best results for 11 problems in the selected dataset, we refer to Koohestani and Poli (2014) and Lewis (1994).
O	For indicative purpose, we compare the results produced by our MSA-VNS approach with those obtained by the following state-ofthe-art algorithms in the literature: S∗+, the hybrid of local search and the best algorithm created by the genetic hyper-heuristic from Koohestani and Poli (2014); GP, the genetic programming system from Koohestani and Poli (2015); SS, the scatter search approach from Sánchez-Oro et al. (2015).
O	For example, for the DWT245 instance, MSAVNS took 76s (Table 3), GP took roughly 69s on an AMD Athlon (tm) dual-core processor 2.20 GHz (Koohestani and Poli, 2015), SS took 29.7s on an Intel Core i7 2600 machine running at 3.4 GHz (Sánchez-Oro et al., 2015), and S∗+ was allowed to run for 1000s (like for all other tested problem instances) on the same computer as the one used for GP (Koohestani and Poli, 2014).
P	We remark, however, that it is rather difficult to compare experimental results of MSA-VNS against those obtained by the algorithms proposed in Koohestani and Poli (2014, 2015); Sánchez-Oro et al. (2015), mostly because the runs were performed on different computers.
P	In such cases, a suitable strategy is to apply fast heuristic techniques for the PMP, for example, an enhanced version of the Sloan algorithm, which was produced using a genetic hyper-heuristic approach (see Koohestani and Poli, 2014).
O	Furthermore, there are applications (for example, Meijer and de Pol, 2015; Mueller et al., 2007) requiring the solution of the PMP where no hard limit on the run time of the algorithms is imposed.
O	In closing this section, we should mention that the C++ code implementing the MSA-VNS algorithm for the PMP is made publicly available as a benchmark for future comparisons, see Palubeckis (2016).
O	By definition, for a given undirected graph, any complete subgraph defines a clique (Pardalos and Xue 1994, Vogiatzis et al. 2015, Wu and Hao 2015).
O	It was Luce and Perry (1949) who introduced, for the first time, the concept of clique in order to study a cohesive subgroup inside a social network (Luce and Perry 1949, Sethuraman and Butenko 2015, Vogiatzis et al. 2015). 
O	The objective of a clique problem essentially consists in finding a complete subgraph of a given graph such that some particular properties are satisfied (see e.g., Pardalos and Xue (1994), Vogiatzis et al. (2015), Wu and Hao (2015), and references therein).
O	Social networks, molecular structures, and pattern recognition are some examples of the practical domains in which variants of cliques are used (see Abello et al. (1999), Boginski et al. (2006), Bomze et al. (1999), Gardiner et al. (1997), Massaro et al. (2002), Sethuraman and Butenko (2015), Vogiatzis et al. (2015), and references therein).
O	Another example can be a social network, where we may be interested in cohesive subgroups that are characterized by their level of income (Boginski et al. 2006, Sethuraman and Butenko 2015).
P	In particular, a variety of exact methods and heuristics have been designed and proposed for solving MCP (Bomze et al. 1999, Wu and Hao 2015) and MWCP (Gruzdeva 2013, Massaro et al. 2002).
P	Due to its simplicity and good performance, it has been used for solving a wide range of theoretical and practical problems such as: Traveling Salesman Problem (TSP), Vehicle Routing Problems (VRP), Arc Routing Problem (ARP), MCP, etc. (see, e.g., Hansen et al. (2001), Schnei75 der et al. (2014), Hansen and Mladenovic (2003), Ferreira da Silva and Urrutia (2010), and references therein).
P	This idea was first proposed by Mladenovic et al. (2010) for a vertex labeling problem.
O	The philosophy of VNS is simple and because of this simplicity, VNS became popular and is widely used for solving a large variety of problems arising from different domains of science and engineering (see e.g., Hansen et al. (2001), Schneider et al. (2014), Hansen and Mladenovic (2003), Ferreira da Silva and Urrutia (2010) and references therein).
P	To this end, we implemented the Bron-Kerbosch al425 gorithm (BKA) that was introduced in (Bron and Kerbosch 1973).
O	A common practice is to break the planning process into several modules that are usually solved sequentially and hence separately (Abdelghany and Abdelghany, 2010; Barnhart and Cohn, 2004).
O	Considering these complexities, major airlines typically adopt an incremental schedule updating approach, in which a preceding flight schedule is modified to accommodate recent changes in demand, competition with other air carriers, and any changes in the available resources (Lohatepanont and Barnhart, 2004).
O	Next, a model similar to the one presented in Lohatepanont and Barnhart (2004) is applied to generate the fleet assignment of this schedule and to simultaneously adjust this flight schedule.
O	The goal of the fleet assignment problem is to optimally assign scheduled flights (i.e. with known departure time) to the most profitable aircraft type in terms of appropriate capacity and performance (see Abara, 1989; Barnhart et al., 2002; Hane et al., 1995; Sherali et al., 2006; Subramanian et al., 1994).
O	Then, a solution approach is used to eliminate unnecessary flights and maintain the set of flights that optimizes a predefined objective (Lohatepanont and Barnhart, 2004; Sherali et al., 2010,2013).
O	The time window is used to specify how much a flight can be shifted from its original departure time to enhance a pre-defined objective (Ahuja et al., 2004; Belanger et al., 2006; Desaulniers et al., 1997; Levin, 1971; Mercier and Soumis, 2007; Rexing et al., 2000).
O	The problem of developing a flight schedule that promotes robust operation has been studied by several researchers including Aloulou et al. (2010), Burke et al. (2010), Lan et al. (2006), Vaze and Barnhart (2012) and Dunbar et al. (2014) Another version of the problem is represented by the work of Yan et al. (2006) which presented a model for air cargo fleet routing and timetable setting with multiple on-time demands.
O	Tang et al. (2008) presented an integrated model and solution algorithms for passenger, cargo, and combi flight scheduling.
P	The model extends the work of Yan and Young (1996) and Yan and Tseng (2002).
P	Yan and Chen (2007), introduced coordinated scheduling models for allied airlines.
P	Jiang and Barnhart (2009) introduced the concept of dynamic flight scheduling with the objective is to match the passenger demand and aircraft capacity for all flights.
P	Pita et al. (2012) developed a model that integrates flight scheduling and fleet assignment considering airport congestion.
P	In another work, Pita et al. (2014) presented a flight scheduling and fleet assignment optimization model for subsidized air transport networks, especially in remote areas.
P	Atasoy et al. (2014) presented an integrated model for airline scheduling, fleeting, and pricing for a monopolized market.
O	In other words, existing models do not address the bi-level optimization problem resulting from the interaction between the airline’s scheduling decisions to maximize profit and the passengers’ itinerary choice among available itinerary options offered by the airline and its competitors (Sherali et al., 2006).
P	Dobson and Lederer (1993) presented a model for airline choice of flight schedule and route prices to maximize profit while considering competitors’ decisions.
P	Lohatepanont and Barnhart (2004) presented a model that integrates schedule design and fleet assignment decisions, while considering demand shift at the itinerary level.
P	Yan et al. (2007) presented a non-linear mixed integer program in which a utility-based choice model is used to determine demand split among the different airlines.
O	In Yan et al. (2008), their previous model is extended to consider the demand uncertainty case.
O	Di Wang et al. (2014) presented a framework that incorporates the spill and recapture effects, where the spill from an itinerary is recaptured by other itineraries based on their attractiveness.
O	When an initial schedule is required by a flight scheduling model (e.g., Lohatepanont and Barnhart, 2004; Sherali et al., 2010, 2013), this initial schedule is typically developed manually based on the experience of the airline’s schedulers.
N	In addition, our model deviates from existing flight scheduling models that emphasize on other scheduling decisions such as fleet assignment or aircraft routing (Ahuja et al., 2004; Belanger et al., 2006; Burke et al., 2010; Mercier and Soumis, 2007; Rexing et al., 2000).
O	Finally, similar to the work of Lohatepanont and Barnhart (2004), the model adequately considers the representation of the passengers’ itinerary choice behavior (Coldren, 2005; Coldren et al., 2003), which allows for accurate estimation of demand spill and recapture among the different itineraries.
O	To formulate the schedule planning problem, a time-space network representation similar to the one described in Berge and Hopperstad (1993) and Hane et al. (1995) is used.
O	Given this maximization problem and assuming a logit-based structure for the travel utilities (Ben-Akiva and Lerman, 1985), the probability pi that an average passenger in city pair {o, d} selects itinerary i among the set of competing non-connecting or connecting itineraries in the choice set  {o,d} is given as follows.
P	In this paper, we use the itinerary choice model developed by Coldren et al. (2003) and Coldren (2005) as an illustrative example.
O	Coldren et al. (2003) identify and select a set of variables that determine the attractiveness of an itinerary for a passenger in comparison to other itineraries.
O	The complete list of these variables with the detailed definition of each variable is given by Coldren et al. (2003) and Coldren (2005).
O	Finally, the problem is known to be NP-hard (Gu et al., 1994) with a nonconvex objective function.
O	The methodology integrates I) a Genetic Algorithm (GA) (Goldberg, 1989; Sastry et al., 2005), II) a passenger assignment-simulation model, and III) a resource-tracking model.
P	As mentioned earlier, to evaluate the revenue associated with each schedule, the solution methodology integrates a passenger flight assignment-simulation model similar to the one presented by Abdelghany and Abdelghany (2008).
O	For this purpose, a discrete choice model developed by Coldren et al. (2003) and Coldren (2005) is used to model the passengers’ itinerary choice behavior.
N	It shall be noted that while there are eighteen models calibrated for the different geographical areas, Coldren et al. (2003) presents only five of these eighteen models for proprietary reasons.
O	A detailed description of this validation exercise can be found in Abdelghany and Abdelghany (2008).
O	In a real implementation of multiple-objective optimization, a set of experiments should be conducted to find the right values of these weights to obtain a meaningful objective function that realistically represents all its components, and identifies the range of the solutions (Deb, 2001, 2014).
P	In this integrated framework, a network based equilibrium model similar to the one presented in Hassan et al. (2009) is used to determine the optimal service frequency for each market.
O	The problems in the transportation have been studied (Caputo and Mininno, 1996; Frisk et al., 2010; Audy et al., 2011; Lozano et al., 2013).
O	The customer’s detailed information was provided by the companies performing PDCA, such as Zhongshan, Shuntian, and Jiantong Inc. in Shenyang in China (Tang et al., 2008, 2014; Yu et al., 2014, 2016).
O	The two problems in the transportation have been researched (Lozano et al., 2013), such as grocery distribution (Caputo and Mininno, 1996), distribution in rural areas (Hageback and Segerstedt, 2004), freight carriers (Krajewska et al., 2008), forest (Frisk et al., 2010), and railway transportation (Sherali and Lunday, 2011).
P	Cruijssen et al. (2007) carry out extensive experiments in order to measure cost savings on a number of characteristics of the distribution problem and found that significant cost savings are achievable.
O	Cruijssen et al. (2010) present an approach for the initiative entering the cooperation among logistics service providers.
P	Krajewska et al. (2008) use the heuristic proposed by Ropke and Pisinger (2006) to solve their problem.
O	To minimize execution costs for a coalition of freight forwarders, Ergun et al. (2007) use a greedy heuristic as well as set partitioning, sets of cycles to solve the instance.
O	The set of solutions includes the kernel, the bargaining set, the stable set, the core, the Shapley value and the nucleolus (Ordeshook, 1986; Osborne and Rubinstein, 1994). Engevall et al. (1998) investigate the routes costs allocation among the customers based on a traveling salesman game.
O	Krajewska et al. (2008) use the Shapley value for sharing profits in cooperative freight carriers in order to balance their request portfolios.
O	Özener and Ergun (2008) propose several cost-allocation schemes based on cooperative game concepts (such as stability and others) applied to a logistics network in which shippers collaborate
P	Sherali and Lunday (2011) analyze four alternative schemes to apportion railcars to manufacturers and propose a new railroad allocation scheme.
O	The Shapley value (Shapley, 1953) is one of the solution methods most common in cooperative game theory.
O	Littlechild and Owen (1973) give a famous simple expression for the Shapley value for airport runway cost games.
O	Kuipers et al. (2013) study the exact expression of Shapley value for the cost sharing in highways.
O	Pickup and Delivery Problem has received widely attention (Cherkesly et al., 2015; Gouveia and Ruthmair, 2015; Iori and Riera-Ledesma, 2015; Madankumar and Rajendran, 2016)
O	Regarding singletrip mode, Tang et al. (2008) establish a multi-objective model of minimizing costs and maximizing service quality, and solve it using a two-stage heuristic algorithm based on savings algorithm.
O	Dong et al. (2008) study the model of minimizing costs by a permutation-based cluster priority heuristics.
P	Dong et al. (2011) propose an exact algorithm for the single-trip mode of PDCA based on set-partitioning model.
P	Tang et al. (2014) propose an exact algorithm based on the tripchain-oriented set-partitioning (TCO-SP) model in order to reduce the operational cost.
P	Regarding reducing carbon emission of PDCA, Yu et al. (2014) investigate the measure to reduce carbon emission of PDCA by multi-trip mode, and demonstrate that multi-trip mode can reduce 17% carbon emission.
O	In the research, Q is set to 4 to protect customers’ satisfaction degree in real PDCA (Tang et al., 2008, 2014)
O	Gershwin (1987), Colledani and Gershwin (2013) and Tolio and Matta (1998) use the decomposition method to analyze a complex system with more than two machines by decomposing it to a set of two-machine lines.
O	Shapley value is one fair method for sharing profit in cooperation (Kuipers et al., 2013).
O	An updated description of variants and methods can be found in Toth and Vigo (2014). 
O	A recent survey on TDVRP variants is available in Gendreau et al. (2015), covering exact and heuristic algorithms.
O	The simplest generalization is the variant of the TDTSP considered in Picard and Queyranne (1978), which has applications within scheduling contexts and generalizes the well-known Traveling Deliveryman Problem (see, e.g., Fischetti et al., 1993; Lucena, 1990; Méndez-Díaz et al., 2008).
P	Exact approaches for this problem can be found in Gouveia and Voß (1995), Abeledo et al. (2012), Miranda-Bront et al. (2013) and Godinho et al. (2014), where instances with up to 100 customers can be solved within reasonable computing times.
P	To the best of our knowledge, the best exact approach in the literature is the Branch-Price and Cut (BPC) proposed in Abeledo et al. (2012).
P	A different approach is proposed in Malandraki and Daskin (1992), where the travel time between two cities depends on the moment of the day in which the arc is traversed.
O	Exact approaches for variants of this problem with minor modifications (i.e., different objective functions and operational constraints) can be found in Stecco et al. (2008), Albiach et al. (2008), MéndezDıaz et al. (2011), Miranda-Bront (2012) and Melgarejo et al. (2015). Regarding applications, Furini et al. (2015) formulate a TDTSP to model an aircraft sequencing problem.
P	To overcome this difficulty, Ichoua et al. (2003) builds upon the model proposed by Hill and Benton (1992) and propose a similar setting as in Malandraki and Daskin (1992) but for the average travel speed within each period.
P	The model proposed by Ichoua et al. (2003) has recently caught the attention of many researchers.
O	Cordeau et al. (2012) tackles the TDTSP with the objective to minimize the makespan.
O	Ghiani and Guerriero (2014) further exploit some of the properties of the travel time function, and study its generality.
P	In a follow up paper, Arigliano et al. (2015) extend the ideas proposed in Cordeau et al. (2012) to the TDTSP with Time Windows (TDTSP-TW).
O	Multi-vehicle versions of the TDVRP have also been tackled by exact algorithms that consider the model proposed in Ichoua et al. (2003). 
O	Dabia et al. (2013) consider the TDVRP with time windows with the objective of minimizing the overall duration instead of the makespan.
O	Related to this research is the work by Sun et al. (2015), where a profitable TDTSP with time windows and precedence constraints are considered.
O	In this paper we tackle the version TDTSP-TW considered also in Arigliano et al. (2015).
N	Firstly, we propose an alternative approach for the TDTSP-TW that builds on the ILP formulation proposed by Sun et al. (2015).
O	Secondly, we evaluate our approach on benchmark instances and compare our results with two sets of instances proposed by Arigliano et al. (2015).
N	In Section 3 we describe with more details some of the developments proposed for the TDTSP and TDTSP-TW with time-dependent travel speeds, and present a new formulation for the TDTSP-TW using the ideas proposed in Sun et al. (2015).
O	In this section we present the definitions and the basic properties of the TDTSP-TW with the travel time model proposed in Ichoua et al. (2003).
O	The route starts at vertex 0 and ends at vertex n + 1, while processing each vertex within its defined time window and computing the travel times following the speed model proposed in Ichoua et al. (2003).
O	For instance, consider the TDTSP studied in Cordeau et al. (2012).
O	These results are extended to the TDTSP-TW in Arigliano et al. (2015).
O	Finally, we also remark some results regarding the generality of the speed model present in Ghiani and Guerriero (2014), where the authors show that any continuous piecewise linear travel time function can be modeled by the travel speeds defined in (1).
N	We begin by showing the formulation proposed in Arigliano et al. (2015) and describing some of its characteristics. 
O	We then present our formulation, which is based on the one proposed by Sun et al. (2015).
O	This section presents the formulation proposed in Arigliano et al. (2015), which we name LBF.
P	Solutions that violate the time windows constraint are forbidden by means of the well known Infeasible Path Elimination Constraints (IPEC), proposed by Ascheuer et al. (2001), in constraints (7).
P	A BC algorithm is developed in Arigliano et al. (2015) using this formulation as starting point.
O	Due to space limitations we omit the details regarding the computation of this bound and refer the reader to Cordeau et al. (2012) and Arigliano et al. (2015).
P	An alternative formulation for the TDTSP-TW can be obtained from the model proposed by Sun et al. (2015) for the Profitable TDTSP with Time Windows and Pickup and Delivery.
P	Sun et al. (2015) report that for this particular problem the model does not produce good results when solved by a commercial solver.
O	We remark that similar decision variables have been considered in the context of the TDTSP by Stecco et al. (2008).
O	The formulation proposed in Sun et al. (2015), which we name TTBF, is shown next.
O	Following the experience reported by Ascheuer et al. (2001), the preprocessing we apply consists of three phases, which are applied in an iterative fashion until no further changes are found.
N	The tightening of the time windows consists in the adaptation of three rules from the classical TSPTW considered in Ascheuer et al. (2001), which allow to adjust the release and deadline times of each vertex.
O	Therefore, we adopt a similar strategy as in Ascheuer et al. (2001) and generalize several constructive heuristics as well some local search operators to the time-dependent case in order to find an initial feasible solution. 
P	Due to space limitations, we skip the implementation details and refer the reader to Ascheuer et al. (2001) for the general definition of each of them.
N	In this sense, as opposed to the approach in Arigliano et al. (2015), none of the following cuts are necessary for the formulation.
O	In addition, as mentioned in the previous section, the approach in Arigliano et al. (2015) considers the computation of an initial lower bound by solving an auxiliary TSPTW subproblem1, which is also used later as part of a set of restrictions that are included as lazy constraints.
N	A different strategy has been adopted and we avoid computing and using in our approach the initial lower bound proposed in Arigliano et al. (2015).
O	The SECs are separated using the routine proposed in Nagamochi et al. (1994).
O	The precedences computed during the preprocessing phase allow us to include valid inequalities from the PrecedenceConstrained TSP proposed in Balas et al. (1995).
O	The BC algorithm includes these inequalities as cutting planes in a similar fashion as in Ascheuer et al. (2001) and Dash et al. (2012).
P	For the separation routine, we also follow the approach proposed in Balas et al. (1995).
O	We first consider the instances reported in Arigliano et al. (2015), which are constructed by extending the instances generated in Cordeau et al. (2012) to the case with time windows.
O	Based on this information, Arigliano et al. (2015) extend the instances by incorporating time windows for the customers in such a way that at least one feasible solution exists.
P	We refer the reader to Arigliano et al. (2015) and Cordeau et al. (2012) for the detailed information regarding the construction and characteristics of the instances.
P	The second set of instances is also proposed by Arigliano et al. (2015)3 but aim to evaluate different characteristics such as the size of the time windows and present a significantly larger number of time periods.
P	LBF-BC: BC algorithm proposed in Arigliano et al. (2015).
O	The results obtained on the first set of instances for methods LBF-BC, TTBF-CPLEX, TTBF-BC and TTBF-CB for traffic patterns A and B defined in Arigliano et al. (2015) are presented in Tables 2 and 3, respectively.
P	We propose an ILP formulation following the research in Sun et al. (2015).
P	Compared to the approach proposed in Arigliano et al. (2015), the proposed BC approaches TTBF-BC and TTBF-CB are able to solve 929 and 940 instances, respectively, out of a total of 960, which represents a difference of more than 300 instances solved.
N	Disney et al. (2000) show that the WIP inventory inaccuracy in a shop floor causes a time-delay in the WIP feedback loop in the production and inventory control system, which makes it difficult to construct a good control policy
P	Wang & Yu (2012) investigate the robust production control problem for a single machine and single part-type manufacturing system with inaccurate WIP inventory, and propose a robust policy that is insensitive to the observation error of inventory.
N	Wang & Chan (2015) study the robust replenishment and production problem for a single-stage production/inventory system with inventory inaccuracy and lead-time for replenishment, which, however, only produces one type of product.
O	The main differences between this paper and the previous three works (Wang & Yu, 2012; Chan & Wang, 2014; Wang & Chan, 2015) are on the following three aspects: 
N	Compared to Wang & Yu (2012) and Wang & Chan (2015), in which only one type of product is manufactured, in this paper we consider an inventory/production system that orders and consumes multiple kinds of raw materials and produces multiple types of products on multiple machines.
N	Compared to Chan & Wang (2014), in which lead-time for replenishment is not considered, in this paper the lead-time for replenishment is also taken into consideration.
O	For example, Wang et al. (2012) develop a radio frequency identification-enabled real-time manufacturing execution system for one-of-a-kind production manufacturing. 
P	Meese (2007) and Rekik et al. (2009) also find that the RFID technology can solve the inventory inaccuracy problem.
O	Therefore, the employment of RFID systems needs to be evaluated carefully (Chang, Klabjan, & Vossen, 2010; Vlachos, 2014).
O	DeHoratius et al. (2008) propose a Bayesian inventory record accounting method for record inaccuracy and an according replenishment policy, which can avoid the phenomenon of “freezing” and recoup much of the cost of inventory inaccuracy
O	Cannella et al. (2015) quantify the impact of inventory record inaccuracy on the dynamics of collaborative supply chains, both in terms of operational performance (i.e. order and inventory stability), and customer service level.
O	Xu et al. (2012) consider a single-period two-echelon supply chain with shrinkage errors based on a single-period newsvendor model.
O	For example, in a case study on the ERP and SCM systems of a valve manufacturing in China, Bose et al. (2008) find errors exiting in the WIP inventory records of this enterprise.
O	For example, to minimize the scheduling performance difference between the worst case and the optimal scheduling, Kouvelis et al. (2000) measure the uncertainty in processing time by the range of its fluctuation and model the robust scheduling problem by an integer programming model.
P	Liu et al. (2007) consider the uncertainty form the arrival time of products and proposed a distributed scheduling algorithm to improve the robustness of job shop scheduling.
O	Benjaafar et al. (2011) consider the system where the announcements to order products arrive continuously over time according to a Poisson process with rate λ, and the amount of time between when an order is announced and when it becomes due is random.
N	For example, Herbon et al. (2003) investigate the impact of uncertain future events on decision-making in a stochastic environment, and point out that one can use substantially restricted amount of information to optimally control a real system.
N	Zhu et al. (2013) introduce the REVD (relative expected value of distribution) model for robust decision-making with incomplete distribution information, and apply it to the newsvendor problem.
P	Similar problems are also studied by Perakis & Roels (2008) and Yue et al. (2006). Cao et al. (2014) propose a partial-information-state-based approach to optimize the long-run average performance in a partially observed Markov decision process (POMDP).
O	However, DeHoratius et al. (2008) point out that direct optimization of the replenishment policy under inaccurate records requires solution of dynamic programs whose state is given by a probability distribution.
O	Moreover, as addressed in Section 1, this paper is an extension of our pervious works (i.e., Wang & Yu, 2012; Chan & Wang, 2014; Wang & Chan, 2015) on related issues from the perspectives of complexity of systems, model, and solving methods.
O	Wu and McGinnis (2011) and Wu (2014) classify downtime events as either run-based or time-based arrival.
O	Closed-form solutions to predict queue performance are nearly intractable due to this inherent complexity and, hence, Buzacott and Shanthikumar (1993) and many others approximated performance analysis.
O	Since it is possible to model a system with downtimes as a multi-class system, our problem can be approached via decomposition without aggregation (DWOA) as done by Bitran and Tirupati (1988) and Whitt (1994).
O	Methods that cope with the non-renewal challenge in singleclass systems, such as Whitt (1995) or Wu and McGinnis (2013), are inapplicable in systems with downtimes even when capturing downtimes by service rate and variability
O	Kim (2005) was the first to reach a viable approximation for such systems by merging (for FCFS policy) the DWOA procedure with the variability function (VF) method proposed by Whitt (1995). Sagron et al. (2015) integrated Kim’s method with a simulation-based VF to approximate the between-class effect in tandem queues with downtimes and reached a more accurate approximation while enabling various service policies (not only FCFS) of handling downtimes.
N	To achieve better computation efficiency than demonstrated by Sagron et al. (2015), this paper proposes the Regression-Based Variability Function (RBVF) to approximate the between-class effect in a queue with downtimes.
O	Similar to other decomposition methods (e.g., Whitt (1983, 1994); Segal and Whitt (1989); Suri et al. (1993)), we first decompose the tandem queue into a set of separate queues, each having independent arrival processes
O	Sagron et al. (2015) basic structure of class-departure variability involves the within-class and between-class effects.
P	The structure of the approximation in (3.1), which Sagron et al. (2015) proposed, was on Whitt’s (1994) DWOA procedure for multi-class queuing networks with class-dependent service.
O	The structure proposed previously by Sagron et al. (2015) differentiates between the within-class and between-class effects that influence the departure process of Class-1.
O	Sagron et al. used simulation for the between-class effect CB2 i and Whitt’s (1995) VF for nonrenewal processes.
O	As shown in Stanford and Fischer (1991) and Stanford (1997), the departure variability of Class-1 in W-class-queue (Cd2 1) is:
O	The first work we exploit here is of Stanford and Fischer (1991) according to whom E(V2) for FCFS policy is:
O	Notice that when μ1 = μ2 and p1 = p2, this H2,M/G,G/1 queue is reduced to the M,M/G,G/1 queue, similar to the results shown by Saito (1989) and Stanford and Fischer (1989).
O	The approximately linear relation associated with the departure process has already been formulated in the literature such in Kim et al. (2005).
O	Whitt (1995) first suggested using VF with decomposition procedures in order to refine the approximation of departure variability.
P	Numerical experiments show that for the range of 0 to 8 in the Ca1 2 and Csj 2 parameters, the error versus simulation mostly rises for queues more remote from the outset if the following common two-moment approximation for waiting times proposed by Kingman (1962) is used:
O	These results were obtained when we derived the arrival variability at queue j + 1 as follows by using Marshall’s (1968) result for the actual departure variability at queue j:
O	The cause for the gaps in errors lies in the variability of the non-renewal process, as shown by Suresh and Whitt (1990) and Sagron et al. (2015) who demonstrated different causes that create the heavy-traffic bottleneck phenomenon.
P	To improve prediction of queue performance, Whitt (1995) showed that, instead of using the actual SCV, it is better to characterize the variability of the non-renewal process approximately by the adjusted variability as reflected on a downstream queue.
P	For this purpose, we use the following Kramer and Langenbach-Belz’s (1976) approximation:
O	For priority G,M/G,G/1 queue (preemption and non-preemption), Cd2 1 is calculated exactly by Stanford (1997).
O	In other general cases with FCFS policy, it is possible to approximate Cd2 1 following Fischer and Stanford (1992) as well as Whitt (1994):
O	Comparison of the RBVF results is to other existing analytic methods (Whitt 1994; Kim 2005), which derive the same prediction for both policies, and to another method (Sagron et al., 2015) that like RBVF incorporates simulation and analytic methods.
O	Evaluation of the predictions is by relative deviation (RD) from predictions obtained by simulation of the tandem in each scenario, not to be confused with sub-simulations with 2-tandem queues used within the RBVF and the Sagron et al. (2015) methods, both of which incorporating simulation and analytic methods.
O	The first method compared is decomposition without combining VF, where CD2 1,1 is approximated by (3.10) as proposed by Whitt (1994), and the rest of CD2 j(ρn) s for j ∈ {2, .., n} are approximated by (3.6) as proposed by Marshall (1968) and refined by Whitt (1983).
O	Then the comparison targets two decomposition methods (Kim 2005; Sagron et al., 2015) that incorporate VF and differ from RBVF only in the approximation of CD2 1,1(ρn).
O	According to Sagron et al. (2015), the estimation of the coefficient of Ca2 1,1 in (4.2) and (4.3) is by the least squares method, as obtained by various cases in sub-simulations of 2-tandem queues.
O	The intercept for a between-class effect in these approximations is calibrated for the bottleneck queue with ρ∗= 0.9, where the sub-simulation of 2-tandem queues as defined in Sagron et al. (2015) derives Wq(0.9)= 12.49 under FCFS policy, and Wq(0.9)= 15.91 under priority policy.
P	Compared to the two analytic methods (Whitt 1994; Kim 2005), the RBVF method proposed here is more accurate with significantly smaller RD errors.
O	Compared to the method that combines an analytic approach and simulation (Sagron et al., 2015), the RSE effort of the RBVF simulation is significantly lower for both policies.
O	In addition, the RBVF method is applicable to all first two-moments of inter-arrival and all service duration distributions of the class of interest, while Sagron et al., 2015 method is applicable only to the second moment of inter-arrival.
N	Compared with the two analytic methods of Whitt (1994) and Kim (2005), the proposed RBVF method is more accurate and more efficient for both policies with significantly fewer ARD errors and significantly less RSE effort required for the simulation.
N	Since the simulation efforts are the same as for the 5-queue scenario, the RBVF method is still preferable to Sagron et al. (2015) method.
O	The efficiency advantage is in terms of the computation efforts being about five times smaller than those associated with the existing analytic procedure combined with simulation of Sagron et al. (2015).
O	The accuracy advantage is in terms of the relative errors being about 3 times smaller than that associated with the method of Kim (2005) and about 6 times smaller than that associated with the method of Whitt (1994).
P	To handle this issue, an alternative modeling approach, socalled multigraph representation, was proposed by Garaix et al. (2010).
O	A key paper in this regard was proposed by Garaix et al. (2010). 
O	Lai et al. (2016) considered this issue for the heterogeneous VRP with limited duration.
P	Following Garaix et al. (2010), they introduced alternative arcs between pair of vertices.
O	The experiments confirmed, on a different problem, the observations made in Garaix et al. (2010).
P	Wang and Lee (2014) introduced the so-called Time Dependent Alternative Vehicle Routing Problem (TDAVRP) that also involves a multigraph representation.
P	To solve this problem, Wang and Lee (2014) developed a heuristic algorithm based on Particle Swarm Optimization (PSO).
P	More recently, Reinhardt et al. (2015) introduced a new generalization of the VRPTW in which additional fixed costs are associated with subsets of edges.
P	Reinhardt et al. (2015) proposed an extension of the VRPTW to represent the problem, called Edge Set Vehicle Routing Problem with Time Windows
P	Recently, Letchford et al. (2014) revisited the branch-and-price approach presented in Garaix et al. (2010) (see Section 2.1), for the VRPTW.
O	It is worth mentioning the connections between Letchford et al. (2014) and a stream of papers addressing variants of the Travelling Salesman Problem (TSP) defined on general graphs, such as road networks.
O	An early work was carried out by Fleischmann (1985).
O	Independently, Cornuéjols et al. (1985) investigated the same problem that they called the Steiner Traveling Salesman Problem (STSP).
O	Recently, in connection with Letchford et al. (2014); 2013) also addressed the STSP.
O	Finally, Huang et al. (2006) investigated a multi-objective TSP in the context of a tourist sightseeing-itinerary planning
P	Garaix et al. (2010) and Lai et al. (2016) gave empirical evidences of these effects in two very different contexts
O	Lai et al. (2016) acknowledged several limits in their conclusions.
O	Garaix et al. (2010) based their conclusions on exact solutions, but only compared the multigraph representation to the fastest-paths-based simple graph.  2671: ) Solomon’s instances, that are a mustdo for the VRPTW; (2) realistic instances constructed following Letchford et al. (2014); (3) instances obtained from real road networks
P	Letchford et al. (2014) suggested that a branch-and-price algorithm for the VRPTW that works directly on the original road network, rather than on a multigraph, could be more efficient in some cases.
O	The branch-and-price has first been applied to the VRPTW by Desrochers et al. (1992).
O	It is based on the Dantzig–Wolfe decomposition (Dantzig and Wolfe, 1960).
P	The column generation sub-problem, so-called pricing problem, aims at finding a set of feasible columns that will be added to the master problem (Desaulniers et al., 2006; 2014; Desrosiers et al., 1995; Feillet, 2010).
O	Desrochers et al. (1992) observed that the ESPPRC, thought NP-hard in the strong sense, admits a pseudo-polynomial algorithm when the elementary path condition is removed.
P	To solve this problem, we adapt the dynamic programming algorithm described in Feillet et al. (2004) to the multigraph case of the ESPPRC.
N	Such solutions provide, most of the times, inappropriate estimations of marginal costs associated with customers, which can lead to very slow convergence (Rousseau et al., 2007).
O	To handle these difficulties, a few techniques are available in the literature that consists mainly in preventing dual variables from taking extreme values and in finding better approximations of the optimal marginal costs (e.g., Rousseau et al., 2007 and Du Merle et al., 1999).
P	In our algorithm, we implemented the interior point method proposed by Rousseau et al. (2007).
O	In our experiments we use four sets of VRPTW instances; the first set of instances is derived from Solomon (1987) benchmark instances for the VRPTW, the second set of instances is generated by Letchford et al. (2014) and we used the same procedure proposed in Letchford et al. (2014) (described below) to generate the third set of instances.
O	The difference between Letchford et al. (2014) instances and those we generated is that we used different densities of customers in the road network.
O	The first set of instances consists of 90 instances derived from Solomon (1987) benchmark instances: R101 to R105, C101 to C105 and RC101 to RC105.
P	Note that these correlation rules follow those introduced in Letchford et al. (2014).
O	Letchford et al. (2014) and Letchford et al.-like (LL) instances.
O	The second set of instances (Letchford et al., 2014 instances) were provided to us by the authors.
O	Using this procedure, Letchford et al. (2014) generated different sparse graphs with N ∈ {25, 50, 75, 100} nodes.
P	Multigraph construction is performed with the preliminary method developed by Ben Ticha et al. (2016).
P	For more details, interested readers are referred to Ben Ticha et al. (2016).
O	Table 3 reports statistics on multigraphs for Letchford et al. (2014) instances.
O	Less alternative arcs exist in Letchford et al. (2014) instances.
O	Obtained results are presented in Tables 6 and 7 for modified Solomon instances, in Table 8 for Letchford et al. (2014) instances, in Table 9 for LL instances and in Table 10 for real instances.
O	Table 8 summarizes results obtained for Letchford et al. (2014) instances.
O	Our results confirm and complete results already reported in Garaix et al. (2010) and Lai et al. (2016) for other categories of problems.
P	Hence, a relevant issue would be to follow Lai et al. (2016) and develop dedicated solution techniques for the multigraph.
P	As we mentioned, a first paper in this regard was proposed by Letchford et al. (2014).
P	According to Letchford et al. (2014), tackling the problem directly with the road network is more efficient, at least at the root node of the branch-and-price tree (the complete branch-and-price scheme was not developed).
O	We started some preliminary experiments and obtained very contrasted results that did not necessarily confirm the conclusions drawn in Letchford et al. (2014).
O	The reason might be that Letchford et al. (2014) consider instances with a high density of customers and allow non-elementary routes in the linear programming relaxation.
P	However, the authors of Noon and Bean (1993) provide reductions from each variant to the exactly-one-in-a-set GTSP, and thus the results in this paper can also be applied to these variants.
O	The GTSP has a variety of applications in operations research (Laporte et al., 1996), including material flow design, vehicle routing, and post-box collection.
O	In robotics, a common problem is to plan tours through a set of points in a robot’s workspace (Saha et al., 2006).
O	A common solution technique is to convert the problem into a GTSP, in which each set contains a sample of different robot configurations for the given location (Ny et al., 2012; Obermeyer et al., 2012; Saha et al., 2006).
O	The GTSP also arises in complex motion planning problems in which the goal is to compute a tour over a set of locations, but with additional constraints on which combinations of locations are or are not allowed in the tour (Imeson and Smith, 2015; Mathew et al., 2015; Wolff et al., 2013).
O	If the edge weights are metric, then the best known approximation algorithm yields an approximation factor of O(log2nlog log nlog m) (Garg et al., 2000), where n is the number of vertices and m is the number of sets.
O	A commonly used solution approach is to reduce the GTSP to an asymmetric TSP instance using the Noon–Bean reduction (Ben-Arieh et al., 2003; Noon and Bean, 1993) and then solve the TSPinstance with a standard TSPsolver (Helsgaun, 2000; Lin and Kernighan, 1973).
O	Other approaches include Lagrangian relaxations (Noon and Bean, 1991) and branch-and-bound techniques (Fischetti et al., 1997) based on properties of the integer linear program representation (Fischetti et al., 1995).
P	In Fischetti et al. (1997), the authors also developed a library of GTSPinstances, called GTSP-Lib, by taking TSP-Lib instances and performing clustering on the vertices.
P	This library was subsequently extended to add larger problem instances in Silberholz and Golden (2007).
O	Early algorithms included Snyder and Daskin (2006) and Silberholz and Golden (2007). 
P	In benchmarks on the GTSP-Lib, performance was subsequently improved in Bontoux et al. (2010) and then in Gutin and Karapetyan (2010). 
P	The memetic algorithm in Gutin and Karapetyan (2010), called GK, yields impressive performance on GTSP-Lib, with runtimes under 60 s and tours consistently within 0.1% of the best known.
N	However, it was shown in Drexl (2013) that the memetic algorithm (Gutin and Karapetyan, 2010) solver’s performance degraded significantly with problem size on rural postman problems, in particular for problems consisting of more than 200 sets.
N	In Karapetyan and Gutin (2011), a generalization of the successful Lin–Kernighan TSPheuristic (Lin and Kernighan, 1973) was proposed for the GTSP, and while runtimes were impressive, the heuristic did not provide better performance than the memetic algorithm in Gutin and Karapetyan (2010).
P	A particle swarm based approach was also proposed in Tasgetiren et al. (2007).
P	Most recently, Helsgaun (2015) combined the Noon–Bean reduction and the powerful TSPsolver LKH (Helsgaun, 2000) to produce the GLKH solver.
P	This solver improved solution quality on GTSP-Lib instances over the GK solver in Gutin and Karapetyan (2010).
O	GLKH is also tested on several other problem libraries, including Large-Lib (proposed in Helsgaun (2015)), MOMLib (Mestria et al., 2013), BAF-Lib (Bontoux, 2008), and ARC-Lib1, which include directed rural postman problems, with GLKH showing a very strong performance on these libraries.
P	Work in Ropke and Pisinger (2006) and Pisinger and Ropke (2007) initially proposed the idea of ALNS for pickup and delivery problems.
O	The ALNS framework has since been successfully applied to several different problems, including in two-echelon vehicle routing (Hemmelmayr et al., 2012), in capacitated vehicle routing (Ribeiro and Laporte, 2012) and recently in continuous berth allocation (Mauri et al., 2016).
P	For insertions we build upon the classic TSP insertion heuristics (Rosenkrantz et al., 1977) and their extensions to the GTSP proposed initially in Fischetti et al. (1997).
O	Our approach is related to the early work in Renaud and Boctor (1998), which proposes a GTSP solver that constructs an initial tour, followed by cheapest insertion of the remaining sets and a local optimization.
O	Adaptive large neighborhood search, and the GLNS solver, fall into a class of algorithms called hyper-heuristics (Burke et al., 2003), as they use online learning for heuristic selection (Burke et al., 2010).
N	We propose a novel insertion mechanism that contains as special cases nearest, farthest and random insertions from Fischetti et al. (1997).
N	Similarly, we propose several new removal methods for the GTSP that generalize the idea of Shaw removal (Shaw, 1997) and worst removal (Ropke and Pisinger, 2006).
O	We provide benchmarking results using the default GLNS settings on six problem libraries: four existing libraries from the literature and two new libraries that stem from our recent work on robot motion planning languages (Imeson and Smith, 2015).
O	We compare the performance of GLNS to the stateof-the-art solvers GLKH (Helsgaun, 2015) and GK (Gutin and Karapetyan, 2010).
O	The GTSP is NP-hard (Noon and Bean, 1993) and contains the TSP as a special case (i.e., where |Vi| = 1 for each set i ∈ {1, . . ., m}).
O	There are several variations to the GTSP, including the case where the cycle must contain at least one vertex in each set and the case in which the sets Vi are not disjoint (Noon and Bean, 1993).
O	There are well known reductions from both of these problems to the GTSP as stated above (Noon and Bean, 1993).
O	In line 13 of Algorithm 1 we accept or decline the modified tour Tnew based on a standard simulated annealing criterion, as in Ropke and Pisinger (2006) (details are in Section 5.3).
O	In this section we begin by summarizing four insertion heuristics that are proposed in Fischetti et al. (1997) as extensions of the well-known TSP insertions (Rosenkrantz et al., 1977).
P	In Rosenkrantz et al. (1977), four insertion heuristics were proposed for iteratively constructing a TSP tour: nearest, farthest, random, and cheapest insertion.
P	In Fischetti et al. (1997, Section 4), they proposed extensions of the TSP insertion heuristics (Rosenkrantz et al., 1977) to the GTSP as follows:
P	In Fischetti et al. (1997), the distance was proposed as dist(Vi, u) = minv∈Vi w(v, u) rather than the form in (1).
P	For TSP instances, each Vi contains exactly one vertex, and the four insertion heuristics are exactly those proposed in Rosenkrantz et al. (1977).
O	For metric instances, it is shown in Rosenkrantz et al. (1977) that nearest and cheapest insertions provide 2-factor approximations to the optimal tour, while farthest and random insertions provide log2 |V| + 1-factor approximations.
O	The mechanism for choosing a set is similar to softmax (Sutton and Barto, 1998), in which λ is fixed, and the set Vi is chosen with probability proportional to exp (λdi).
O	Given an unsorted array of  numbers, selection of the kth smallest value can be performed in expected time of O() using a simple randomized algorithm (Cormen et al., 2009, Chapter 9).
O	Given these distances, Algorithm 3 runs in expected time of O(Nr) using the randomized selection in Cormen et al. (2009, Chapter 9), and the distances di can be updated in O(Nr) time after each subsequent insertion (by the implementation details above).
P	A more efficient implementation of cheapest insertion requires significantly more space (Rosenkrantz et al., 1977).
O	The runtimes for nearest, farthest, and random insertion follow from the analysis in the proof of Proposition 3.4, and are simple extensions of the runtimes in Rosenkrantz et al. (1977).
P	The performance of large neighborhood search algorithms are improved by increasing randomness during removals and insertions (see for example Ropke and Pisinger (2006) or Pisinger and Ropke (2007)), which helps in exploring a larger portion of the solution space and in avoiding repetition of locally optimal choices.
O	(TSP insertion runtimes Rosenkrantz et al., 1977).
P	Motivated by Ropke and Pisinger (2006), given a maximum noise level η ≥ 0, we perform the following: For each vertex v ∈ Vi and edge (x, y) ∈ ET, we generate a uniform random number rand ∈ [0, η] and compute the insertion cost as (1 + rand)  w(x, v) + w(v, y) − w(x, y) .
O	For values of λ ∈ (1, ∞) we obtain randomized versions of worst removal with varying degrees of randomization, similar to the randomization method proposed in Ropke and Pisinger (2006).
O	Note, in Ropke and Pisinger (2006), the magnitude of the additive term was based on the maximum edge cost in the problem instance.
O	At each iteration, we use a standard roulette wheel selection mechanism (Ropke and Pisinger, 2006) to select a removal and an insertion heuristic from the corresponding banks according to their weights.
P	Following the strategy in Ropke and Pisinger (2006), we initialize the temperature such that in the first trial, a tour with p1% higher cost than the initial tour (p1 is an solver parameter) is accepted with a probability of 1/2.
O	As in Ropke and Pisinger (2006), we record the same score for the insertion and the removal, calculating the score for the iteration as
P	A Flexible Manufacturing System (Gray et al., 1993; Crama, 1997), or FMS, provides versatility and flexibility in production planning with respect to the variety of manufactured products, and fast adaptability in case of unexpected problems.
O	As showed by Tang and Denardo (1988), the SSP can be decomposed in two subproblems:
O	The Sequencing Problem is an N P-Hard problem as demonstrated by Crama et al. (1994).
O	On the other hand, the Tooling Problem is trivial and can be accom36 plished in deterministic polynomial time by the Keep Tool Needed Soonest (KTNS) policy, as shown in Tang and Denardo (1988).
O	For a fixed sequence of jobs, the optimal schedule for tool switches is trivially generated in deterministic polynomial time by the KTNS policy (Tang and Denardo, 1988).
P	The first work to address the SSP was Tang and Denardo (1988), which formally introduced the SSP and the KTNS pol90 icy for the Tooling Problem.
P	Gray et al. (1993) provides an integrated resource planning hierarchy and a clas94 sification of tool management issues in flexible manufacturing systems, including the SSP.
O	Later, Crama (1997) also discussed problems in flexible manufacturing systems and the SSP as well.
O	A formal proof that the SSP is N P-Hard when C ≥ 2 is found in Crama et al. (1994).
P	Proceeding with the same idea, Hertz et al. (1998) proposed several metrics to estimate edge cost on a TSP graph.
P	Additionally, various TSP heuristic methods were adapted to reflect some peculiarities of the SSP, including the GENI and GENIUS algorithms (Gendreau et al., 1992), which delivered the best results. 
O	Privault and Finke (1995) modeled the tooling problem with nonuniform switching times as a network flow problem on acyclic graphs; this strategy was also applied in a case with uniform switching times as an alternative to the KTNS policy.
P	It also models the SSP as the K-Server Problem and proposes an adaptation of the partitioning algorithm (McGeoch and Sleator, 1991).
O	Later, Privault and Finke (2000) reported that this algorithm outperformed the farthest insertion heuristic and a job grouping heuristic based on the shortest edge and 2-opt heuristics.
O	Hertz and Widmer (1996) considered an n-job shop environment with nonidentical machines and tooling constraints.
O	Avci and Akturk (1996) considered precedence constraints among jobs, tool sharing, and tool lifetime while addressing the tooling problem using an integer programming model.
N	Other SSP variations described in Matzliach (1998) considered tools of non-uniform sizes in an online environment, where complete in formation about the jobs to be processed is not available
P	Djellab et al. (2000) introduced an SSP representation using hypergraphs and proposed the iterative best insertion heuristic.
P	The reported results improved on the previous results of Crama et al. (1994), although at a cost of increased computational time.
P	Heuristic methods previously proposed in the literature were applied to real-world industries by Shirazi and Frizelle (2001).
O	An SSP with identical parallel machines and job processing times was addressed by Fathi and Barnette (2002).
O	Song and Hwang (2002) considered and formally described a TP case in which the objective is to minimize the number of tool transporter movements between the flexible machine and the tools’ storage location, rather than minimizing the number of tool switches.
O	A production scheduling problem in the context of printed circuit board manufacturing, equivalent to the SSP, was considered in Ghrayeb et al. (2003).
P	Al-Fawzan and Al-Sultan (2003) developed five different versions of tabu search, using long- and short-term memory with strategic and probabilistic oscillations to guide the heuristics.
O	Denizel (2003) presented an integer linear programming formulation, a branch and-bound procedure, and a lower bounding procedure using Lagrangean decomposition (later transformed into a Lagrangian decomposition-based heuristic) for group ing jobs that share tools, such that each group does not exceed the capacity of the magazine.
O	An SSP offline case in which each tool may occupy more than one slot of the tool magazine (i.e., the tool sizes were non-uniform) was considered for the first time in Tzur and Altman (2004).
O	Laporte et al. (2004) proposed an integer linear programming model and used a branch-and-cut algorithm to solve it.
O	Zhou et al. (2005) applied a beam-search metaheuristic based on a new branch176 and-bound scheme for the SSP.
P	This was the first time that a beam-search was applied to the problem, and the method improved the results found by Bard (1988).
O	The GENIUS method, previously applied to the SSP by Hertz et al. (1998), received a preprocessing phase in Salonen et al. (2006); this phase grouped jobs with similar tools into super jobs.
P	Revisiting the SSP, Crama et al. (2007) proved that considering non-uniformly sized tools and fixed job sequences is also an NP-Hard problem.
P	Ghiani et al. (2007) proved that the SP is a symmetric problem and improved the previous branch-and-bound method of Laporte et al. (2004) by exploiting such a property in the model.
O	Ant Colony Optimization approaches were presented by Konak and Kulturel Konak (2007) to minimize the number of tool switching instants, rather than the number of tool switches.
O	The computational experiments considered the instances from Denizel (2003) and reported that the method was able to match a large portion of the optimal solutions available and find near-optimal solutions in the other cases.
P	Later, two tabu search implementations Konak et al. (2008) were proposed to address the same problem.
O	Amaya et al. (2008) were the first to approach the SSP with a memetic algorithm.
P	The memetic, being the best performer among the three proposed algorithms, was able to match the results from the beam-search of Zhou et al. (2005).
P	A new set of instances for the SSP was introduced by Yanasse et al. (2009), along with a new branch-and-bound algorithm.
N	Although this algorithm obtained better solutions than Laporte et al. (2004) on problems the latter failed to solve, this new algorithm could not generate solutions for instances with 25 jobs and 15, 20, or 25 tools.
N	Senne and Yanasse (2009) presented a new beam search based on the Yanasse et al. (2009) branch-and-bound, which also outperformed the results achieved by Zhou et al. (2005).
O	Zeballos (2010) considered an SSP variation using parallel machines, due dates, part routing, tool lifetimes, and dependent processing times.
P	Ghiani et al. (2010) proposed an SSP formulation based on the Minimum Cost Hamiltonian Cycle problem with a nonlinear objective function.
P	A branch-and-cut algorithm was proposed to solve this formulation, and experiments showed that it could solve problems with up to 45 jobs and 30 tools in a relatively short computa221 tional time compared with the Laporte et al. (2004) branch-and-bound.
O	Again, Amaya et al. (2011) approached the SSP with memetic cooperative al223 gorithms that compose a chain of memetic agents, which communicate with each other using classic network topologies such as ring, broadcast, and random.
N	This new memetic algorithm obtained better results than Amaya et al. (2008) and, consequently, better results than Zhou et al. (2005).
P	A two-phase heuristic was proposed by Chaves et al. (2012), comprising a construction phase where an initial feasible solution is built, and an improvement phase that employs a standard implementation of the iterated local search metaheuristic.
P	The solution found by this method was applied as an upper bound to the Yanasse et al. (2009) branch-and-bound; this reduced computational time by up to 83%, and reduced the number of explored nodes on many instances.
P	A third version of the memetic algorithm, proposed by Amaya et al. (2013), included a refinement based on the cross-entropy technique, which is a Monte Carlo approach to combinatorial optimization.
P	This new version of the memetic algorithm improved on the results achieved by Amaya et al. (2011).
O	An SSP with tools of non-uniform size was studied by Marvizadeh and Choobineh (2013).
O	Catanzaro et al. (2015) surveyed integer programming models for the SSP, highlighting the main features of each one.
O	Raduly-Baka and Nevalainen (2015) presented the Modular SSP.
O	Adjiashvili et al. (2015) considered a version of the TP with setup times for loading tools, processing times for jobs, and the ability to perform tool switches without stopping the flexible machine.
P	Currently, the state-of-the-art method for solving the SSP is represented by a hybrid metaheuristic proposed by Chaves et al. (2016).
P	This method, named CS+BRKGA, recently obtained the best results in many instances reported in the literature, including those proposed by Yanasse et al. (2009) and Crama et al. (1994).
O	Burger et al. (2015) approached the Color Print Scheduling Problem, which can be modelled as the SSP.
P	The SSP formulations of Tang and Denardo (1988) and Laporte et al. (2004) were applied in a real-world case study.
P	The authors concluded that the model of Tang and Denardo (1988) performed better for small instances and the model of Laporte et al. (2004) performed better for larger instances.
P	Additionally, solution methods based on the decomposition of the SSP into job grouping and group sequencing, as proposed by Salonen et al. (2006), were implemented and compared to each other.
P	Recently, Beezão et al. (2017) considered the SSP on identical parallel machines, and proposed an adaptive large neighborhood search implementation and two mathematical formulations. 
N	These formulations had difficulties solving even small instances with 15 jobs, and the metaheuristic was able to outperform the two heuristics of Fathi and Barnette (2002) on randomly created instances that represents six different scenarios.
O	Further, Furrer and Mütze (2017) recently presented a branch-and-bound based algorithmic framework for the SSP that considers setup and processing times, tool switches, and switching instants.
P	The computational experiments used real-world instances from the Mailroom Insert Planning Problem (Adjiashvili et al., 2015) and other randomly generated problems, which were solved optimally in a few seconds.
P	This is a new heuristic in the SSP context; however, it was inspired by a method for solving a related problem (Carvalho and Soma, 2015).
O	Tang and Denardo (1988) designed a complete graph G = (V, E), in which the set nodes represent the set of jobs J and the weight of edges (i, j) ∈ E is an upper bound to the number of tool switches that occur when this pair of jobs is processed consecutively
P	Other authors (Crama et al., 1994; Hertz et al., 1998; Privault and Finke, 2000; Catanzaro et al., 2015) have also adopted this graph representation, although edge weight estimatives and solution methods vary.
P	A second type of graph was proposed by Chaves et al. (2012), in which nodes represent tools and edges connect tools required by the same job, inducing cliques (i.e., complete subgraphs) on the graph.
O	Crama et al. (1994) presented a definition for 0-blocks and 1-blocks while studying the SSP: a maximal sequence of consecutive entries (either 0 or 1, respectively) in the same row of a binary matrix
O	Crama et al. (1994) explored a particular situation in which each row of S φ has exactly one 1-block.
O	Following this idea, Crama et al. (1994) attempted to find submatrices of S φ holding this property.
O	Owing to the nature of these newly proposed methods, we chose the Iterated Local Search (ILS) metaheuristic (Lourenço et al., 2003) to this end.
N	Although a standard implementation of this method was recently used (Chaves et al., 2012) to address the SSP, we believe that the use of tailored components contributes to improved metaheuristic performance.
O	Parameters α and β, as well as the order of the local search procedures and the number of iterations, were defined using the irace package (LópezIbáñez et al., 2016).
O	The reported results are compared to the state-of-the-art CS+BRKGA method proposed by Chaves et al. (2016) briefly described in Section 2.
O	Notice that the standard deviation of the CS+BRKGA method was originally reported only for groups C3 and C4 of Crama et al. (1994).
O	The 1,350 instances proposed by Yanasse et al. (2009) were divided into five groups (A, B, C, D, and E). Groups A, B, C, and D differ by the number of tools and number of instances.
P	Because this group is a trivial one, both methods were able to find all optimal solutions proved by Yanasse et al. (2009).
O	We also compared the ILS results to the lower and upper bounds from Chaves et al. (2012).
O	The 160 instances proposed by Crama et al. (1994) are divided into four groups (C1, C2, C3, and C4), which differ from each other by the number of jobs, tools, and machine capacities.
P	Catanzaro et al. (2015) introduced a new set of 160 instances, divided into four groups (datA, datB, datC, and datD) that differ from each other by the number of jobs, tools, and machine capacities.
P	Column BKS represents the best known solution, reported by Catanzaro et al. (2015) along with the instances.
O	It did not specify the method and running time used to achieve these solutions, although the original article compares integer and linear programming models of Tang and Denardo (1988), Laporte et al. (2004), and three other new models.
P	As can be noticed, this set of instances is quite similar to the set of instances proposed by Crama et al. (1994).
P	The first cross-docking terminals date back to 1930’s and were introduced by the US trucking industry and then around 1950’s by the US army (Arnaout et al., 2010).
P	Its cross-docking practices are known to be one of the most efficient implementations of this logistics strategy in supply chain management (Chandran, 2003).
O	Federal Express, the United Postal Services, and the US Postal Service are prototypical examples of the cutting edge in cross-docking (Apte and Viswanathan, 2000).
O	Belle et al. (2012) present a comprehensive review and classification of problems arising in crossdocking. 
P	Buijs et al. (2014) provide a recent survey of crossdocking networks that highlights the need for integration and synchronization of such decision problems.
P	For additional reviews on various cross-docking related topics, we refer to Boysen and Fliedner (2010) and Agustina et al. (2010).
P	Naskaris et al. (2014) presents a case study in which a cross-dockstrategy is evaluated to optimize the supply of local produce to grocery stores in North Carolina, USA.
O	The CDAP was first addressed by Peck (1983).
O	In Tsui and Chang (1990), the authors study a CDAP model where each inbound door is assigned to only one origin and each outbound door to only one destination, respectively.
P	Tsui and Chang (1992) propose a branch-and-bound method to solve the problem studied by Tsui and Chang (1990).
N	Zhu et al. (2009) extend the existing CDAP models to consider a more realistic case where the number of origins and destinations is much larger than the number of inbound and outbound doors.
P	Guignard et al. (2012) introduce heuristics to solve the model introduced in Zhu et al. (2009).
P	Nassief et al. (2016) propose an MIP formulation for the CDAP that is solved by means of Lagrangean relaxation to derive lower and upper bounds.
O	Nassief et al. (2017) study several MIP formulations for the CDAP which are theoretically and computationally compared. 
P	For additional works on the CDAP we refer to Oh et al. (2006) and Bozer and Carlo (2008).
O	We also refer to Luo and Noble (2012), Liao et al. (2013), Kuo (2013), Choy et al. (2012), and Nassief et al. (2017) for other CDAPs that incorporate additional features such as limited capacity on storage and staging areas and scheduling decisions.
P	The VRPCD was initially introduced by Young et al. (2006).
O	Another tabu search algorithm is introduced by Liao et al. (2010) and compared against the one of Young et al. (2006).
N	Wen et al. (2009) consider the same problem without the synchronization constraint.
P	For similar works introducing heuristic algorithms we refer to Tarantilis (2013), Dondo and Cerd´a (2013) and Morais et al. (2014).
O	Santos et al. (2011a) study a model that integrates vehicle dependencies by penalizing the unloaded requests at the cross-dock.
P	Santos et al. (2011b) introduce an alternate formulation for the same problem by representing pick-up and delivery routes with a single variable.
P	Santos et al. (2013) extend the works of Santos et al. (2011b) and Santos et al. (2011a) by allowing the pickup trucks to serve the customers without stopping at the cross-dock.
P	Agustina et al. (2014) developed an integrated model for a food retailer company that is responsible for handling the internal operations (inventory) and outgoing shipments.
O	The multi-commodity flow based formulation for the DAVRP can be seen as an adaptation to the formulation introduced by Garvin et al. (1957) for vehicle routing problems in the context of oil delivery.
P	The above formulation of the capacity constrained elementary shortest path problem is inspired from the two-index vehicle-flow formulation introduced by Jepsen et al. (2008).
O	Baldacci et al. (2011) present a relaxation of the ESPPRC called the ng-route relaxation (ngSPPRC).
O	Pecin et al. (2013) present an efficient implementation of the ng-SPPRC based on dynamic programming.
O	The focus of pattern classification is to recognize similarities in the data, categorizing them in different subsets (Cristianini and Shawe-Taylor, 2000; Carrizosa and Morales, 2013; Vapnik, 1995).
P	In many fields, such as the financial and the medical ones (Guyon et al., 2002; Lee and Tsung-Lin, 2009), classification of data (samples in Machine Learning language) is useful for analysis or diagnosis purposes
N	Hence it is necessary to screen off the relevant features from those which are irrelevant (Bi et al., 2003).
P	The process that selects the features entering the subset of the relevant ones is known as Feature Selection (FS) and the related literature is extremely rich (see e.g. Guyon and Elisseeff (2003); Guyon et al. (2006); Kittler (1986); Meyer et al. (2008)).
O	As far as mathematical programming-based approaches are concerned, we cite here Weston et al. (2000) and, more specifically, Bradley et al. (1998), where (FS) is pursued by formulating a mathematical program with a parametric objective function embedding a concave approximation of the zero-norm of the feature vector.
P	Other approaches based on zero-norm minimization are in Weston et al. (2003) and Rinaldi and Sciandrone (2010).
P	In a recent paper (Aytug, 2015) a model based on penalization of the number of features entering into the classification process is treated by means of generalized Benders decomposition.
O	The objective of this paper is to treat explicitly the (FS) problem as a Mixed Binary Programming (MBP) one (Bertolazzi et al., 2016; Maldonado et al., 2014), in the framework of the SVM (Support Vector Machine) (Chen and Lin, 2006; Cristianini and Shawe-Taylor, 2000; Nguyen and de la Torre, 2010; Vapnik, 1995) approach.
O	The main novelty of our approach relies in the application of the Lagrangian Relaxation approach to our model, which is closely related to the one presented in Maldonado et al. (2014), and in the use of the method described in Frangioni (2002) which belongs to the class of the bundle ones (Astorino et al., 2013, 2011; Hiriart-Urruty and Lemaréchal, 1993). 
O	It implements an ascent procedure for solving the related Lagrangian Dual problem, which is, of course, a nonsmooth one (Fortz et al., 2017; Frangioni and Gorgone, 2014; Frangioni et al., 2017; Gaudioso et al., 2009).
O	In Section 3 we describe the method and in Section 4 we discuss the numerical results obtained on some benchmark datasets from cancer classification (Meyer et al., 2008) as well as on some datasets widely used in classification literature.
O	Note that the rationale behind minimizing both the L1 and L0 norm of w is to achieve, respectively, large separation margin and selection of the relevant features (see also Liu and Wu (2007) for an alternative computational approach).
O	The model we propose may be considered as a variant of the one presented in Maldonado et al. (2014), as in our approach the limitation in the number of features is pursued by means of an appropriate setting of the objective function, while in Maldonado et al. (2014) budget-type constraints are in action.
O	In particular we adopt the (generalized) bundle method (GBM) introduced in Frangioni (2002), which is an iterative ascent method.
P	We have implemented the proposed approach within a generalpurpose C++ bundle code developed by Frangioni (2002) which, in turn, embeds a specialized quadratic solver described in Frangioni (1996).
P	The software has been already used with success in other applications such as Frangioni and Gorgone (2014) and Fortz et al. (2017).
O	Before testing our approach, we have performed the so called model selection phase, see Bi et al. (2003), whose objective, in the SVM framework, is the tuning of the parameter C.
O	Detailed descriptions are in Meyer et al. (2008) and Chang and Lin (2011).
O	which is exactly the optimization problem described in Bradley and Mangasarian (1998), where the SVM approach embedding into the objective function the L1 norm of w is adopted.
P	A motivation for selecting the above model is that it has been shown in Bradley and Mangasarian (1998) that it is also a valid tool for feature selection purposes.
O	We remark (see Mangasarian (1997) for a detailed discussion on the use of different norms) that minimization of the L1 norm of w corresponds to maximize the separation margin, measured by means of the L∞ norm.
O	For comparison purposes we focus on the papers Bradley et al. (1998) and Maldonado et al. (2014).
O	In Bradley et al. (1998) the results of four different feature selection algorithms are presented.
O	In Maldonado et al. (2014) the maximum testing correctness of 88.9 is achieved with 18 non selected features, corresponding to the 53% of the total.
O	In Maldonado et al. (2014) the best result obtained in terms of testing correctness is 78.0 with all features selected.
P	Previous research Crispin and Syrichas (2013) demonstrated the effectiveness of Quantum Annealing (QA) for solving many instances of the Capacitated Vehicle Routing Problem (CVRP).
O	This term is extremely sensitive to variations in either parameter, and tuning is further complicated because the Metropolis criteria (Metropolis et al., 1953) is simultaneously dependent upon temperature.
O	A Design of Experiments (DoE) method (Ridge and Kudenko, 2010) can be helpful in uncovering major dependencies between such variables, but a course of factorial experiments can be time-consuming, and predicting the ranges for numerous and sensitive variables is difficult without once again resorting to guesswork or serially hand-tuning.
P	Late Acceptance Hill Climbing (Burke and Bykov, 2017) has a single parameter controlling the size of a fitness array which acts as a ‘memory’ of good solutions.
O	Cuckoo Search is reported (Nie et al., 2014) to be superior to Genetic Algorithms in part because of having only two parameters - nest abandonment rate and population size.
O	In QA, it has been shown (Titiloye and Crispin, 2011) that the number of parameters can be reduced by one, by setting the magnetic field value to be constant.
O	Quantum Annealing is an energy-based metaheuristic which uses the Path-Integral Monte Carlo (PIMC) method (Battaglia et al., 2005) to approximate the ground state of the Ising Model.
O	Fig. 2 shows the QACVRP implementation used previously by Crispin and Syrichas (2013).
O	As shown in Titiloye and Crispin (2011), Γ was held constant during PT tuning whilst suitable values for temperature T and P were determined by experimentation. 
O	The reference instance was selected because it has the approximate median number of customers within the chosen benchmarks of Crispin and Syrichas (2013), which range from 50 to 262 customers.
O	With the parameters uncoupled from one another in FJ-QACVRP (Fixed J Quantum Annealing for Capacitated Vehicle Routing Problems), a systematic means of establishing T for the first annealing phase would be of benefit when tackling groups of problem instances, such as the benchmarks of Augerat et al. (1995) as attempted by QACVRP under the PT Tuning scheme.
O	To find this factor, the fitness landscape Watson (2010) of the problem needs to be considered.
O	The energy landscape which is explored by a metaheuristic while it solves a problem can be visualized in the form of a scatterplot or Fitness Cloud (Collard et al., 2004).
O	The geometric features of the plot give a visual indication of the run-time behaviour of the algorithm, and therefore may be regarded as a dynamic cost model (Watson, 2010).
O	They were further analysed to produce metrics for characterising (Vanneschi et al., 2006) and measuring the difficulty of optimization problems (Vanneschi et al., 2006).
O	A good value for J was discovered in the course of determining the parameters of the reference instance, P-n101-k4 in Crispin and Syrichas (2013).
O	QACVRP then solved selected instances 20 times each with Cre f = {3, 40, 22.5 × 10−3} determined in Crispin and Syrichas (2013), and with Monte Carlo steps MC = 10 × 106.
O	Table 5 show the results of FJ-QACVRP on the CVRP instances in the benchmarks of Taillard (1993).
O	Table 6 shows the results of FJ-QACVRP on the moderately sized CVRP instances in the benchmarks of Christofides and Eilon (1969), Gillett and Johnson (1976), and Christofides et al. (1981).
O	Tables 7 and 8 show the results of FJ-QACVRP on the benchmark of Golden et al. (1998), which include larger sized CVRP and DCVRP instances respectively.
O	Table 9 show the results of FJ-QACVRP on the benchmark of Li et al. which is comprised of procedurally-generated (Li et al., 2005) very large-scale DCVRP instances.
O	For a valid comparison with QACVRP, the chosen benchmarks were those as used in Crispin and Syrichas (2013) - sets B and P benchmarks of Augerat et al. (1995). 
O	Magnanti and Wong (1984) describe a unifying framework for deterministic single and multicommodity NDPs, and specify the design of the framework for facility location and traffic network design problems.
P	Specific NDPs are also studied in depth for different practical contexts, e.g., telecommunications and computer network design in Pióro and Medhi (2004) and Babonneau et al. (2013). Poss (2011) provides a detailed summary of models and algorithms for NDPs with diverse network and problem settings.
P	The literature of stochastic NDPs typically assumes fully known distributional information on the demand uncertainty, allowing to solve the problem by using largescale stochastic programs with a finite, but large number of realizations (i.e., “scenarios”) (see, e.g., Santoso et al., 2005; Ukkusuri and Patil, 2009).
O	The scenarios can be generated either from certain assumed distributions (e.g., using the Sample Average Approximation (SAA) method (see Kleywegt et al., 2002)) or based on statistical information (e.g., using a moment-matching method (Høyland et al., 2003)).
O	In cases where the demand distribution is not fully known, another avenue for optimizing NDPs is through robust optimization (Ben-Tal and Nemirovski, 1998; Bertsimas and Sim, 2003,2004), to optimize the worst-case objective value for any uncertain parameter realization over a pre-defined uncertainty set.
P	Atamtürk and Zhang (2007) propose a two-stage robust optimization approach for solving network flow and design problems with uncertain demand.
O	They also apply a budget uncertainty set (see Bertsimas and Sim (2004)) under which they provide an upper bound on the probability of a robust solution being infeasible for any random demand vector.
P	Altin et al. (2011), and Koster et al. (2013) describe a variety of formulations, complexities, valid inequalities, and computational results for robust NDP with uncertain (hose) demand.
O	Álvarez-Miranda et al. (2014) discuss the complexity and propose heuristic algorithms for robust NDPs; Cacchiani et al. (2016) optimize single-commodity robust NDPs by using the branch-and-cut algorithm based on efficient separation procedures.
O	Robust NDPs are often reformulated as two-stage mixed-integer linear programs, and can be optimized through decomposition, cutting-plane, and/or column-generation methods (see, e.g., Ayoub and Poss, 2016; Lee et al., 2013; Shen et al., 2016; Atamtürk and Zhang, 2007).
O	Furthermore, in (Ben-Ameur and Kerivin, 2005; Mattia, 2013; Poss and Raack, 2013; Scutellà, 2009), the authors investigate robust NDPs with dynamic routing decisions made in multiple stages when sequentially realizing the demand, rather than using static network flows in the two-stage setting.
P	Bertsimas et al. (2010); Delage and Ye (2010); Jiang and Guan (2016) propose different moment-based ambiguity sets for deriving tractable reformulations of DR stochastic or chance-constrained programs.
O	The design of a box-shaped support is analogous to the use of a box-shaped uncertainty set in robust optimization (see Soyster, 1973).
O	The ambiguity set can be constructed through other means, such as using nested confidence sets constructed with the decision maker’s prior information (Wiesemann et al., 2014), using φ-divergences from the observed density (Ben-Tal et al., 2013), or using an assumption of radial distributions on the data (Calafiore and El Ghaoui, 2006).
O	Since ρin is binary and (14d) gives us the bounds 0 ≤ πi ≤ Gi, we can subsequently reformulate the bilinear terms ρinπi using the McCormick inequalities (see McCormick, 1976).
N	We employ the SAA method (Kleywegt et al., 2002) to formulate the stochastic NDP model, for which we generate i.i.d. demand samples from a pre-assumed distribution.
O	The L-shaped method (or equivalently the Benders decomposition approach) (vanSlyke and Wets, 1969) is often employed to derive cuts and iteratively optimize (31), which could be large scale depending on the size of the sample set  (see, e.g., Botton et al., 2013; Fortz and Poss, 2009). 
P	We let ui ≡ qiπi and use the McCormick inequalities (McCormick, 1976) to obtain an equivalent mixed-integer linear programming formulation for the inner problem:
N	We describe another benchmark approach using the two-stage robust NDP model introduced by Atamtürk and Zhang (2007), which aims to design a network to minimize the worst-case (i.e., maximum) second-stage cost of network flows and demand shortage over all possible demand.
O	We optimize the robust NDP model (33) using the Benders decomposition method (see Birge and Louveaux, 2011; vanSlyke and Wets, 1969), for which at iteration M, we solve a relaxed master problem
P	Using the techniques in Bertsimas and Sim (2004), we define binary variables qi for each i ∈ D, such that qi = 1 if the realization of demand di deviates from its mean value, and qi = 0 otherwise.
O	We mainly test the NOBEL-US network from the SNDlib (Orlowski et al., 2010), which is a library of test instances for Survivable fixed telecommunication Network Design1.
O	As one aspect of emergency route planning, evacuation can be defined generally as the removal of people and materials from buildings and out of an affected area by assigning one from among a limited set of routes and gateways to various evacuees in real-time, while maximizing some measure of success for the entire evacuation mission (Saeed Osman & Ram, 2013).
O	Although the basic requirements for disaster response and evacuation are essentially the same in all types of incident scenarios, specific disasters have unique characteristics such as amount of warning available (Hamza-Lup, Hua, and Aved, 2006).
O	Under precautionary evacuation, the estimation of the evacuation time compared to the hazard propagation time and the estimation of the risk can be done a priori (Saeed Osman & Ram, 2013).
O	Currently available approaches for addressing evacuation are based on proactive planning (Sheremetov et al. 2004, Saadatseresht et al. 2008).
O	Certain evacuation planning models employ mathematical programming: Andreas (2006) solved evacuation routing problem using LP and nonlinear programming.
O	Chiu et al. (2007) formulated the traffic assignment and evacuation departure schedule decisions as a single-destination celltransmission-model-based linear programming model. 
P	Chiu and Zheng (2007) presented a dynamic traffic assignment modeling technique based on the LP formulation of the cell transmission model to solve for the optimal decisions for traffic assignment, and departure schedule for multi-priority groups in response to a nonotice disaster response.
O	Yi-Chang et al. (2007) proposed a network transformation and traffic assignment modeling technique to solve the joint evacuation destination route flow departure problem and formulated the illustration of modeling procedure and solution properties in LP.
O	Stepanov and Smith (2008) suggested an integer programming formulation for the optimal assignment and analysis of evacuation routes in transportation networks.
O	Yuan and Wang (2009) studied the transportation problem in disaster area and proposed a multiobjective path selection model for emergency evacuation.
O	Saeed Osman & Ram (2011) proposed a discrete optimization model for evacuation scheduling; this approach models the capacity-constrained evacuation scheduling problem over discrete time as an integer optimization model.
O	Goerigk et. al. (2013) described multiple approaches for finding both lower and upper bounds for the bus evacuation problem, and applying them in a branch and bound framework.
P	Verma and Gaukler (2015) provided two models that consider the impact a disaster on the disaster response facilities and the population centers.
P	Liberatore et. al. (2014) proposed a model for solving the problem of planning for recovery of damaged elements of the distribution network such as bridges and roads, so that the consequent distribution planning would benefit the most.
P	Most of the algorithms developed in the prior researches for addressing the emergency evacuation problem have improved on the linear programming method (Kim & Shekhar 2005, Lu et al. 2005, and Shekhar & Kim 2006), since LP was identified inefficient and lacked scalability (Hamacher & Tjandra, 2002).
O	Capacity Constrained Route Planning (CCRP) is a well-known heuristic approach that makes use of well-known shortest path algorithms and extends them by incorporating capacity constraints (Lu et al., 2005).
P	Kim et al. (2007) presented two scalable heuristics for the calculation of evacuation route planning, intelligent load reduction (ILR) and incremental data structure (IDS).
P	The two scalable heuristics are focused on improving the performance of the existing Capacity Constrained Route Planner (CCRP) heuristic. Despite the good results, they cannot produce evacuation route and schedule compare to CCRP (Yusoff et al., 2008).
P	Gupta & Yadav (2004) proposed an algorithm based on the network optimization theory that uses graph theoretical approach to identify the number of paths available for movement of the people.
P	Özdamar & Yi (2008) developed a fast constructive heuristic based on a greedy l-neighborhood search technique for identifying a feasible, acceptable solution for disaster relief and evacuation logistics.
P	Zong et. al. (2014) presented an evacuation model based on minimizing evacuation time, temporal–spatial conflict, and congestion.
P	Abounacer et. al. (2014) proposed an epsilonconstraint method and proved that it generates the exact Pareto front of a complex three-objective location– transportation problem.
P	Rath & Gutjahr (2014) developed a math-heuristic for a three-objective warehouse location–routing problem in disaster relief.
P	Saeed Osman and Ram (2013) generated real world datasets from a combined path network.
P	The complete process of creating a combined building and road path network data is presented in Saeed Osman and Ram (2012 and 2013).
O	The case problems involved in this research use real world network datasets of nodes, gateways, edges reported by Saeed Osman and Ram (2013).
O	Scheduling includes planning and giving priority to activities, which need to be done based on an operation sequence (Sule, 1997).
O	Parallel machines is related to how to schedule the group of jobs on a number of machines in order to ensure jobs processing within reasonable time and includes jobs assignment to machines with the similar capabilities so that each job is processed by one of the machines (Logendran, et al. 2007).
O	solving such an optimization problem can determine which machines are needed and which jobs according to what sequence are processed on each machine (Sule, 1997).
P	Chang, et al. (2004) offered a simulated annealing (SA) algorithm and compared its results to a commercial solver.
O	Shao, et al. (2008) considered a solution approach based on neural networks assuming zero available times.
O	Chung, et al. (2009) presented a mathematical model and three heuristic methods to minimizes the total completion time under non-zero available time.
O	Minimizing the maximum jobs completion time has been studied by Kashan, et al. (2008), Damodaran and Velez-Gallego (2010), Wang and Chou (2010) and Damodaran, et al. (2011).
P	Damodaran, et al. (2009) proposed a GA in order to minimize the completion time of BPMs and compared their results to the SA algorithm (Chang, et al. 2004), random key genetic algorithm (RKGA) (Xu and Bean, 2007), and CPLEX commercial solver.
P	Cheng, et al. (2013) proposed Multi-objective ant colony optimization (MOACO), that is the main criterion used for ant’s routes selection to dominant undeveloped convergences. In the following, Chen, et al. (2010), Xu, et al. (2013), Cakici, et al. (2013), Bilyk, et al. (2014), Liu, et al. (2014) and Jia and Leung (2015) have considered different mathematical models with release time for jobs or dynamic job arrivals, which minimize makespan and/or total tardiness in identical parallel BPMs by using exact and/or several heuristics.
O	Then the results are compared with the prevalent meta-heuristic methods (e.g., ACO, GRASP and VNS). Also, Abedi, et al. (2015) expressed a bi-objective model to minimize makespan and total weighted tardiness and earliness with release time for jobs and capacity limits in compatible job families.
P	In non-identical parallel BPMs to minimize the makespan, Damodaran, et al. (2011) and Damodaran, et al. (2012) presented a particle swarm optimization (PSO) algorithm.
P	Xu and Bean (2007) considered an integer programming model that minimizes the total completion time and used the GA to solve this model. 
P	Suhaimi and Damodaran (2014) proposed a Lagrangian relaxation method, and then compared with PSO and RKGA in the literature.
P	Hulett and Damodaran (2015) presented a mathematical model to minimize total weighted tardiness of jobs and solved by the PSO algorithm and finally compared with the differential evolution algorithm.
P	Jia, et al. (2015) presented many heuristic methods to minimize makespan.
O	on an unbounded BPMs, Feng, et al. (2013) studied two groups scheduling, namely A and B, with different objective functions.
P	In unrelated parallel BPMs, Li, et al. (2013) and Arroyo and Leung (2017) proposed several heuristics that minimize the makespan.
O	Also, Shahvari and Logendran (2015) and (2017) considered incompatible job families and several assumptions to minimize the total weighted tardiness and total weighted completion time in s-batching machines.
P	In uniform parallel BPMs, for minimizing the makespan, Zhou, et al. (2016) considered different assumptions, proposed a discrete differential evolution (DE) algorithm, and then compared the results with PSO and RKGA. Jiang, et al. (2016) considered batch transportation, then proposed hybrid algorithm DPSO-GA.
O	Batches processing times can be constant or variable (Chang, et al. 2004).
O	Because of the importance of these cost reductions on improving the performance of companies, a multiobjective mixed-integer non-linear programming model considering ready times according to the models proposed by Xu, et al. (2013) and Li, et al. (2013) is presented.
O	Pareto solutions in a multi-objective problem are a set of non-dominated points that dominate all other solutions, and often known as Pareto front (Deb, et al. 2002).
P	HS algorithm, like GA, is a kind of improving and evolutionary algorithm based on the logic of musicians to find the best harmony and compose the most beautiful music, harmony search (HS) method presented by Geem, et al. (2001).
O	In this study, the non-dominated ranking and sorting approach suggested by Deb, et al. (2002) is used for finding a Pareto-optimal methodology.
O	Various studies in this area have used this method to optimize their decision variables, such as Damodaran, et al. (2012), Suhaimi and Damodaran (2014) and Hulett and Damodaran (2015). 
O	The MOPSO algorithm used in this paper is the same as Coello Coello, et al. (2004).
P	This, Deb, et al. (2002) introduced a modified method, namely NSGA-II, which is one of the most efficient and strongest available algorithms for solving multi-objective optimization problems and obtains Pareto-optimal points by a non-dominance rule (not dominated). 
O	This new version uses the crowded distance calculation, which has proper extension in the changes area of the objective functions and gives the freedom of a selection of its considered design among optimized designs to designer (Deb, et al. 2002). 
P	The ACO algorithm firstly introduced by Dorigo, et al. (1991) and Dorigo (1992). 
P	It is a very common algorithm used in several studies in various fields, including parallel BPMs models (Cheng, et al., 2013; Xu, et al., 2013; and Chen, et al., 2010).
O	It is used for solving the proposed model and is similar to the approach proposed by L´opez-Ib´a˜nez (2004) and Cheng, et al. (2013).
O	Similarly for representing the second vector to the ACO approach, it is assumed that each node represents a job and each path represents assignment of the jobs to the machines as same as the approach used by Cheng, et al. (2013).
O	Implementing examinations according to optimum factors (Cheng and Chang, 2007).
O	Therefore, we match this array with our problem by adjustment techniques (Lee, et al. 1996).
O	Sample problems are created according to sample problems in Damodaran, et al. (2009).
O	The production environment is studied in the mentioned paper (Tavakkoli-Moghaddam, et al. 2007), is flow shop; Therefore, after changing this equation and modifying it, it is matched with a batch parallel machines production environment.
O	In this section, in order to compare the results achieved by algorithms, some comparison metrics of meta-heuristic algorithms are introduced by Tavakkoli-Moghaddam, et al. (2007). 
O	This criterion presented by Scott (1995). 
P	At last, two different types of test problems considering various aspects of jobs and machines based on Damodaran, et al. (2009) have been considered and solved by four foregoing proposed algorithms.
O	Applications of assembly scheduling in industry and services have been reported in several works: Potts et al. (1995) describe the case of personal computer manufacturing where the different components of the computer are produced in the first stage to be later assembled in a second stage (a packaging station).
O	Lee et al. (1993) describe the case of a fire engine assembly plant. In this case, the body and chassis of fire engines are produced in parallel, and assembled in a second stage.
P	Finally, another application is presented by Al-Anzi and Allahverdi (2006a); 2006b) and Allahverdi and Al-Anzi (2006) in the area of distributed database systems.
P	This problem has been first tackled by Lee et al. (1993), and its NP-hardness in the strong sense (even when the first stage is composed of 2 machines in parallel) has been established by Potts et al. (1995).
P	A number of efficient heuristics for the problem have been proposed by Sun et al. (2003).
P	Regarding exact solutions, the best approach is the Branch & Bound algorithm proposed by Hariri and Potts (1997), which in many cases is able to schedule up to 8000 jobs in less than 100 s.
O	Other objective considered in the literature is the maximum lateness (Al-Anzi and Allahverdi, 2006b; Allahverdi and AlAnzi, 2006).
O	Finally, additional constraints such as setup times (AlAnzi and Allahverdi, 2007), more than one machine in the second stage (Al-Anzi and Allahverdi, 2013; Sung and Kim, 2008), or additional stages for the transportation of components (Koulamas and Kyparisis, 2001; Shoaardebili and Fattahi, 2015) have been also addressed.
N	As mentioned above, our paper is devoted to the 2-stage assembly scheduling problem with the minimisation of total completion time as objective, which can be denoted as Am||jCj according to the notation in Potts et al. (1995).
O	Minimisation of the total completion time is an important scheduling objective since completion time can be viewed as a surrogate for the cycle time of the jobs, which in turns influences the inventory levels and the lead times that can be quoted by a company (Framinan et al., 2014).
O	Another connection is with the customer order scheduling problem with total completion time as objective (see e.g. Framinan and Perez-Gonzalez, 2017; Leung et al., 2005), denoted as PDm||jCj.
N	The first reference for total completion time minimisation is Tozkapan et al. (2003), where the authors address the problem (weighted minimisation) for the first time.
N	Al-Anzi and Allahverdi (2006a) also address this problem, stating some conditions that the processing times of an instance must fulfil to be optimally solved, and proposing both constructive heuristics and metaheuristics for the problem.
O	This problem has been addressed for the total completion time objective first by Sung and Kim (2008) when there are two assembly machines, and later generalised for m ≥ 2 assembly machines by Al-Anzi and Allahverdi (2012).
P	The most efficient approximate method for the problem is the Artificial Immune Intelligence (AIS) metaheuristic by Al-Anzi and Allahverdi (2013), as these authors conduct an exhaustive computational experience showing that AIS outperforms the rest of existing approximate methods.
O	To the best of our knowledge, this problem has been addressed only by Xiong et al. (2014), also considering setup times.
P	These authors propose the so-called ESPT constructive heuristic that could be potentially interesting for our problem and indeed, when there is only one factory and no setups are considered, it is equivalent to one of the already mentioned constructive heuristics by Al-Anzi and Allahverdi (2006a).
P	Finally, Al-Anzi and Allahverdi (2006a) study a number of theoretical properties for the problem under consideration.
O	As mentioned in Section 1, a number of properties for the problem have been studied by Al-Anzi and Allahverdi (2006a).
O	Theorem 1 establishes the conditions for the first stage to be dominant in a similar way to Theorem 1 in Al-Anzi and Allahverdi (2006a).
O	Corollary 3 expresses the specific conditions for the secondstage SPT rule to be optimal for the assembly problem, therefore it would correspond – albeit with different conditions due to a flaw in the proof – to Theorem 2 in Al-Anzi and Allahverdi (2006a).
O	First, to design a testbed where the instances do not belong to the extreme cases, as there are specific algorithms available for these problems: If the first stage is dominant, there are several efficient algorithms for the problem, most notably the algorithm by Framinan and Perez-Gonzalez (2017).
P	The first constructive heuristics for the problem have been proposed by Tozkapan et al. (2003).
P	The A1 and A2 algorithms are compared with TCK1 and TCK2 by Al-Anzi and Allahverdi (2006a), resulting that the best performance is obtained by TCK2 and A1.
P	This strategy has been successfully employed for other related problems, such as the permutation flowshop with total completion time objective (see Fernandez-Viagas and Framinan, 2015), or the customer order scheduling problem (see Leung et al., 2005).
O	Al-Anzi and Allahverdi (2006a) propose three constructive heuristics for the problem: heuristic S1 is obtained by sorting the jobs in non descending order of pj.
P	The S1–S3 heuristics have not been compared with other heuristics, although they are used by Al-Anzi and Allahverdi (2006a) as starting seeds for two metaheuristics for the problem.
P	Finally, Al-Anzi and Allahverdi (2006a) propose two algorithms for the problem, labelled A1 and A2.
P	As our intention is to provide an estimate of the completion times of this artificial job, we suggest sorting the unscheduled jobs according to Algorithm S2, which is the sorting algorithm providing the best results according to the computational experience by Al-Anzi and Allahverdi (2006a).
O	There are several metaheuristics available for the problem, all by Al-Anzi and Allahverdi (2006a): a simulated annealing algorithm, a Tabu Search algorithm, and a Hybrid Tabu Search algorithm (HTS in the following).
N	Although the performance of HTS is excellent (according to AlAnzi and Allahverdi, 2006a it provides an average 0.15% deviation from the best known solutions in an extensive testbed), we believe that there is room for improvement by allowing a higher diversification in the exploration of solutions.
O	To change the type of neighbourhood if the algorithm is stuck in a local optima, borrowing ideas from Variable Neighbourhood Search or VNS (Mladenovic´ and Hansen, 1997).
P	To do so, we generate a testbed using the parameters given by Allahverdi and Al-Anzi (2009).
N	Second, we compare the best-so-far metaheuristic for the problem (the Hybrid Tabu Search by Allahverdi and Al-Anzi, 2009) with the Variable Local Search proposed in Section 4.
O	In addition, we also compare the best-so-far metaheuristic for the multi-machine assembly problem (the Artificial Immune System by Al-Anzi and Allahverdi, 2013, see Section 1).
O	TCK1 and TCK2 heuristics by Tozkapan et al. (2003).
O	S1–S3 and A1, A2 heuristics by Al-Anzi and Allahverdi (2006a).
P	Regarding the rest of heuristics, the experiment confirms the results already obtained by Al-Anzi and Allahverdi (2006a), although the statistical analysis allows to assert that TCK2 is better than A1.
O	In addition, we include the bestso-far metaheuristic available for the multi-machine assembly case, i.e. the AIS algorithm by Al-Anzi and Allahverdi (2013).
N	We first analyse some problem properties and correctly formulate some flaws found in the work by Al-Anzi and Allahverdi (2006a).
O	The Ring Spur Assignment Problem introduced in Carroll et al. (2011) arises in the design of nextgeneration telecommunications networks and can be considered as a location-allocation problem.
O	Such a resilient network is indeed a logical topology for an existing physical infrastructure –that is, we exploit the pre-installed capacity in the physical synchronous digital hierarchy (SDH) (Carroll et al., 2013).
P	Figure 1b represents an optimal solution to the instance ’France’ in Figure 1a from SNDlib (Orlowski et al., 2007).
P	In Carroll et al. (2011), the authors proposed two formulations for this problem.
P	Carroll et al. (2013) relying on the formulation F1 in Carroll et al. (2011), identified several classes of valid inequalities and proposed the corresponding separation routines.
P	Carroll and McGarraghy (2013) decomposed the problem into two integer programming (IP) problems and described a branch-and-cut decomposition heuristic algorithm.
O	From the network topology perspective, the problem has some similarity with some variants of hub location problems such as (Rodríguez-Martín et al., 2014), the plant-cycle location problem (Labbé et al., 2004), the team-orienteering problem (Fischetti et al., 1998b) and the multiple depots ring star problem (Sundar and Rathinam, 2014).
P	In earlier works, e.g. Carroll et al. (2013), the authors employed a ring representative node approach to model the local rings and derive the ring bound constraints, resulting in larger initial models.
N	It must be noted that b is an instance-dependent penalty parameter calculated independently for every problem instance in such a way that spur arcs are discouraged (see Carroll et al. (2013) and Carroll et al. (2011)).
O	let S a and S b be two disjoint components of the tertiary-level network (Carroll et al., 2013):
O	Carroll et al. (2013) defines the ghost ring as follows.
O	Our aim here is to use the feasibility pump heuristic (Fischetti et al., 2005) to find a feasible solution during B&B and to propose a way to polish (if possible) and improve such solutions on the basis of the information from the problem instance.
N	Should this be true, the improvement can be significant, as the values of the spur arc penalty in Carroll et al. (2013) for the SNDlib problem instances is b, 3 ≤ b ≤ 17, times more expensive than the cost of normal edges.
P	The following inequalities proposed in Rodríguez-Martín et al. (2014) are valid for P(RS AP2):
P	See the proof in Rodríguez-Martín et al. (2014).
P	We then use the max-flow algorithm of Edmonds and Karp (1972) between every pair of nodes, say i and j.
O	We then use the max-flow algorithm of Edmonds and Karp (1972) between every pair of nodes, say i and j and identify S and V † /S .
O	We then enumerate all the cycles in such a graph using Tiernan’s method (see Tiernan (1970)) and examine every cycle to identify the violated cuts.
N	While Fischetti et al. (1998a) and other researchers proposed several heuristic separation algorithms for such constraints, our initial computational experiments confirm that for this problem such techniques are not sufficiently successful in identifying the 2-matching structures (in particular in the tertiary level subgraph).
P	Letchford and Lodi (2002) proposed a polynomial-time algorithm for separating such inequalities. 
O	Therefore, we opt to find combs based on block decomposition as used for the classical TSP implemented in Concorde (Applegate et al., 2007) (it can be downloaded at Cook’s).
O	The tolerance  is set to 1e-6 and the max-flow problems are solved using the Boost implementation of Edmonds and Karp (1972) algorithm for directed graphs (with the undirected graphs being converted to directed ones in an appropriate way).
O	In order to carry out our computational experiments, we use the instances from SNDlib (exactly those instances used in Carroll et al. (2013)).
N	The parameter b is calculated in the same manner as in Carroll et al. (2013) and is such that establishing spur arcs is not encouraged.
N	It is not trivial to perform a fair comparison between this model and the model in Carroll et al. (2013), in terms of computational behaviour.
P	However, our primary computational experiments show that when both codes run on the same hardware, this combination of model and the branch-and-cut approach we have devised is, in general, superior to the one in Carroll et al. (2013) with respect to both the computational time and the number of instances that are solved to optimality.
O	In Carroll et al. (2013), within a time limit of 3 hours: for ’giul39’, the authors report a feasible solution with an integrality gap of 10.65 per cent; for ’pioro40’, a solution with a gap of 23.44 percent; for ’Germany’, one with a gap of 0.51 per cent, while the process run into memory issues after around 8,000 seconds, and for ’ta2’ a solution with a gap of ’2.46’.
P	In fact, we can say that the feasible solutions for ’ta2’ and ’Germany’ reported in Carroll et al. (2013) are indeed optimal solutions.
O	In this paper, we have proposed an integer programming model for the Ring Spur Assignment Problem (RSAP) that was introduced in Carroll et al. (2013).
P	The (discrete) 1-center problem (Agarwal et al., 1998), a discrete facility location problem, and the Weber problem, (Drezner, 1992; Drezner and Hamacher, 2002), a continuous facility location problem, are among the most famous SFLPs.
O	A standard approach for solving the Weber problem is the Weiszfeld method (Weiszfeld and Plastria, 2009) which is a simple closed form iterative formula.
P	Several studies investigate the convergence of the Weiszfeld method, see e.g., Chandrasekaran and Tamir (1989), Katz (1974), Kuhn (1973), and a number of modifications have been developed, see e.g., Vardi and Zhang (2001), since its first introduction by Weiszfeld in 1937.
O	Distribution and transportation related activities are growing all over the world, especially in urban areas, due to a continuous increase in the demand of goods and services (The Population Division of the Department of Economic and Social Affairs of the United Nations, 2016).
P	Thus, the importance of green freight transportation in city logistics has been growing to reduce the harmfuleffects of CO2 emission (Demir et al., 2014b).
P	For instance, in the context of the traveling salesman problem, the authors in Urquhart et al. (2010b) use an evolution ary algorithm to discover tours with low CO2 emission.
O	For the vehicle routing problem with time windows (VRPTW), the work in Urquhart et al. (2010a) investigates, using evolutionary algorithms, the trade-off between the number of vehicles used, total CO2 emission, and the total distance traveled.
P	Figliozzi in Figliozzi (2011) considers a time-dependent vehicle routing problem which takes the effects of congestion and speeds of the vehicles on the emission amounts into account.
P	Bektas¸ and Laporte (2011) introduce an extension of the VRPTW named as the pollution routing problem (PRP) which minimizes fuel emission and driver costs.
O	Their solution approach is based on the discretization of the speeds on roads while ignoring a part of the fuel consumption formula in the comprehensive modal emission model (CMEM) (Barth et al., 2005).
P	The authors in Demir et al. (2012) develop an adaptive large neighborhood search algorithm (ALNS) in order to solve the PRP.
O	The bi-objective PRP is studied in Demir et al. (2014a) where the cost of CO2 emission and the drivers’ wages are considered as two conflicting objectives and ALNS is used as the solution procedure along with a speed optimization algorithm.
P	A matheuristic approach for the PRP is proposed in Kramer et al. (2015).
P	We assume that the amount of CO2 emitted by a vehicle is proportional to its fuel consumption which is aligned with the related literature, see e.g., Demir et al. (2011).
P	For a review and comparison of different vehicle emission models, the reader is referred to Demir et al. (2011).
P	As the fuel consumption model, we use the comprehensive modal emission model (CMEM), suggested in Barth et al. (2005), for heavy-good vehicles.
P	The reader is referred to the survey papers (Alizadeh and Goldfarb, 2003; Lobo et al., 1998) for an overview of second order cone programming, algorithms, and application areas.
N	A worst case complexity of the GWP, due to the SOCP formulation, is O(n3.5) (Lobo et al., 1998), where n is the number of customers.
O	According to Kovacs et al. and Pillac et al., the FTSP is an extension of the vehicle routing problem with time windows (VRPTW), which is NP-hard.
O	Tsang and Voudouris and Xu and Chiu were among the first authors to study the problem.
P	Hashimoto et al. applied the Greedy Randomized Adaptive Search Procedure (GRASP) metaheuristic and Cordeau et al. developed constructive heuristics and customized the Adaptive Large Neighborhood Search (ALNS) heuristic to solve this problem.
P	Kovacs et al. studied a similar problem considering tasks with the same priority but with different time windows.
P	In Dohn et al. the number of assigned tasks in a day is maximized subject to restrictions related to the teams and tasks time windows and with a limited number of teams.
O	Li et al. intended to minimize the total number of workers and total displacement in the port of Singapore by allocating different types of workers in teams.
P	In Overholts II et al. the daily maintenance of military equipment in USA is scheduled to maximize the weighted sum of the maintenance tasks performed.
N	In Tsang and Voudouris, the objective was the minimization of the total cost: the cost of displacement of engineers, the cost of overtime, and the cost (or penalty) of tasks not executed.
P	Tang et al. developed a tabu search metaheuristic for a real problem of United Technologies Corporation.
O	Pillac et al. analyzed the similarity between the technician routing and scheduling problem and the vehicle routing problem with time windows.
P	Recently, Cort´es et al. presented a real problem of maintenance of printing machines and digital copiers of a large company in Santiago, Chile.
P	Observe that while Tang et al., Overholts II et al., Cordeau et al., and Hashimoto t al. directly consider the priorities of the tasks, Tsang and Voudouris, Li et al., Dohn et al., Kovacs et al., Pillac et al. , and Cort´es et al. tackle the allowed time windows to perform the tasks.
P	As far as we know, the only research that simultaneously approaches these relevant characteristics of the maintenance services, as well as working hours of technicians, is the one presented by Xu and Chiu.
P	Motivated by the practical relevance of the FTSP, e.g. [1, 2, 3, 5, 6, 7, 11], and the reduced number of studies that consider time windows and priorities of the tasks concurrently, in the present paper we consider the same problem addressed by Xu and Chiu through the development of methods that explore these specific characteristics.
P	Introduced by Holland in 1975, the genetic algorithms (GAs) are inspired by Darwin’s evolution theory and work with populations of solutions that evolve over successive generations.
P	For instance, Gon¸calves et al. applied the RKGA to the job shop problem and achieved the best known results in the literature in 72% of the tested problems, outperforming seven different versions of GA and other metaheuristics, such as Simulated Annealing and GRASP.
P	Comparisons between standard GA, RKGA, and BRKGA for different optimization problems can be found in Gon¸calves and Resende and Gon¸calves et al.; the authors concluded that the BRKGA is more effective than the RKGA.
P	Noronha et al. studied the problem of Routing and Wavelength Assignment and the results obtained by BRKGA outperformed the best heuristics from the literature (the best fit decreasing heuristic and the partition coloring problem heuristic), both in quality and computational cost.
O	According to Mor´an-Mirabal et al., the BRKGA metaheuristic can be described as follows.
O	Table 7 shows the minimum, average, and standard deviation of GAPUB considering the 80 instances of each case for the three proposed constructive heuristics and the heuristic developed by Xu and Chiu, hence forth denoted by XC.
O	Based on Gon¸calves and Resende, the following parameter values were analyzed:
O	Aiming to compare the proposed metaheuristic with other methods from the literature, the performance of the BRKGA-STT when applied to the problem addressed by Kovacs et al. [4] was analyzed.
O	Undheim et al. present different ways to implement standby redundancy in a cloud context. 
P	Moreover, software systems and conceptual frameworks that employ standby redundancy by running passive standby VMs on the infrastructure are proposed by Cully et al. and Distler et al.
P	Ashrafi et al. present optimization models with the goal to optimize the reliability of a software system.
O	More recently, Meedeniya et al. use multi-objective optimization to explore the trade-off between reliability and energy consumption when building redundancy into an embedded system.
O	A recent survey on resource management in clouds is found in Jennings and Stadler.
P	Speitkamp and Bichler present static optimization models for the problem of placing VMs on servers in enterprise data centers.
P	In relation to the B&P algorithm proposed in this paper, the solution approach of Kramer et al. is interesting since it is one of the few approaches in the VM placement literature that is based on column generation.
O	Both Kramer et al. and Breitgand and Epstein end the column generation after solving the linear relaxation of the restricted master problem to optimality.
O	Shi et al. consider a similar static placement problem that includes requirements specifying that a set of VMs should run on different nodes for fault tolerance reasons, or the a node can only run a set of VMs from the same user for security reasons.
O	Furthermore, Goudarzi and Pedram and Ardagna et al. regard a dynamic placement problem of an IaaS provider, which considers placement of multi-tier services with decisions related to the load balancing and bounds on the performance. 
O	However, Bin et al. regard a related placement problem of an IaaS provider where the VMs are allocated a fixed number of backup locations to which they can be migrated in case of a failure.
O	Gullhav et al. present a method that takes the number of active and passive replicas of each component, the assigned resources (translated to service times of the requests) of the replicas and failure probabilities as input, and outputs an approximated response time distribution of the service.
P	This strategy is also used in the literature, e.g., by Gunnerud et al. which find all their integer solutions by solving an IP.
P	We also compare a multiplicative penalty function constraint handling technique of Schl¨unz et al. for multiobjective optimisation to the constrained-domination technique proposed by Deb et al.
O	Improvements on Engrand’s algorithm were suggested by Suppapitnarm et al., with Kellar suggesting further refinements on the improved algorithm.
P	The first application of a state-of-the-art multiobjective metaheuristic to an MICFMO problem instance that we could find, appeared in 2009 in a paper by Hedayat et al. in which the authors applied the NSGA-II.
O	Several years later, in 2014, Schl¨unz et al. applied the MOOCEM to a constrained MICFMO problem instance and compared the optimisation results to historical and heuristic solutions.
O	Shortly thereafter, Schl¨unz et al. also used the MOOCEM within the context of validating a scalarising approach for MICFMO.
P	Along with the NSGA-II algorithm, Deb et al. proposed a constraint handling technique using their so-called constrained-domination principle (CDP).
P	This led Schl¨unz et al. to develop a new constraint handling technique based on a multiplicative penalty function (MPF).
P	The highly-popular NSGA-II was developed by Deb et al. as an improvement over its predecessor.
P	Zitzler et al. developed the SPEA2, also as in improvement over its predecessor.
P	In the second approach, proposed by Hu et al. solutions are explicitly represented as permutations while the velocity-update flight operator is redefined based on the similarity of two solutions.
P	It was found that the permutation-based approach by Hu et al. yielded the most promising results and, as such, we implemented it in our OMOPSO implementation.
P	The P-ACO algorithm was developed by Doerner et al. for solving the multiobjective portfolio selection problem.
P	The AMOSA algorithm, developed by Bandyopadhyay et al. , is one of the latest multiobjective simulated annealers.
O	Schl¨unz et al. first applied the MOOCEM to the MICFMO problem.
O	Recall that Hedayat et al. were the first to apply the NSGA-II to the MICFMO problem.
O	Our implementation of the OMOPSO algorithm also demonstrates the applicability of the permutation-based approach by Hu et al.as an alternative to the random keys approach used in the single-objective particle swarm optimisation ICFMO literature.
O	Derrac et al. discuss two types of analyses that are typically performed in comparative studies, namely single-problem and multi-problem analyses.
O	Chen et al. added the assumption that players have piecewise constant valuation functions, and provided a proportionally fair and envy-free deterministic DSIC mechanism.
O	A randomized DSIC super-fair division is discussed in Mossel and Tamuz and Chen et al.
O	In Cohler et al., the authors provided a tractable, nearly optimal envy-free mechanism when the agents truthfully report their valuations of the cake.
P	To do so, we implement the algorithm proposed in Hoang et al., which uses the concept of return functions. 
O	We wish to mention from the outset that there is a huge difference between this paper and Hoang et al. , namely, the focus here is on selecting or designing a mechanism, whereas in Hoang et al. no such issue is involved as the mechanism is given. 
P	We propose to use the method introduced by Hoang et al. [2010], where the key feature is the introduction of the return function.
P	In Hoang et al. [2010], the authors proved that strategy profiles could be substituted by return-function profiles in order to study best-reply dynamics and Bayesian–Nash equilibria.
P	Based on these remarks, Hoang et al. [2010] designed an algorithm for computing Bayesian–Nash equilibria, where the core idea is to track a sort of fictitious play dynamics in the space of returnfunction profiles to search for a fixed point.
P	We refer to Hoang et al. [2010] for further discussions on this weighting.
O	However, in Hoang et al. [2010], the authors discuss theoretical convergence under some hard-to-verify assumptions and show numerically, in a setting similar to the example considered in Section 3, that such an algorithm does converge to an equilibrium.
O	In Hoang et al. [2010], it was shown numerically (and in some instances, analytically) that the ideal mechanism is not BIC.
O	Also in Hoang et al. [2010], it was experimentally shown that these numbers are sufficient to stabilize the sequence of best-reply return functions.
O	Interestingly, computations by Hoang et al. [2010] show that the Bayesian–Nash equilibrium strategy of the ideal mechanism consists in overbidding undesirable attributes while underbidding desirable ones.
P	The cornerstone of this approach is the computation of Bayesian–Nash equilibria with a fictitious play on return-function profiles, as presented by Hoang et al. [2010].
P	In the previous study of Lessing et al. [2004] and Crawford et al. [2006] for solving the SCP using ACO-based algorithms, an ant builds a solution by choosing one of the unselected columns at each step until all rows are covered.
O	To minimize the number arcs (columns) from which an ant can choose one at each step, Ren et al. [2010] and later Al-Shihabi et al. [2015] used a different construction graph. In this graph, an ant first selects row ^ i to cover, and then column j ∈ J^ i.
P	The instances were introduced by Pessoa et al. [2013], where they have modified a set of SCP instances.
P	We adopt the local search of Ren et al. [2010], which is used to solve the SCP, to improve the solutions found by ants.
O	Pseudo-code for the local search algorithm used in Ren et al. [2010].
P	To benchmark their GRASP-PR algorithm, Pessoa et al. [2013 solved all instances using the default setting of the CPLEX 11 solver.
O	Using CPLEX11, Pessoa et al. [2013] obtained the optimum solutions for a substantial number of instances: all 45 instances of the k = kmin group and 25 instances of the k = kmed and k = kmas groups.
O	A joint replenishment problem (JRP) coordinates the replenishment of multiple items that are jointly ordered and shipped by using a single truck (Goyal 1974; Moon and Cha 2006).
N	As a result, the total cost of the JRP will be less than that of the system, where the items are replenished individually (Khouja and Goyal 2008).
O	Hence, the main objective of the JRP is to minimize the total cost by determining the best family cycle time and replenishing frequency for each item (e.g., Khouja and Goyal 2008; Robinson et al. 2009).
P	For example, a simple procedure to determine the order quantities for a group items was proposed in Shu (1971).
P	An enumeration approach to determine the optimal order quantities of a group of items was proposed in Goyal (1974).
P	Then, a non–iterative method to solve the JRP was proposed in Silver (1975).
P	After that, direct grouping strategy and indirect grouping strategy were compared in van Eijis et al. (1992), and the indirect grouping strategy was recommended based on the percentage of the total cost saving.
O	Moreover, two heuristic procedures to determine the optimal solution for the JRP was presented in Hariga (1994).
P	Lipschitz optimization technique was applied to propose an efficient optimal solution in Wildeman et al. (1997).
P	A heuristic called “RAND” was introduced in Kaspi and Rosenblatt (1983) and then, the method was further improved in Kaspi and Rosenblatt (1991).
P	Performance of the “RAND” and genetic algorithm (GA) to solve the JRP was compared in Khouja et al. (2000).
P	In addition, a modified “RAND” heuristic algorithm called “QD–RAND” was proposed to solve the basic JRP in Cha and Moon (2005).
P	Heuristics methods such as GA and “C-RAND” were used to solve a constrained JRP and performance was compared in Moon and Cha (2006).
P	Though there are many heuristics methods to solve different types of the JRP such as a basic JRP, constrained JRP, and JRP with quantity discount, GA and evolutionary algorithm (EA) are more effective (e.g., Olsen 2008).
O	Moreover, solutions obtained from the GA with local search as well as the “RAND” were compared with the global optimal solution in Hong and Kim (2009), and based on the analysis, it is concluded that the solutions obtained from GA and “RAND” were very close to that of the global optimum.
O	Recently, GA is also used to solve a constrained multi–products economic production models (e.g., C´ardenas-Barr´on et al. 2012; C´ardenas-Barr´on et al. 2014).
O	In the literature, the two-dimensional GA has been used to solve different problems, for example, cutting problem, knapsack problem, and packing problem (e.g., Herbert and Dowsland 1996; Ono and Ikeda 1998; Bortfelldt 2006; Bortfelldt and Winter 2009).
O	Moreover, the same technique was applied in physics to solve different problems, for example, ising problem, entropic method, and photonic crystal slab (e.g., Anderson et al. 1991; Jia et al. 2009).
P	In addition, Olsen (2008) indicated that EA is the most effective and a natural choice to solve a constrained JRP, and hence, we also used an EA to solve the JRP.
O	For example, Goyal (1975) and Moon and Cha (2006) investigated a JRP with a budget constraint.
P	Khouja et al. (2000) studied a JRP with a maximum load constraint.
O	In particular, defective items with economic order quantity model has been investigated in Porteus (1986), Rosenblatt and Lee (1986), Salameh and Jaber (2000), C´ardenas-Barr´on (2000), Goyal and C´ardenas-Barr´on (2002), Papachristos and Konstantaras (2006), Maddah and Jaber (2008), and Wahab and Jaber (2010). 
O	For example, Paul et al. (2014) studied an unconstrained JRP with defective items.
O	In the literature, such restrictions among items have been considered in JRP (e.g., Olsen 2008; Wang et al. 2012).
O	Hence, a particular number of best chromosomes will be imitated to the next generation with a certain probability (Beasley et al. 1993; Kruse et al. 2013).
O	A horizontal crossover is implemented by randomly selecting any row of two parent chromosomes and exchange them (e.g., Anderson et al. 1991).
O	A horizontal mutation is implemented where a row of the chromosome will arbitrarily be selected, and then two genes will be exchanged randomly (Beasley et al. 1993; Kruse et al. 2013).
P	Olsen (2008) proposed an evolutionary algorithm (EA) to solve a constrained JRP.
O	This problem can also be investigated by using another solution methodology, differential evolution method (e.g., Wang et al. 2012) and then it can be compared with the proposed GA.
O	The special case of a 2EVRP with only one satellite can be seen as a VRP (Cuda et al., 2015; Perboli et al., 2011).
O	The structure of the second level is a multi-depot vehicle routing problem (MDVRP), where the depots correspond to the satellite locations (Jepsen et al., 2012).
O	Following the notations of Boccia et al. (2011) we focus on 3/T /T problems.
P	Jacobsen and Madsen (1980) were amongst the first to introduce a twoechelon distribution optimisation problem.
P	An improved solution algorithm for the same problem can be found in Madsen (1983).
O	Following the nomenclature of the authors and the classification in the recent survey on two-echelon routing problems by Cuda et al. (2015), the problem includes location decisions.
O	Crainic et al. (2004) used data from Rome to study an integrated urban freight management system.
O	Crainic et al. (2009) formulated a time dependent version of the problem, including time windows at the customers.
P	Crainic et al. (2010) studied the impact of different two-tiered transportation set-ups on total cost.
P	Perboli et al. (2011) introduced a flow-based mathematical formulation and generated three sets of instances for the 2EVRP with a maximum of 50 customers and four satellites, based on VRP instances. 
P	Perboli and Tadei (2010) solved additional instances and reduced the optimality gap on others by means of new cutting rules.
P	Crainic et al. (2011) solved the 2EVRP with a multi-start heuristic.
P	Jepsen et al. (2012) presented a branch-and-cut method, solving 47 out of 93 test instances to optimality, 34 of them for the first time.
P	Hemmelmayr et al. (2012) developed a metaheuristic based on adaptive large neighbourhood search (ALNS) with a variety of twelve destroy and repair operators.
P	This approach tends to privilege accuracy (high quality solutions) over simplicity and flexibility (Cordeau et al., 2002).
N	Note that the results of ALNS on the problem instances with 50 customers cannot be compared with the proven optimal solutions by Jepsen et al. (2012), since the algorithm does not consider a limit on the number of vehicles per satellite, but rather a constraint on the total number of vehicles.
P	Santos et al. (2014) implemented a branch-and-cut-and-price algorithm, which relies on a reformulation of the problem to overcome symmetry issues.
P	Baldacci et al. (2013) presented a promising exact method to solve the 2EVRP.
O	Recently Zeng et al. (2014) published a greedy randomized adaptive search procedure, combined with a route-first cluster-second splitting algorithm and a variable neighbourhood descent.
P	Laporte (1988) presented a general analysis of location routing problems and multi-layered problem variants.
O	In a slightly different context, Laporte and Nobert (1988) formulated a vehicle flow model for the 2ELRP.
O	Following the notation by Boccia et al. (2011) they analysed 3/R/R, 3/R/T, 3/T /R, and 3/T /T problem settings.
P	Boccia et al. (2011) provided three mathematical formulations for the 2ELRP using one-, two-, and three indexed variables inspired from VRP and MDVRP formulations.
P	Nguyen et al. (2012a) introduced two new sets of instances for the 2ELRPSD
P	In Nguyen et al. (2012b) the authors improved their findings on the same instances by using a multi-start iterated local search.
P	Contardo et al. (2012) proposed a branch-and-cut algorithm, which is based on a two-indexed vehicle flow formulation, as well as an ALNS heuristic.
O	Schwengerer et al. (2012) extended a variable neighbourhood search (VNS) solution approach for the location routing problem from Pirkwieser and Raidl (2010) and applied it to several instance sets, including the two aforementioned ones with a single depot.
P	For further details on both problem classes, we refer to the recent survey by Cuda et al. (2015).
P	To the best of our knowledge, only one heuristic has reported, to this date, computational results on larger 2EVRP instances (Hemmelmayr et al., 2012).
P	Different mathematical formulations have been proposed for the 2EVRP (Perboli et al., 2011; Jepsen et al., 2012; Baldacci et al., 2013; Santos et al., 2013, 2014) and for the 2ELRP (Boccia et al., 2011; Contardo et al., 2012).
P	In this section, we display compact formulations based on the model of Cuda et al. (2015).
P	The proposed metaheuristic follows the basic structure of a large neighbourhood search (LNS), which was first introduced by Shaw (1998).
P	Such a ruin and recreate approach (Schrimpf et al., 2000) has been successfully applied to multiple variants of vehicle routing problems in the past (see, e.g., Pisinger and Ropke 2010).
O	In order to reduce the complexity, moves are only attempted between close customers, as done in the granular search by Toth and Vigo (2003).
O	Further information on these moves can be found in the survey by Vidal et al. (2013).
P	In general, our algorithm requires less local and large neighbourhood operators than the ALNS proposed by Hemmelmayr et al. (2012).
P	Sets 2 and 3 were proposed by Perboli et al. (2011) and have been generated based on the instances for the CVRP by Christofides and Eilon.
P	Set 4 was proposed by Crainic et al. (2010); all of them were downloaded from OR-Library (Beasley, 2014).
O	The instance Sets 2 to 5 as used in Hemmelmayr et al. (2012) were also communicated to us by email (Hemmelmayr, 2013).
O	We noticed a few key differences with the ones available from Beasley (2014).
O	Set 6 instances were provided from the authors (Baldacci, 2013).
O	The names of the instances downloaded from Beasley (2014) and used by Hemmelmayr et al. (2012) were the same, but the instances with 50 customers included different locations for the satellites.
P	Instance names used by Baldacci et al. (2013) have the satellite numbers incremented by one.
O	Apart from that they are identical with what we received from Hemmelmayr (2013).
O	For example Set 2a instance named E-n51-k5-s2- 17 (Satellites 2 and 17) corresponds to E-n51-k5-s3-18 in the result tables of Baldacci et al. (2013).
O	Again, the sources Beasley (2014) and Hemmelmayr (2013) were identical for instances with 21 and 32 customers, but different for instances with 50 customers.
O	All Set 3 instances from Hemmelmayr (2013) place the depot at coordinates (0,0), whereas the files of Beasley (2014) have the depot located at (30,40).
P	Set 3a includes all instances with less than 50 customers, Set 3b the larger instances which have been used by Hemmelmayr et al. (2012), and Set 3c the larger instances as they are available at the OR-Library, and have been used by Baldacci et al. (2013), among others.
N	As proposed in Baldacci et al. (2013), we solved both versions and follow their nomenclature: Set 4a with the limit per satellite, and Set 4b when the constraint of vehicles per satellite is relaxed.
P	This set of instances has been proposed by Hemmelmayr et al. (2012).
P	Baldacci et al. (2013) were able to find solutions on the small instances with only five satellites.
P	To the best of our knowledge, solutions on these instances have only been reported in Baldacci et al. (2013).
O	The source of the instance sets is also provided (Hemmelmayr, 2013; Beasley, 2014; Baldacci, 2013).
P	We compare the performance of our method on the 2EVRP instances with the hybrid GRASP +VND by Zeng et al. (2014) and the ALNS by Hemmelmayr et al. (2012), when applicable; as well as the currently best known solutions for each instance from the literature.
P	The next columns display the results of the proposed method (LNS-2E), and methods by Hemmelmayr et al. (2012) (HCC), Zeng et al. (2014) (ZWZW) for the 2EVRP when applicable, and Schwengerer et al. (2012) (SPR) for the 2ELRPSD.
P	Following the work of Schwengerer et al. (2012), we also used average and best of 20 for the 2ELRPSD for better comparison
O	Hemmelmayr et al. (2012) and Schwengerer et al. (2012) report CPU times (which may be slightly smaller than wall clock times).
N	Zeng et al. (2014) only report the time when the best solution was found, but no overall runtime of the algorithm
N	To the best of our knowledge, we are the first ones to report solutions on the 2c instances obtained from Beasley (2014).
N	For Set 3c, the optimal objective values are derived from Baldacci et al. (2013) and Jepsen et al. (2012), but no results from HCC or ZWZW are available.
N	Jepsen et al. (2012) considered a limit on the number of city freighters available at each satellite, HCC and ZWZW did not impose this limit, and instead considered the limit on the total number of city freighters only.
O	Baldacci et al. (2013) addressed both variants of the instances to compare their results to both previous results, introducing a new nomenclature: Set 4a for the instances including the limit of vehicles per satellite, and Set 4b when this limit is relaxed.
O	102 out of the 108 instances have been solved to optimality by Baldacci et al. (2013).
O	To the best of our knowledge, Hemmelmayr et al. (2012) were the only authors who published results on the large Set 5 instances with 10 satellites to this date.
P	Baldacci et al. (2013) report solutions on the small Set 5 instances (100 customers/5 satellites), improving three out of six instances to optimality.
O	The algorithm of Hemmelmayr et al. (2012) was evaluated with a limit of 500 iterations.
O	The results can be seen in Tables 10 and 11. BKS are derived from Schwengerer et al. (2012); Nguyen et al. (2012a,b); Contardo et al. (2012).
O	Also on this problem class LNS-2E is competitive, with solutions being on average within 0.6% of the solutions found by the state-of-the-art VNS by Schwengerer et al. (2012) (SPR).
P	Crainic et al. (2010) provide a detailed overview on the distribution of customers and satellites in instances of Set 4.
P	Although a product’s price and demand are interdependent and affect each other, they still are seldom optimized jointly although this tendentially leads to increased revenues (Jacobs et al., 2000).
O	Recently, Kocabiyiko˘glu et al. (2014) have shown under which circumstances coordination these decisions is worth the effort and under which the effort would exceed its benefit.
O	Weatherford (1997) was one of the first authors to recognize the importance of jointly optimizing prices and seat allocations.
O	In Feng and Xiao (2006) it was assumed that there is only a pre-specified set of prices from which a subset was chosen in their optimal solution.
P	Chew et al. (2009) proposed a dynamic program for one product with a twoperiod lifetime.
O	Cizaire and Belobaba (2013) assumed two fare classes but these were set in a twoperiod time frame and the decisions in the earlier time frame affected the decisions in the later one.
O	Their model combined the approaches by Weatherford (1997) and Chew et al. (2009).
O	One of the first to consider the joint pricing and seat allocation optimization in a network was de Boer (2003).
P	See also Bitran and Caldentey (2003) for a literature overview and several proposed models. 
O	Cˆot´e et al. (2003) were among the first authors to consider competition in a joint pricing/allocation problem.
P	Raza and Akgunduz (2008) introduced a two-player model for simultaneous price and seat allocation competition on a single flight leg for two price classes.
O	In Zhao and Atkins (2008) the problem of N newsvendors under simultaneous price and inventory competition was investigated.
O	See e.g. Adida and Perakis (2010), Gallego and Hu (2014) as well as Mart´ınez-de-Alb´eniz and Talluri (2011).
O	This is a quite common assumption made also in e.g. Cˆot´e et al. (2003), Raza and Akgunduz (2008) as well as Zhao and Atkins (2008).
O	See also Talluri and van Ryzin (2004b, Sec. 7.3) as well as Phillips (2007, Ch. 3). Da pf ≥ 0 is the demand with both players’ prices being equal to zero and β a pf ≥ 0 shows how this demand reacts to a price change of the considered airline a.
O	Since the solution can be interpreted directly in terms of how to set the booking limits for the individual products, it forms a pure strategy (Fudenberg and Tirole, 1991, p. 5).
N	Note that the model Ma presented in the previous section is not linear which makes it hard to solve because optimality is harder to achieve in non-linear constrained problems than in linear ones (see e.g. Bazaraa et al., 2006).
O	Although this is a simplifying assumption, it is quite common in RM theory and practice (see e.g. de Boer et al., 2002, p. 91, Talluri and van Ryzin, 2004a, p. 16 as well as Talluri and van Ryzin, 2004b, Sec. 7.1).
P	There, the players’ strategies are mutual best responses (Fudenberg and Tirole, 1991, p. 11).
O	This is the case if also 1) the flight network is acyclic which is always the case for time-indexed flight networks used in revenue management, 2) the capacity values are integer parameters, and 3) all products only occupy one seat (see also de Boer et al., 2002, p. 77).
P	An algorithm for computing pure NE as mutual best responses computed from quantity-based DLPs was recently introduced by Grauberger and Kimms (2014b) and is described in Appendix A of this paper.
O	However, this would necessitate own algorithms to determine each product price individually, e.g. the golden section method (for the products in NAa ) or the cyclic coordinate method (for the products in set A), see e.g. Bazaraa et al. (2006, Ch. 8).
O	There, at least two thirds of the members have one or two hubs and most airlines do not serve less than 20 and more than 200 destinations, see OneWorld (2014), SkyTeam (2014), and StarAlliance (2014).
P	To avoid these issues, Grauberger and Kimms (2014b) proposed a perturbation of the product prices to receive unique optimal solutions.
P	Similar results were found by Zhao and Atkins (2008) for newsvendors under simultaneous price and quantity competition.
O	As Isler and Imhof (2008) noted, a repeated price competition may lead to a spiral-down effect in which the prices and revenues decrease ever further.
O	In recent years, a growing public concern about greenhouse gas emissions and health related pollution from the transportation sector has led to more attention to electric and other alternative fueled vehicles both in academies and industry (World Health Organization).
O	For instance, in Copenhagen, Denmark, buses travel approximately 110 million kilometers a year and the bus fleet on average produces around 0.9 kg CO2 per kilometer along with other pollutants (Movia).
O	These limitations result in ‘range anxiety’, which 11 is the fear of running out of battery and the concern of making an unplanned trip (Bakker (2011)).
P	The VSP has been extensively studied in the literature and extended to different variants, including the Multi-Depot VSP (MD-VSP) (Bodin et al. (1983) and Carpaneto et al. (1989)), the Multiple Vehicle Types VSP (Lenstra and Kan (1981)), and the VSP with Route Constraints (VSP-RC) (Bunte and Kliewer (2009)) where different types of route constraints can be enforced, including route duration (Freling and Paixao (1995)), route distance (Bodin et al. (1983)) or maximum vehicle bus line changes (Kliewer et al. (2008)).
O	A variant of the VSP that considers recharging/refueling options is the Alternative Fuel Vehicle Scheduling Problem (AF-VSP) studied by Adler (2014).
P	In other words, the AF-VSP is a special case of our E-VSP. In Adler (2014), the author propose a construction heuristic as well as a column generation approach to solve the AF-VSP, and tests his algorithms on the metropolitan bus system of Phoenix, Arizona.
P	Erdogan and Miller- Hooks (2012) introduce the Green Vehicle Routing (G-VRP).
O	Similar to Adler (2014), they also assume full charging and a fixed charging time for each visit to the recharging station.
O	Felipe et al. (2014) consider an extension of the G-VRP, where recharging stations are of different types with different costs and recharging speeds.
O	Schneider et al. (2014) extend the G-VRP to the Electric Vehicle Routing Problem with Time Windows (E-VRPTW) by considering ustomers’ time windows, unlimited number of recharges per route and a variable recharging time which depends on the remaining fuel level when a vehicle arrives at the recharging station.
O	However, partial charging is still not an option in Schneider et al. (2014), i.e., a vehicle must be fully recharged when leaving the recharging station. 
O	Hiermann et al. (2016) study the Electric Fleet Size and Mix VRPTW, where electric vehicles with different battery capacities, load capacities, energy consumptions and recharging rates are used.
P	In Goeke and Schneider (2015) a mixed fleet of electric vehicles and conventional vehicles is used.
P	Desaulniers et al. (2014) further extend the E-VRPTW by allowing partial charging.
P	A mathematical model for the E-VSP can be derived from the model for the E-VRPTW presented in Schneider et al. (2014).
O	In fact, if one does not need to model partial recharging and multiple depots then the E-VRPTW model by Schneider et al. (2014) is sufficient to model the E-VSP.3295: 
P	There are several ways strengthening the inequalities (3.9)-(3.12) based on the methods proposed by Desrochers and Laporte (1991).
O	which are straightforward adaptations of inequalities (28) and (29) in Desrochers and Laporte (1991).
O	With these bounds a valid inequalities involving yi can again be derived from (28) and (29) in Desrochers and Laporte (1991):
P	ALNS, firstly proposed by Ropke and Pisinger (2006), has been successfully applied to VRPTW and various VRP extensions.
O	Since the start time is fixed, the graph of an E-VSP can be reduced significantly compared to a VRPTW with wide TWs (Haghani and Banihashemi (2002)).
P	ALNS is an extension of Large Neighborhood Search (LNS) originally proposed by Shaw (1998).
P	The initial solution of for ALNS is generated by a greedy constructive heuristic, similar to the Concurrent Scheduler Algorithm for the AF-VSP proposed by Adler (2014) and originally designed by Bodin et al. (1978).
N	We have implemented a class of regret insertion heuristics (Potvin and Rousseau (1993)).
P	A set of E-VSP benchmark instances are generated in a way similar to the generation of the MD-VSP instances in Carpaneto et al. (1989) and inspired by Pepin et al. (2009).
P	It is estimated that CHP units based on the Combined Cycle or Open Cycle Gas Turbine (CCGT, OCGT) technology can reach an efficiency of 90%, while typical values are 60% for standard power-only plants based on the same technology (Veerapen and Beerepoot, 2011).
P	Indeed, CHP units provide an efficient way of producing heat that can be made essentially carbon neutral by switching from fossil fuel to biomass (Fernandez Pales et al., 2014).
O	Furthermore, owing to their flexibility, CHP units can support the integration of large-scale electricity production from variable and partlypredictable renewable sources, e.g., wind and solar, by providing back-up power when the output of the latter is insufficient (Veerapen and Beerepoot, 2011).
O	To unlock this potential, it is critical that the heat infrastructure and production assets are thought, planned and managed effectively and jointly with the other energy carriers (electricity, gas), the transport system and the industrial assets in the same urban area (Meibom et al., 2013).
O	This is consistent with the current situation in many regional power markets, where the largest part of the exchange of electricity takes place one day in advance (Weber, 2010).
O	In view of the stochastic and dynamic nature of the problem, the framework of Affinely Adjustable Robust Optimization (AARO) is appropriate, see Ben-Tal et al. (2009).
O	The problem of optimizing the operation of a storage-CHP unit system is tackled via stochastic programming by Palsson and Ravn (1994) and via stochastic dynamic programming by Ravn and Rygaard (1994).
P	A deterministic approach for scheduling a district heating system is studied by Aringhieri and Malucelli (2003), resulting in a Mixed Integer Linear Programming formulation of the problem.
P	Rolfsman (2004) used a stochastic programming approach to the problem including a model of the uncertainty in heat demand and day-ahead power prices.
O	The stochastic programming approach has been enriched by further considering the intra-day stage by De Ridder and Claessens (2014) or by allowing for the submission of offering curves in the day-ahead market by Dimoulkas and Amelin (2014).
P	The unit commitment problem has been studied via (two-stage) Adaptive Robust Optimization (ARO) considering uncertainty in load (Bertsimas et al., 2012), wind power production Jiang et al. (2012) or contingencies Street et al. (2011).
P	The dispatch problem, where unit commitment variables are not considered, for both energy and reserve has been studied with a similar approach by Zugno and Conejo (2015) with a view to immunizing the solution to uncertainty in wind power production.
O	Warrington et al. (2013) incorporated linear decision rules within a stochastic optimization approach to a similar dispatch problem.
P	Furthermore, Lima et al. (2015) proposed an ARO model for the short-term optimization of a virtual power plant under uncertainty in prices and wind power production.
N	Secondly, we set up a case-study using realistic data for unit parameters, heat consumption and prices obtained for the Copenhagen area, which accounts for roughly 20% of the total consumption of heat in Denmark (Varmelast.dk, 2014).
P	We refer the reader to Morales et al. (2014) for further details.
O	Hence it is often referred to as marginal electricity loss for heat production, see Weber (2005).
O	We consider an electricity pool with time-varying prices set by the interception between supply and demand, see Morales et al. (2014).
O	We assume price consistency, i.e., that the expectation of the balancing market price be equal to the day-ahead price, see Zavala et al. (2014).
O	They are obtained by applying strong linear duality row-wise to each of the constraints in (23d), as commonly done in robust optimization to determine the robust counterpart of a single inequality constraint, see Bertsimas et al. (2011).
P	We refer the reader to Ben-Tal et al. (2009) for a thorough introduction to the concept of uncertainty set.
O	This could be performed by using quantile regression, see for example Nielsen et al. (2006).
P	We also refer to Bertsimas et al. (2014) for a thorough introduction to datadriven uncertainty set construction.
P	Bertsimas et al. (2012); Jiang et al. (2012); Zugno and Conejo (2015) are just few examples where budget uncertainty sets are used within applications to energy systems.
O	Parameters for the CHP units are based on values for existing units of the Copenhagen area as reported in Varmeplan Hovedstaden (2014).
O	Power prices for Eastern Denmark (DK2 area of Nord Pool) are publicly available for download at Energinet.dk (2015).
P	We use heat consumption data from the western Copenhagen area, VEKS, available at Madsen (2015).
O	The variances we used for the heat demand and balancing market prices are consistent with the state-of-the-art on forecasting, which reports RMSE values of about 7% for heat load (Nielsen and Madsen, 2006; Zelinka et al., 2013) and 33% for balancing market prices (J´onsson et al., 2014).
P	We employed 100 scenarios after reduction from an initial set of 2000 performed using the fast-forward method presented in Heitsch and R¨omisch (2003).
O	The bin packing problem is known to be NP-hard (Garey and Johnson (1979)).
O	Apart from the simple lower bound, lPn i=1 wi C m, other lower bounds developed by Fekete and Schepers (2001), Martello and Toth (1990) (bound L3) and Alvim et al. (2004) are also used.
P	It has been proven ecient on several combinatorial optimization problems (Vasquez et al. (2003); Zuerey and Vasquez (2015)). 
P	Martello and Toth (1990) proposed a branch-and-bound procedure (MTP).
P	Scholl et al. (1997) developed a hybrid method (BISON) that combines a tabu search with a branch-and-bound procedure based on several bounds and a new branching scheme.
O	Schwerin and Wäscher (1999) odered a new lower bound for the BPP based on the cutting stock problem, then integrated this new bound into MTP and achieved high-quality results.
P	Valerio de Carvalho (1999) proposed an exact algorithm using column generation and branch-and-bound.
P	Gupta and Ho (1999) introduced a minimum bin slack (MBS) constructive heuristic.
P	Fleszar and Hindi (2002) developed a hybrid algorithm that combines a modied version of the MBS and the Variable Neighborhood Search.
P	Alvim et al. (2004) presented a hybrid improvement heuristic (HI_BP) that uses tabu search to move the items between bins.
P	Singh and Gupta (2007) proposed a compound heuristic (C_BP) which combines a hybrid steady-state grouping genetic algorithm with an improved version of Fleszar and Hindi's Perturbation MBS.
P	Loh et al. (2008) developed a weight annealing (WA) procedure, by relying on the concept of weight annealing to expand and accelerate the search by creating distortions in various parts of the search space.
P	Fleszar and Charalambous (2011) oered a modication to the PerturbationMBS method (Fleszar and Hindi (2002)) where a new sucient average weight (SAW) principle is introduced to control the average weight of items packed in each bin (referred to as Perturbation-SAWMBS).
N	The authors also reported signicantly lower quality results for the WA heuristic compared to those given in Loh et al. (2008).
O	To the best of our knowledge, the most recent work in this area, is reported in Quiroz-Castellanos et al. (2015).
P	Brandão and Pedroso (2013) devised an exact approach for solving the bin packing and cutting stock problems based on an Arc-Flow Formulation of the problem which is then solved with the commercial Gurobi solver.
O	With regard to the two-dimensional VPP, Spieksma (1994) proposed a branch-and-bound algorithm, while Caprara and Toth (2001) reported exact and heuristic approaches as well as a worst-case performance analysis.
P	A heuristic approach using a set-covering formulation was presented by Monaci and Toth (2006).
P	Masson et al. (2013) proposed an iterative local search (ILS) algorithm for solving the Machine Reassignment Problem and VPP with two resources; they reported the best results for the classical VPP benchmark instances of Spieksma (1994) and Caprara and Toth (2001).
P	Generating an optimal packing for a set of items, as originally proposed in Gupta and Ho (1999), is a common procedure introduced in several papers (Fleszar and Hindi (2002), Fleszar and Charalambous (2011)).
O	The P ack move is the same as the "load unbalancing" used in Alvim et al. (2004).
P	This set consists of ve classes of instances: 1) a class developed by Falkenauer (1996) made of two sets, uniform and triplets (denoted respectively by U and T in the tables of results) each with 80 instances;
O	a class developed by Scholl et al. (1997) consisting of three sets set_1, set_2, and set_3 with 720, 480 and 10 instances respectively;
P	a class of instances developed by Schwerin and Wäscher (1999) made of two sets, was_1 and was_2, each with 100 instances;
P	a class of instances developed by Wäscher and Gau (1996), called gau_1, containing 17 problem instances;
O	the hard28 class, consisting of 28 dicult problems instances used, e.g. in Belov and Scheithauer (2006).
O	All instances can be downloaded from the Web page of the EURO Special Interest Group on Cutting and Packing (ESICUP (2013)).
P	Optimal solutions for the rst three classes (1570 instances in all) were obtained using several heuristics, including HI_BP (Alvim et al. (2004)) and GGA-CGT (Quiroz-Castellanos et al. (2015)).
P	HI_BP optimally solves 12 of the 17 instances in gau_1, while other recent heuristics yielded more optimal results (e.g. 15 by C_BP (Singh and Gupta (2007)) and 16 by PerturbationSAWMBS (Fleszar and Charalambous (2011)) and GGA-CGT).
O	The same applies to the HI_BP algorithm, as reported in Quiroz-Castellanos et al. (2015).
P	The exact method based on the Arc-Flow Formulation, as presented in Brandão and Pedroso (2013), can solve all instances to optimality within a reasonable computing time, including all instances from the hard28 dataset.
P	The lower bound used for each instance was the best of the following four bounds: L1 = lPni=1 wiCm,20 L (p)∗ proposed in Fekete and Schepers (2001) with p = 10, L3 bound from Martello and Toth (1990) and Lϑ proposed in Alvim et al. (2004).
O	By multiplying the CPU time of each algorithm by its corresponding scaling factor calculated above, we obtain a Unied Computational Time (UCT) (Perboli et al. (2008)).
O	The 2-DVPP instances used to evaluate the performance of CNS_BP were extracted from Spieksma (1994) and Caprara and Toth (2001).
O	Classes 2, 3, 4, 5 and 8 are known to be easily solvable by simple greedy heuristics (Monaci and Toth (2006)), hence our experiments focus on the remaining classes, a total of 200 instances.
P	The previous best results have been obtained by the iterated local search (MS-ILS) heuristic reporteded in Masson et al. (2013).
P	It should be noted that 330 out of 400 instances could still be solved by the exact approach proposed by Brandão and Pedroso (2013); 60 out of the 70 unsolved instances belong to classes 4 and 5 and can be easily solved optimally with non-exact approaches.
O	The MS − ILS results reported in Masson et al. (2013) were obtained with a time limit of 300 seconds on an Opteron 2.4 GHz with 4 GB of RAM memory running Linux OpenSuse 11.1.
O	In these problems, shifts and days-off must be assigned to employees in order to meet the required staffing demand, while respecting legislative and organisational guidelines as well as personal requests (Van den Bergh et al., 2013).
O	A later step in the scheduling process then assigns tasks taking into account employee availability based on the shift assignments (Kolen et al., 2007).
O	However, when tasks are known at the time of roster generation, assigning tasks and shifts simultaneously potentially leads to lower cost solutions compared to making the assignments sequentially (Ernst et al., 2004).
P	In the academic literature, both exact and heuristic cut generation algorithms have been proposed for problems similar to the TSPR (Detienne et al., 2009; Guyon et al., 2010).
P	Dowling et al. (1997) consider flexible shifts and tasks that are fixed in time.
O	Lap`egue et al. (2013) present a problem in which tasks are also fixed in time, without explicitly defining shifts.
O	Meisels and Schaerf (2003) discuss a class of general employee timetabling problems in which both tasks and shifts are fixed in time. 
P	Krishnamoorthy et al. (2012) introduce a problem in which tasks, fixed in time, are assigned to multi-skilled employees whose working times are predetermined.
P	Alternative algorithms for this problem are discussed by Baatar et al. (2015); Smet et al. (2014).
O	Elahipanah et al. (2013) discuss a task scheduling problem with both preemptive and nonpreemptive tasks (see also Prot et al. (2015)).
P	The problem presented by Detienne et al. (2009) requires predefined work patterns and activities to be assigned to employees. 
P	Guyon et al. (2010) extend this work by introducing time windows for the tasks.
O	Cˆot´e et al. (2011) use implicit models with context-free grammars to model complex rules regarding shift design.
O	Musliu et al. (2004) study the minimum shift design problem.
P	Robinson et al. (2005) address a problem in which the preemptive tasks are defined by a release date and a deadline, such that their time of execution must also be determined.
P	Brucker and Qu (2014) extend this model with qualification requirements.
O	Brucker et al. (2011) study the complexity of various personnel scheduling models, including a project centered planning model that integrates preemptive task scheduling and work pattern assignment to employees. 
O	The intricate coupling between task and shift assignments results in an integrated problem which was previously deemed computationally impractical for realistically sized instances (Ernst et al., 2004).
P	A polynomial time algorithm for finding all maximal cliques in Ge is presented by Krishnamoorthy et al. (2012).
P	Therefore, the established methodology for dealing with such constraints is to consider them as soft and penalise violations in the objective function with a static weight representing the constraints’ importance (Burke et al., 2004).
P	A relaxation of the master problem is thus obtained, which can be solved using column generation (L¨ubbecke and Desrosiers, 2005).
N	This often results in the pricing problem generating columns never selected in an optimal solution (Rousseau et al., 2007).
P	To address this issue, different stabilisation approaches have been proposed (Vanderbeck, 2005)
O	In this work, column generation is stabilised by smoothing the dual prices (Pessoa et al., 2013)
P	The horizontal decomposition algorithm is based on the constructive matheuristic introduced by Smet et al. (2014) which sequentially solves subproblems consisting of b employees.
O	Constantino et al. (2014) address a nurse rostering problem by decomposing it into an assignment problem for each day of the scheduling period.
P	Whereas traditional local search neighbourhoods have a tendency to quickly converge to locally optimal solutions, very large-scale neighbourhood search algorithms (VLSN) overcome this issue by efficiently exploring large, complex neighbourhoods (Ahuja et al., 2002).
P	Hamming distance was introduced as a concept in integer programming by Fischetti and Lodi (2003) in the context of local branching.
O	Therefore, following the methodology from Smet et al. (2014), k 0 is set to the square root of |T| multiplied by a scaling factor τ (Equation (43)).
O	Soman et al. (2004a) state that the majority of research contributions do not address specific characteristics of food processing, e.g. high capacity utilisation, sequence-dependent setups and limited shelf life due to product decay. 
P	In general practice, lot-sizing and scheduling problems are solved separately in successive hierarchical phases (Claassen and Beek, 1993; Drexl and Kimms, 1997; Kreipl and Pinedo, 2004; Soman et al., 2004a; Soman et al., 2007).
O	As a consequence, the planning process has to be redone (with or without over-time) and/or frequent rescheduling takes place in daily practice (Kreipl and Pinedo, 2004).
O	Currently, there exists a general consensus regarding a closer integration of lot-sizing and scheduling decisions, see Meyr (2000), Gupta and Magnusson (2005), Jans and Degraeve (2008), Almada-Lobo et al. (2008), Clark et al. (2011), and Menezes et al. (2011).
P	Guimarães et al. (2014) survey the main modelling approaches and present a classification framework for integrated lot-sizing and scheduling models.
P	Some recent real-life applications of simultaneous lot-sizing and scheduling can be found for instance in (Baldo et al., 2014; Camargo et al., 2014; Figueira et al., 2015).
O	Kreipl and Pinedo (2004) give an extensive overview of practical issues for planning and scheduling processes.
O	In a special issue on lot-sizing and scheduling, Clark et al. (2011) confirm the need for more realistic and practical variants of models for simultaneous lotsizing and scheduling.
O	There is a complicating issue with respect to sequence-dependent setup costs and times, commonly referred to as the assumption of the triangular setup conditions (Almada-Lobo et al., 2008; Clark et al., 2011; Clark et al., 2014; Gupta and Magnusson, 2005)).
O	Menezes et al. (2011) confirm that nontriangular setups may occur in FPI due to contamination between production lots.
O	Clark et al. (2014) mention that contamination is a particularly concern for FPI.
O	Moreover, the approach offers a natural starting point for integrating delivery time windows in lot-sizing and scheduling models as mentioned by Clark et al. (2011).
O	From a modelling point of view, it is convenient to distinguish two general classes of models (Eppen and Martin, 1987), i.e. small bucket (SB) and big (or large) bucket (BB) modelling approaches.
O	The basic DLSP includes (sequence-independent) setup costs and setup carry-over at zero setup time (Fleischmann, 1990). 
O	Porkka et al. (2003) compare models with and without setup carry-overs.
O	Comparable results are found by Sox and Gao (1999).
P	We refer to Drexl and Kimms (1997) and Salomon et al. (1991) for a broader view on variants of the DLSP.
O	Fleischmann (1994) analyses the multi-item single machine DLSP with sequencedependent setup costs.
P	Salomon et al. (1997) continue the latter study and reformulate a DLSP that captures sequence-dependent setup times (DLSPSD).
P	Jordan and Drexl (1998) present a comparable model in which idleness is indicated by an artificial product too.
O	A recent approach is due to Guimarães et al. (2014).
O	Wolsey (1997) extended the work of Constantino (1996) for problems with sequenceindependent setups for small bucket formulations with sequence-dependent setup times and costs.
O	Time intervals in a BB model may represent a time slot of one week (or more) in practice (Drexl and Kimms, 1997).
O	Decision variables, parameters and objective function are comparable in both problems (Drexl and Kimms, 1997). 
P	Suerie and Stadtler (2003) use the simple plant location problem to obtain a tight new model formulation for setup carryover in the CLSP with sequence-independent setup costs and times.
P	Sox and Gao (1999) introduce the Generalized Capacitated Lot-sizing Problem (GCLP).
P	The authors also apply the network reformulation approach as proposed by Eppen and Martin (1987) and compare the behaviour of a set of models.
P	For confirmation we refer to proposed variants of the CLSP (Almada-Lobo et al., 2007; Almada-Lobo et al., 2008; Gopalakrishnan, 2000; Gopalakrishnan et al., 1995; Guimarães et al., 2013; Gupta and Magnusson, 2005; Haase and Kimms, 2000; Menezes et al., 2011), variants of hybrid BB and SB models like the General Lot-sizing and Scheduling Problem (GLSP) (Ferreira et al., 2012; Ferreira et al., 2009; Fleischmann and Meyr, 1997; Guimarães et al., 2013, 2014; Meyr, 2000; Transchel et al., 2011), and variants of block planning approaches, originally introduced by Gunther et al. (2006).
O	The literature review on extensions of capacitated lot-sizing by Quadt and Kuhn (2008) and Guimarães et al. (2014) confirm the trend in which (hybrid) BB approaches are preferred to SB models.
P	For a complete overview we also refer to (Claassen and Hendrix, 2014).
P	Studies in which issues of integrating perishability are considered, mostly refer to managing perishability in production-distribution planning (Amorim et al., 2012; Amorim et al., 2013; Chen et al., 2009).
O	Amorim et al. (2011) state that papers discussing simultaneous lot-sizing and scheduling for perishable goods are very rare.
P	The early work of Van Zyl (1964) and Nahmias (1975) already focused on the concept of a fixed life time, where the product is lost after reaching a certain shelf life.
O	Raafat (1991) reviews literature on continuously detoriating inventory models and discusses the concept of deterioration where a fixed percentage of the stored items is lost each period (exponential decay). 
P	We refer to Karaesmen et al. (2011) and (Bakker et al., 2012) for a more recent reviews on perishability in inventory systems.
O	One of the first contributions in the area of lot-sizing is provided by Soman et al. (2004b).
P	Entrup et al. (2005) propose three MILP models that integrate shelf-life issues into production planning and scheduling for an industrial case study of yoghurt production.
O	Lee and Yoon (2010) consider a coordinated production-and-delivery scheduling problem that incorporates different inventory-holding costs between production and delivery stages.
O	Chen et al. (2009) conclude that papers discussing production scheduling and/or distribution of perishable goods are relatively rare.
O	Amorim et al. (2011) state that papers discussing simultaneous lot-sizing and scheduling for perishable goods are even rarer.
P	A general overview on managing perishability in production-distribution planning is provided by (Amorim et al., 2013). 
P	We follow Entrup et al. (2005) for modelling perishability by introducing an age-dependent pricing component where the value of the product decreases with the age (or increases in case of ripening).
P	For a general overview of lot-sizing problems we refer to several reviews of the past (Drexl and Kimms, 1997; Karimi et al., 2003; Kuik et al., 1994) and two more recent overviews (Jans and Degraeve, 2008; Quadt and Kuhn, 2008).
O	This assumption is made in many lot-sizing and scheduling models (Drexl and Kimms, 1997). 
O	This aspect is included by an age-dependent component in the inventory-holding costs (Entrup et al., 2005).
O	We use small numerical examples to compare the behaviour and characteristics of the model with the general small bucket model GSB from literature (Wolsey, 1997). 
P	Complexity considerations for (variants of) the DLSP are published in Bruggemann and Jahnke (2000) and Salomon et al. (1991).
O	However, if setup times are considered, even the feasibility problem of DLSP is NP-complete (Salomon et al. 1991, Trigeiro et al. 1989). 
P	Brueggemann and Jahnke (2000) prove that the DLSP is NP-hard in the strong sense.
P	It is easy to show that the SB1 formulation encloses the general DLSP formulation of Brüggemann and Jahnke (2000) by making the setup times independent of the following product and by scaling the production speed to one unit per period.
O	The concept of the relax-and-fix (R&F) heuristic (Dillenberger et al., 1994; Stadtler, 2003) is a typical example of a time-oriented decomposition approach.
O	This section describes a basic variant of an R&F heuristic (Pochet and Wolsey, 2006a) which is used in the numerical analysis of Section 5.
O	Firstly, if the objective for simultaneous lot-sizing and scheduling should include the best compromise between total setup costs and total inventory holding costs, a time oriented aggregation (e.g. in BB models and its variants) may easily disrupt the general principle of optimality for lot-sizing (Pochet and Wolsey, 2006b).
P	Smoothing the heuristic solution by creating some overlap between successive iterations may be an option for further research (Pochet and Wolsey, 2006b). 
P	We refer to Escudero and Salmeron (2005), Federgruen et al. (2007), James and Almada-Lobo (2011), Guimarães et al. (2013) and Oliveira et al. (2014) for an overview of various (hybrid) strategies in an R&F framework. 
P	Powell et al. (1984) was one of the pioneering works that studied the DVAP applied to the trucking industry.
O	Following up on this, Powell (1986) refined and extended the previous model, incorporating uncertainties related to the number of vehicles kept in stock and also keeping track of both empty and full vehicle movements.
O	In subsequent studies, Powell (1987) and Powell (1988) presented alternative mathematical models to be used in operational settings, aiming to help in the management of a fleet of vehicles, anticipating the consequences of the decisions to be taken.
P	Dejax and Crainic (1987) proposed a taxonomy for the empty vehicle repositioning problem in the freight transportation sector, and also presented a comprehensive review about the subject.
P	Frantzeskakis and Powell (1990) developed a new heuristic solution method to solve the DVAP with stochastic demands, and Braklow et al. (1992) presented an interactive large-scale optimization model, aiming to definine the routing of parcels in a transportation network, and also the repositioning of empty vehicles in response to the imbalance of demand. 
P	Powell et al. (1995) deepened and extended the study of dynamic vehicle allocation and its variants to the rail, trucking and air transportation modes, including the repositioning of empty containers.
P	In subsequent studies, Powell (1996) and Powell and Carvalho (1998a,b), presented hybrid models combining the best features of the previous models, and included an experimental test with the use of a simulator. 
P	Hall (1999) developed metrics and dimensions for the spatial and temporal imbalance in the freight flows and studied an application of these metrics for a trucking transportation network.
P	Hall and Zhong (2002) researched the decentralized methods of control and management of a fleet of vehicles in LTL trucking transportation.
P	Other discussions about the DVAP are also presented in Ghiani et al. (2003), and in Vasco (2012).
P	The problem to be solved is characterized mainly at the operational level of planning and only a few papers presented results of practical application in transportation companies (see, e.g., Powell (1988)).
P	In Section 3 we describe a greedy constructive heuristic, an improvement procedure using local search, and also an application of the GRASP (Greedy Randomized Adaptative Search Procedure) (Feo and Resende, 1989, 1995) and the Simulated Annealing (SA) (Kirkpatrick et al., 1983; Cerny, 1985) metaheuristics to solve the DVAP. 
P	Considering the mathematical models presented in the literature, we used the linear programming model presented in Ghiani et al. (2003) as a starting point to model the problem studied in this paper, which is an adaptation of the model originally presented in Powell et al. (1984) and Powell (1986).
O	Similar to Ghiani et al. (2003) and other studies found in the literature, we opted to use a deterministic mathematical model to represent the DVAP due to the following reasons:
P	these models are computationally easier to solve than stochastic models (Birge and Louveaux, 1997);
P	The mathematical model presented here is a modification of the linear programming model usually used to represent the DVAP (Ghiani et al., 2003).
O	The DVAP with multiple groups of vehicles (set E) can be modeled as a linear minimum cost multicommodity flow problem, changing the demand constraint to work as a bundle (knapsack) constraint (Powell et al., 1995).
O	Thus, differently from the model usually used to represent the DVAP in the literature, the modified problem becomes NP-complete (Ghiani et al., 2003).
P	If the problem was comprised of only one commodity, it could be modeled and easily solved using linear programming by relaxing the integrality requirement of the variables in (5) (Ghiani et al., 2003).
P	The GRASP metaheuristic is a multi-start procedure used to solve, for example, combinatorial optimization problems and is also considered a semi-greedy heuristic that add variability in the greedy heuristic (Feo and Resende, 1989, 1995) and consists in an iterative procedure, where each iteration consists basically in two phases:
O	Once the selected element is incorporated to the partial solution, the candidate list is updated and the incremental costs are reevaluated (this is the adaptive aspect of the heuristic) (Feo and Resende, 1995).
O	After the starting solution created by the GRASP procedure, we opted to use the Variable Neighborhood Search (VNS for short) metaheuristic proposed by Mladenovi´c and Hansen (1997) as a local search procedure embeded in GRASP.
N	Contrary to other metaheuristics based on local search methods, VNS does not follow a trajectory but explores increasingly distant neighborhoods of the current incumbent solution, and jumps from this solution to a new one if and only if an improvement has been made (Hansen and Mladenovi´c, 2001).
P	Path-relinking was originally proposed by Glover and Laguna (1997) with the objective of exploring the trajectory between elite solutions that were obtained using GRASP.
O	Details about the combination of GRASP with path-relinking can be seen, for example, in Resende and Ribeiro (2005).
P	The simulated annealing (SA) heuristic is an extension of a Monte Carlo method developed by Metropolis et al. (1953) for the efficient simulation of a solid’s evolution to thermal equilibrium.
P	Thirty years later, Kirkpatrick et al. (1983) and, independently, Cerny (1985) established an analogy between minimizing the cost function of a combinatorial optimization problem and slow cooling a solid, by utilizing the optimization process presented by Metropolis (Koulamas et al., 1994).
P	The neighbor then replaces the incumbent with probability 1 if it has a better goal value, and with some probability strictly between 0 and 1 if it has a worse goal value (Wolsey, 1998).
P	On the other hand, for the process to converge in the long run, the probability of accepting worse solutions decreases over time, so the algorithm should end up converging to a “good” local minimum (Wolsey, 1998).
O	Mishra et al. [31] proposed static power management scheme which allocates global static slack according to the degree of parallelism in a given static schedule generated from any list scheduling heuristic algorithm.
O	Zhu et al. [61] proposed two novel power-aware scheduling algorithms for task sets with and without precedence constraints executing on multiprocessor systems, based on the concept of slack sharing among processors.
O	Aydin et al. [1] addressed the power-aware scheduling of periodic tasks to reduce CPU energy consumption in hard real-time systems through dynamic voltage scaling, and proposed a static solution to compute the optimal speed at the task level on the base of the worst-case workload for each arrival.
P	Ge et al. [13] provided several distributed performance-directed DVFS scheduling strategies for use in scalable power-aware HPC clusters so as to obtain significant energy savings without increasing execution time.
O	Tavares et al. [41] studied a pre-runtime scheduling method which considered the DVFS technique for reducing energy consumption and took the intertask relations and runtime overhead into account.
P	Von Laszewski et al. [44] focused on scheduling virtual machines in a compute cluster to reduce power consumption by the DVFS technique, and presented an efficient scheduling algorithm to allocate virtual machines in a DVFS-enabled cluster by dynamically scaling the supplied voltages.
O	Liu et al. [29] developed an energy-performance balanced task duplication based clustering scheduling algorithm in homogenous clusters, which can significantly save energy by judiciously shrinking communication energy consumption when assigning parallel tasks to computing nodes.
P	Zong et al. [66] proposed two energy-aware duplication-based scheduling algorithms, namely, energy-aware duplication algorithm, EAD, and performance-energy balanced duplication algorithm, PEBD, to achieve the goal of optimizing both performance and energy efficiency in clusters.
P	He et al. [19] developed a rolling-horizon scheduling strategy for the energy constrained distributed real-time embedded systems.
P	Zhu et al. [62] presented a fault-tolerant scheduling algorithm called QAFT for real-time tasks with QoS needs on heterogeneous clusters.
P	Zhu et al. [63] proposed an adaptive energy-efficient scheduling, AEES, for aperiodic and independent real-time tasks on heterogeneous clusters with dynamic voltage scaling.
P	Later, Zhu et al. [64] developed an energy-efficient elastic scheduling for aperiodic, independent and non-real-time tasks with user expected finish times on DVFS-enabled heterogeneous computing systems.
O	Wang et al. [46] concentrated on minimizing energy for precedence-constrained parallel task execution in a cluster, proposed two scheduling algorithms in DVFS-enabled clusters for executing parallel tasks: the PATC and PALS, and developed a green SLA-based mechanism to reduce energy consumption by returning users’ increased tolerant scheduling makespan.
O	Zhang and Guo [60] addressed the issue of minimizing overall energy consumption of a real-time sporadic task system, considering a generalized power model.
P	Guzek et al. [18] investigated and solved the multi-objective precedence constrained application scheduling problem on a distributed computing system.
P	Moschakis and Karatza [32] developed the simulated annealing and thermodynamic simulated annealing in the multi-criteria scheduling of a dynamic multicloud system with virtual machines of heterogeneous performance serving Bag-of-Tasks applications.
P	Stewart and Shen [39] studied how to maximize the green energy used in data centers with the request distribution in multi-datacenter interactive services. 
O	Zhang et al. [59] provided the GreenWare, a novel middleware system that conducts dynamic request dispatching to maximize the percentage of renewable energy used to power a network of distributed data centers, subject to the desired cost budget of the Internet service operators.
P	Goiri et al. [15] proposed the GreenSlot, a parallel job scheduler for data centers partially powered by solar energy.
O	Krioukov et al. [23,24] suggested green energy-aware job schedulers for a single data center, and explored the scheduling of workload to match the available renewable energy supply so as to make more intermittent renewable energy available for use.
P	Liu et al. [30] investigated whether a balance of geographical load can encourage the use of green renewable energy and reduce the use of brown fossil fuel energy.
P	Goiri et al. [16] provided the GreenHadoop, a MapReduce framework for a data center powered by a photovoltaic solar array and the electrical grid.
O	Goiri et al. [17] presented the Parasol, a prototype green data center built as a research platform, and the GreenSwitch, a model-based approach for dynamically scheduling the workload and selecting the source of energy to use. 
P	Deng et al. [10] applied the two-stage Lyapunov optimization to design an online control algorithm – SmartDPSS, which optimally schedules multi-source energy to power a data center with arbitrary demand in a cost minimizing fashion. 
P	Bird et al. [6] introduced a new conceptual approach to green data centers by using multiple, distributed networked data centers that are co-located with renewable energy.
P	Li [27] studied a power and performance management problem for parallel computations in clouds and data centers.
P	Lei et al. [26] proposed the SGEESS, a smart green energy-efficient scheduling strategy, to increase utilization of renewable energy,
P	Since objectives in a MOP are often conflicting with each other, the optimal solution is a not a single one but rather a set of trade-off solutions, i.e., the so-called Pareto optimal solutions [49].
O	Like Guzek et al. [18], we use the relative speed rs instead of the frequency and the value of α is set to 1 to normalize the voltage–frequency tables.
O	Also, goal vectors, G of size Ng are randomly generated within the bounds [ ] 0, 1.2 as suggested in Wang et al. [47].
P	Readers are referred to Wang et al. [47] for a detained description of the fitness calculation
P	Queipo [33] proposed a meta-model (or surrogate-model) as the approximation of the original function to replace calls to the expensive function evaluations.
P	Liu [16] proposed a Gaussian process surrogate model assisted evolutionary algorithm, which map the training data to a lower dimensional space by employing a dimension reduction technique, and a new surrogate model-aware search mechanism is used to make the search focus on the promising subregion.
O	Zhou [17] proposed a memetic algorithm using multi-surrogates.
O	Liu [34] proposed a fast differential evolution using k-nearest neighbor predictor as function approximation.
P	Zhang [36] developed a predictive distribution model combining Gaussian stochastic model with fuzzy clustering based model to measure the expected improvement of each individual.
P	Park [37] also proposed an efficient differential evolution using knearest neighbor as function estimator to alleviate a burden of a large number of function evaluations.
P	Wang [38] proposed a novel hybrid discrete differential evolution algorithm (HDDE), in which a local search algorithm based on insert neighborhood structure is embedded to balance the exploration and exploitation by enhancing the local searching ability. 
O	Bhattacharya [9] proposed a hybrid differential evolution with biogeography-based optimization (DE/BBO), which combines the exploration of DE with the exploitation of BBO effectively.
O	Piotrowski [40] proposed a differential evolution with separated groups (DE-SG), which distributes population into small groups, defines rules of exchange of information and individuals between the groups and uses two different strategies to keep balance between exploration and exploitation capabilities.
O	Li [41] proposed a modified differential evolution with self-adaptive parameters method (MDE), in which a probability rule is used to combine two different mutation rules to enhance the diversity of the population and the convergence rate of the algorithm.
P	Qin [42] proposed a self-adaptive differential evolution algorithm (SaDE), in which both trial vector generation strategies and their associated control parameter values are gradually self-adapted by learning from their previous experiences in generating promising solutions.
P	Zhang [43] proposed an adaptive differential evolution with optional external archive (JADE) which improved optimization performance by implementing a new mutation strategy with optional external archive and automatically updated the parameters by evolving the mutation factors and crossover probabilities based on their historical record of success.
P	Wang [44] proposed a differential evolution algorithm with composite trial vector generation strategies and control parameters (CoDE), the primary idea of which is to randomly combine several trial vector generation strategies with a number of control parameter settings at each generation to create new trial vectors.
P	Mallipeddi [45] proposed an ensemble of mutation strategies and control parameters with DE (EPSDE), in which a pool of distinct mutation strategies, along with a pool of values for each control parameter, coexists throughout the evolution process and competes to produce offspring.
P	CLPSO, proposed by Liang et al. [51], is a particle swarm optimization (PSO) variant, in which the personal historical best information of all the particles is used to update a particle's velocity.
P	CMA-ES, as a very famous and efficient evolution strategy, is proposed by Hansen et al. [52].
O	GS-SOMA is proposed by Zhou et al. [55].
O	GPEME, proposed by Liu et al. [16], uses a surrogate model-aware evolutionary search framework.
P	SAGA-GLS, proposed by Zhou et al. [56], uses computationally cheap hierarchical surrogate models to reduce the expensive function evaluations.
O	Without considering memory and energy constraints, Benoist and Rottembourg [3], Habet et al. [19– 21] and Lemaître et al. [28] developed general mathematical programming models for EOS scheduling.
P	Liao et al. [31,32], Lin et al. [33–36] and Marinelli et al. [39] proposed the time-indexed formulation of EOS scheduling, and established integer programming models.
O	Lemaître et al. [27] and Verfaillie and Schiex [48] formulated EOS scheduling as constraint satisfaction problems.
O	Agnése and Bensana [1], Bensana et al. [4] and Verfaillie et al. [49] proposed valued constraint satisfaction problem (VCSP) formulations for SPOT-5 satellite scheduling, without considering energy constraints.
O	Knapsack problem: Vasquez et al. [46,47] and Wolfe and Stephen [55] formulated EOS scheduling as 0–1 knapsack problems.
P	Gabrel et al. [13,14] adopted a directed acyclic graph (DAG) model to describe the satellite scheduling problem.
O	Besides, Sarkheyli et al. [43] and Zufferey et al. [57] modeled EOS scheduling as graph coloring problems.
P	Besides, Frank et al. [12] adopted the Constraint-Base Interval (CBI) language to describe the problem.
P	Agnése and Bensana [1] and Bensana et al. [4] proposed depth-first branch and bound algorithms for SPOT-5 satellite scheduling.
O	Also, Benoist and Rottembourg [3], Bensana et al. [4] and Verfaillie et al. [49] suggested Russian Doll search algorithms, which are based on branch and bound but replace one search by n successive searches on nested subproblems, using the results of each search when solving larger subproblems, to improve the lower bound on the global valuation of any partial assignment.
P	Besides, Gabrel and Vanderpooten [14], Hall [22] and Lemaître et al. [28] developed dynamic programming methods to get the optimal solutions of EOS scheduling problems.
P	Agnése and Bensana [1], Bensana et al. [4] and Lemaître et al. [28] proposed greedy algorithms to get feasible solutions for EOS scheduling problems.
P	On the basis of heuristic rules, Bianchessi et al. [6,9], Hall [22], Wang et al. [51–53] and Wolfe and Stephen [55] developed constructive algorithms, which can solve the problem efficiently, without guaranteeing the optimality of the solutions.
P	Bianchessi and Righini [7], Lin et al. [33,36] and Marinelli et al. [39] adopted lagrangian relaxation heuristics to solve the problems, obtaining close-to-optimal solutions.
P	Lin et al. [33–36] formulated the coverage of clouds as a set of covered time windows, and forbade the tasks to be observed in the covered time windows of scheduling.
P	Liao et al. [31,32] considered the uncertainties of clouds, formulated the cloud coverage for each observation window as a stochastic event, and established a model with the objective of maximizing the weighted sum of a function of the profits and the expected number of accomplished tasks.
O	Advertising today accounts for 99% of the revenue”. Google's advertisements generated more than 59 billion dollars in 2014 [16].
P	For example, Google has published two anonymous sets of real-data about their computer clusters [31].
P	Ciocan and Farias [9] have obtained an expected worst case guarantee of 0.342 with the following procedure.
P	In contrast, Feldman et al. [13] and Jaillet and Lu [18] use dual solutions to improve current strategy.
O	Jaillet and Lu [18] also try to infer the total number of requests T (instead of assuming to be known ahead of time).
O	Finally, Van Hentenryck and Bent [32] develop practical algorithms for online stochastic combinatorial optimization.
P	Karp et al. [24] deal with a simple form of this problem maximizing the number of requests matched to buyers.
O	Kalyanasundaram and Pruhs [21] provide a 1 − − competitive e 1 algorithm for the b-matching problem, which is defined as an online bipartite resource allocation problem where each buyer can be matched at most b times.
P	Mehta et al. [28] introduce the Adwords problem and propose a 1 − −competitive e 1 algorithm.
O	Buchbinder et al. [8] show that a primal–dual algorithm can be designed for this problem with the same competitive ratio.
O	Jaillet and Lu [17] propose a 1/2 −competitive primal–dual algorithm for the online bipartite resource allocation problem in a special homogeneous case.
P	Goel and Mehta [15] prove that a greedy algorithm is a 1 − 1/e −competitive algorithm for the AdWords problem.
P	Devanur et al. [11], Agrawal et al. [2], Eghbali et al. [12], Molinaro and Ravi [29], Kesselheim et al. [25] all propose near-optimal algorithms for various settings of the online bipartite resource allocation and online packing problems under the random permutation model.
P	Our work builds upon the primal–dual algorithm presented in Buchbinder et al. [8] and the online stochastic algorithms proposed by Van Hentenryck and Bent [32].
O	Second, we will consider a different model of request arrivals, and assume as in Jaillet and Lu [18], that the distribution governing the arrival times of the requests is known.
O	The second method to estimate the number of requests Tj requires that we change our way to model the problem. Instead of defining a problem by its number of requests T, we propose to solve the online bipartite resource allocation problem over a planing horizon as Jaillet and Lu [18] did.
O	In the general problem case, the dual variables updates of Jaillet and Lu [17] are used for the primal–dual and for the Δ-re-optimized primal–dual algorithms.
O	The number of requests T is now drawn from an exponential distribution as in Jaillet and Lu [18].
O	The work presented by Bilheimer and Grey [6] formally defines the FCNDP-UOF.
P	Both Erkut et al. [12] and Kara et al. [18] work focus on exact methods, presenting a mathematical formulation and several metrics for the hazardous materials transportation problem.
N	At Mauttone et al. [24], not only was presented a different model, but also a Tabu Search for the FCNDP-UOF.
P	Both Amaldi et al. [3] and Erkut et al. [11] presented heuristic approaches to deal with the hazardous materials transportation problem.
P	At last, Gonzalez et al. [14] presented an extension of the model proposed by Kara and Verter [18] and also a GRASP.
P	Introduced by Fischetti and Lodi [13], the Local Branching (LB) technique could be used as a way of improving a given feasible solution.
P	Developed by Lourenço et al. [19], the Iterated Local Search (ILS) is a metaheuristic that applies a local search method repeatedly to a set of solutions obtained by perturbing previously visited local optimal solutions.
P	In order to test the performance of the presented heuristic, we used network data obtained from Mauttone, Labbé and Figueiredo [24].
P	In Tables 1–3 were used 135 instances generated by Mautonne, Labbé and Figueiredo [24], whose results were published by them just for 5 instances.
O	Armbrust et al. [2] gave a definition from an academic perspective: “Cloud computing refers to both the applications delivered as services over the Internet and the hardware and systems software in the data centers that provide those services.”
P	Bakos and Brynjolfsson [5] studied the strategy of bundling distinct information goods and selling them for a fixed price.
O	Sundararajan [25] indicated that fixed-fee unlimited-usage pricing and usage-based pricing schemes should be included in different stages of information markets.
O	As Cheng and Sin [9] noted in their state-of-the-art review of major research results in parallel-machine scheduling problems, various job characteristics, machine configurations and performance criteria are of theoretical interest as well as practical significance.
P	Azizoglu and Kirca [3] presented some dominance properties for Pm T ∥ ∑ j and proposed a branch-and-bound algorithm that can solve instances with up to 15 jobs and 3 machines.
O	Yalaoui and Chu [29] developed more dominance properties and bounding rules, and showed that their branch-and-bound algorithm could obtain optimal solutions in some cases with 30 jobs and 2 machines.
P	Shim and Kim [24] also provided dominance properties and lower bounds to show that the suggested algorithm could find optimal solutions for problems with up to 30 jobs and 5 machines in a reasonable time.
P	As for the uniform parallel-machine problems, Dessouky et al. [10] presented algorithms for different objectives under the strong assumption that the jobs are identical, and proposed a dynamic programming algorithm for minimizing the total completion time subject to release dates.
O	Leung et al. [20] addressed the bi-criteria consisting of one classical and one non-classical objective functions with two different bi-criteria structures in parallel machine scheduling.
P	Lee et al. [18] studied the same problem with the hierarchical bi-criteria structure and developed approximation algorithms with worst-case performance analyses. 
P	A different job processing cost,which is determined by the time slots used by the job, is considered by Wan and Qi [27] for single-machine scheduling.
P	Carroll [8] provided a rule called Cost over Time (COV) for sequencing single- or multiplecomponent jobs.
P	Montagne [21] introduced Montagne's Ratio Rule (MRR) for the minimum total weighted tardiness problem.
O	For single-machine scheduling, Baker and Bertrand [4] presented the Modified Due-Date rule (MDD) which is a combination of the Earliest Due Date first rule (EDD) and the Shortest Processing Time first rule (SPT).
P	Morton et al. [22] designed the Apparent Urgency (AU) rule for the total weighted tardiness minimization on single and parallel machines.
P	Ho and Chang [15] devised the Traffic Priority Index Rule (TPI), which computes the priority indices by the traffic congestion ratio (TCR), for minimizing the mean tardiness. 
P	Customized bundle strategies, as suggested by Wu et al. [28], for clients or customers of different groups with large orders are absolutely necessary for attracting more clients. 
O	This problem was investigated by Richard et al. [23] who used villages, sparsely populated hamlets and some roads in the country side as demand points, some of which also served as potential sites. 
P	The location of a number of health resources such as geriatric and diabetic health care clinics in the rural area of Burgos in Spain was examined by Pacheco and Casado [21] using scatter search.
O	A study to locate a number of bicycle stations in the city of Isfahan, Iran, was conducted by Kavesh and Nasr [17] using harmony search.
P	A real life application that aims to minimise the number of emergency warning sirens in Dublin (Ohio) was explored by Wei et al. [29] who adopted an enhanced Voronoi-based approach to cover the entire area with the minimum number of facilities.
O	A humanitarian aid problem to locate a number of urgent relief distribution centres to help with the casualties due to an earthquake in Taiwan that measured at 7.3 on the Richter scale, and caused over 2500 deaths and 8000 injuries, was recently investigated by Lu [18] using simulated annealing.
P	Elzinga and Hearn [11] proposed an efficient geometrical-based algorithm for solving optimally the problem.
O	Other authors attempted some enhancements to speed up the search, such as Xu et al. [30] and Elshaikh et al. [10] and references therein.
P	There is, however, a relatively small number of authors who have studied the p−centre problem; see Plastria [22] and the references therein. One of the commonly used approaches is based on Cooper's [5] locate–allocate procedure.
P	Drezner [6] presents two methods, namely, a multistart similar to Cooper's locate allocate adapted to the p-centre problem (referred to as (H1)) followed by a composite heuristic made up of H1 and a post optimiser that allocates the critical points between the clusters (called (H2)).
P	Eiselt and Charlesworth [12] propose three constructive and improvement-based heuristics.
O	Their first one resembles the locate–allocate procedure of Cooper, the second uses the vertex substitution of Teitz and Bart [28] with the critical points used for reallocation, and their third one is based on the drop method.
O	Very recently, Elshaikh et al. [10] devise an enhanced version of the Elzinga and Hearn algorithm for the 1-centre problem which is then embedded within a powerful VNS-based heuristic to solve the p-centre problem. 
O	For the case of area coverage, which can be of interest, for example, to agriculture, environment and mobile phone coverage technology, a Voronoi diagram-based heuristic, using an iterative procedure based on the locate–allocate principle, was proposed by Suzuki and Okabe [27].
N	Wei et al. [29] extended the above Voronoi-based approach to account for irregular and nonconvex shapes, including the possibility of forbidden regions where the new facilities cannot be sited.
N	Though the area and the point coverage problems are related, these preceding approaches should not be used directly for point coverage given that the results can be misleading as demonstrated by Murray and Wei [20].
P	Excellent results for both the discrete and the continuous cases are found by Chen and Chen [3] who extended the work of Chen and Handler [4] in several interesting ways.
P	In (b), we adopt the set covering-based approach based on Salhi and Al-Khedhairi [25] that uses the efficient exact method of AlKhedhairi and Salhi [1] with tight upper and lower bounds at the initialisation phase of the binary search.
P	This result is also highlighted by Hansen and Mladenović [15] and Gamal and Salhi [13] for the multi-source Weber problem.
O	We also implemented the optimal method of Drezner [6] where we obtained the optimal solutions for bothn n = = 439 and 575 though the CPU time was excessively large, sometimes in excess of 24 h, especially for n = 575and small values of p, see Table 1 for the summary results. 
P	The optimal solutions for n = 439 were previously found by Chen and Chen [3] using the best of their relaxation methods.
P	For completeness, we have also added the recently published results by the two best variants of VNS given in Elshaikh et al. [10].
O	Also, in general, the proposed perturbation heuristics behave comparably well against the recent VNS based metaheuristics (Elshaikh et al. [10]) while producing over 40% best solutions (21 out of 50).
P	These new centres could be used within the new reformulation local search (RLS) framework (see Brimberg et al. [2]) to increase the set of potential sites, and hence, improve the ability to find better solutions in the continuous space.
P	The proposed perturbation methodology can be extended to very large continuous problems where little work has been done as highlighted in Irawan and Salhi [16]. 
P	We also believe that the optimal method of Drezner [6] and the relaxation-based technique of Chen and Chen [3], both have scope for improving their implementations, and hence, could be worth revisiting.
O	The proposed perturbation heuristic significantly outperforms some known composite heuristics and obtains comparable results when compared to those powerful metaheuristics recently given in Elshaikh et al. [10].
P	Our aim with this paper is to contribute to the efficient resolution of a specific integrated planning and scheduling problem studied recently by Kis and Kovács [15].
P	The work described herein is original in the sense that it proposes a new detailed scheduling model based on a different pseudo-polynomial formulation that we solve through a full-space method according to the taxonomy introduced by Maravelias and Sung [18].
P	Maravelias and Sung [18] divided the modeling approaches proposed in the literature for integrated planning and scheduling problems into three groups.
P	An example of the former can be found in [14] for production planning in petroleum refineries. Joly et al. [14] formulated nonlinear and mixed integer programming models, and solved them directly by using optimization solvers. 
O	A monolithic formulation was explored recently by Kis and Kovács in [15].
O	The approach described herein is comparable to the approach of Kis and Kovács [15] in the sense that we propose a detailed scheduling model based on a monolithic formulation, relying essentially on scheduling variables.
O	The work described by Amaro and Barbosa-Póvoa [2] illustrates this type of approaches. Rolling horizon methods are considered in [18] as hierarchical decomposition approaches.
O	This approach was followed in [15], for example. In [16], Li and Ierapetritou used a similar approach.
P	The third group of solution methods identified by Maravelias and Sung [18] are the so-called full-space methods that consist in solving directly the complete detailed scheduling model.
O	This resolution can be based on standard methods as in [14] or on heuristics as in [27].
O	As referred above, the integrated planning and scheduling problem studied herein is similar to that described recently by Kis and Kovács [15].
O	For example, Carlier and Néron [7] dealt with the resource-constrained project scheduling problem, which consists in scheduling optimally a set of non-preemptive activities requiring variable amounts of a certain resource.
P	More recently, Su et al. [22] solved a scheduling problem on machines with different speeds and limited availability with a view to minimizing either the makespan or the maximum lateness.
P	Detienne [10] solved several variants of the scheduling problem on machines that might have been unavailable at given time periods. 
P	Sadykov and Wolsey [20] explored a scheduling problem where both the processing times and the costs of jobs depended on the machine that was being used.
P	Yang et al. [28] used an enhanced genetic algorithm to solve a job shop scheduling problem with release dates, due dates, deadlines and costs associated with the earliness and lateness of the jobs.
P	For ease of presentation, we will refer to these approaches as monolithic, hierarchical H1 and hierarchical H2, as Kis and Kovács did [15].
P	Bettinelli et al. [1] provides a good overview on course timetabling problems, and L¨ubbecke [17] gives some further comments regarding practical and implementation issues.
P	Lewis [15] proposes a three-phase heuristic that uses Simulated Annealing (SA) for the last two in order to improve the generated schedule.
P	Jat and Yang [12] propose a two phase approach using Genetic Algorithms and Tabu Search. 
P	Chiarandini et al. [9] propose a heuristic based on stochastic local search.
P	Ceschia et al. [6] perform an extensive study for the PECTP by considering several variations of the problem and propose a metaheuristic based on SA.
P	Nothegger et al. [21] propose an ant colony optimization algorithm. 
P	Cambazard et al. [4] study a wide variety of approaches, including Constraint Programming (CP) and a list-coloring relaxation of the PECTP.
P	Finally, van den Broek and Hurkens [23] propose an ILP based heuristic for the problem. 
O	A different perspective of the problem is addressed in van den Broek et al. [24], where the authors study a real timetabling problem at TU Eindhoven.
P	Carter [5] describes the characteristics of the problem and provides the details of the scheduling system developed at the University of Waterloo.
P	Murray et al. [20] and Muller and Murray [19] build upon this research, mainly using the method proposed by Carter [5] to construct the initial timetable.
O	Muller and Murray [19] tackle the overall problem, generating the initial timetable as in Carter [5] and providing further developments, including several new local search operators, for the student sectioning stage. 
O	In a follow up paper, Ceschia et al. [7] consider a generalization of this problem.
P	Sorensen and Dahms [22] propose a two stage decomposition heuristic based on an ILP formulation for a High School Timetabling Problem.
P	Kristiansen et al. [13] consider an ILP for the High School Timetabling Problem, which is solved in two stages by means of a general purpose solver, but in this case resulting in an exact algorithm.
O	Finally, Daskalaki and Birbas [10] consider a university timetabling problem where groups of students are enrolled in a set of courses, similarly to the PECTP.
P	Firstly, we study a problem with a direct and practical application that integrates, combines and tackles jointly two other problems from the related literature, namely the PECTP and the problem defined in van den Broek et al. [24].
O	Similarly to Carter [5], the first stage defines an assignment of classes to time-slots and then, based on these settings, the second stage assigns the students to the classes.
P	Firstly, Carter [5], once the problem is partitioned into smaller subproblems, generates only one timetable based on a pre-assignment of students to sections.
O	Compared to the related literature, Carter [5] tackles these two problems sequentially: first the classroom assignment and then the automated student sectioning.
P	A similar observation can be made regarding the approach in Muller and Murray [19], where the timetabling of classes and the assignment of classes to rooms is taken from the initial sectioning stage, although they propose several local search operators to find improved (feasible) solutions.
P	As mentioned in the introduction, the GPECTP considers simultaneous characteristics from the PECTP and from the research in van den Broek et al. [24].
P	Therefore, in the list-coloring approach proposed by Cambazard et al. [4] the set of events chosen by a student define, as they name it, a student clique.
P	Ahmadi et al. [1] investigated one such example in the manufacturing industry: a manufacturer producing semifinished lenses and competing globally.
O	Moreover, Biskup [3] claimed that learning primarily occurs because of repeated processing timeindependent operations, such as assembling, controlling, and operating machines and processing data.
O	Sung and Yoon [30] demonstrated that this problem is nondeterministic polynomial-time (NP) hard.
P	Wang and Cheng [36] and Leung et al. [16,19,21] have proposed heuristic algorithms for obtaining near-optimal solutions, and Yoon and Sung [52] developed a branch-and-bound algorithm for obtaining the optimal solution.
P	Ahmadi et al. [1] too showed that the order scheduling problem to minimize the total completion time is NP-hard and developed heuristic algorithms for it.
P	Considering the due dates as criterion, Leung et al. [17] addressed the order scheduling problem to minimize the maximum lateness (i.e., the number of tardy orders) and established heuristic algorithms.
O	Lee [15] considered the order scheduling problem to minimize total tardiness.
O	Koulamas and Kyparisis [13] and Kuo and Yang [12] modified Biskup’s [3] learning model for increased practicability; the learning occurs not because of repeated processing time-independent operations, such as setups, but because of repeated production activity.
O	In addition, relevant studies on time-dependent processing are available in [47,50], for two special issues, [48], for a deterioration model in which the actual processing time of a job depends not only on the starting time of the job but also on its scheduled position, and Wang and Wang [35,37] and Wang and Zhang [38], for flowshop scheduling with a learning effect.
P	PSO, refer Shi and Eberhart [28], has been widely used to solve combinatorial optimization problems. 
P	In addition, Lee [15] proposed four heuristic algorithms for the same problem without learning consideration and claimed that the order-scheduling MDD (OMDD) heuristic is the most effective among the four heuristics.
O	More specifically, the problem under consideration is the PFSP to minimise total earliness and tardiness, which is denoted as Fm prmu E T | | ∑ j j + ∑ according to the notation in e.g. Pinedo [26].
P	Thereby, several algorithms have been proposed for the problem, such as those by Zegordi et al. [42] and by M'Hallah [19]. 
O	Regarding flowshop scheduling problems with E/T objective without insertion of idle times, Moslehi et al. [20] propose an optimal algorithm for the PFSP with two machines to minimise the sum of maximum earliness and tardiness.  
P	Different branch-andbound algorithms are developed by Madhushini et al. [18] for several multi-objective functions including total earliness and tardiness.
P	Zegordi et al. [42] was first in proposing an approximate algorithm (a simulated annealing algorithm) to solve the PFSP to minimise the sum of weighted earliness and tardiness.
P	Schaller and Valente [35] propose a genetic algorithm (GA) that outperforms several metaheuristics for related problems, also yielding favourable results regarding the CPU times when compared to the algorithm proposed by Zegordi et al. [42].
P	Finally, M'Hallah [19] proposes an Iterated Local Search (ILS) where a variable neighbourhood descent is iteratively repeated after a perturbation mechanism.
P	The NEHedd is an adaptation of the NEH heuristic, originally proposed by Nawaz et al. [22] for the PFSP to minimise makespan.
P	Regarding the characterisation of the problem depending on the due dates of the instance, Bagchi et al. [1] have divided the single-machine E/T problem with common due date in two different single-machine problems depending on the common due date. 
O	Chandra et al. [3] have extended the argumentation for the PFSP with common due dates and classify the problem into three different cases.
O	Fernandez-Viagas and Framinan [6] have divided the Fm prmu T | | ∑ j with different due dates for each job in four areas depending on the means and variances of the due dates, each area representing a different optimisation problem.
O	Nevertheless, Li et al. [16] take advantages of the invariance of the completion times of jobs prior to the insertion position for the PFSP to minimise flowtime,so saving between 30–50% of CPU time are achieved.
P	Fernandez-Viagas and Framinan [7] designed an efficient constructive heuristic following a similar procedure of insertion in last position of the partial sequence.
P	Similarly to the speed up methods proposed by Li et al. [16] and Vallada and Ruiz [39], this method stores the completion time of each job on each machine of the partial sequence Πk.
O	These due dates are generated using the procedure described by Potts and Van Wassenhove [27], i.e. following a uniform distribution between P TR ·( − − 1 /2) and P TR ·( − + 1 /2), where P is a lower bound for the makespan.
O	This benchmark is composed of a set of 540 instances of Vallada et al. [40] (available in http://soa.iti.es) and is the most extended benchmark for the PFSP with due dates.
P	Raj: Adaptation of the Raj heuristic by Rajendran [29], originally proposed for the Fm prmu C | | ∑ j problem.
P	RZ: Adaptation of the RZ heuristic by Rajendran and Ziegler [30] proposed for the Fm prmu C | | ∑ j problem, with the initial order replaced by the EDD rule.
P	RZ_LW: Adaptation of the RZ_LW heuristic by Rajendran and Ziegler [30], originally proposed for the Fm prmu C | | ∑ j problem.
P	FRB4k: Adaptation of the FRB4k heuristic by Rad et al. [28], originally proposed for the Fm prmu C | | max problem.
O	However, a direct comparison between both indicators presents several problems related to the weight of each problem size, as shown by Fernandez-Viagas and Framinan [7].
O	In this Section, they are included as initial solution for one of the best metaheuristic for this problem, i.e. the ILS by M'Hallah [19], replacing the original seed sequence of the metaheuristic (EDD rule).
P	Therefore, we propose a reformulation of the problem obtained through Dantzig–Wolfe decomposition [7].
O	Therefore, the pricing problem is a variant of the 0–1 Knapsack Problem (KP) [18].
O	Pricing implementation: Our Phase 2 branching strategy may resemble that of Ryan–Foster [22]; however, the feature of always matching head items with non-head ones allows us to mitigate the main drawback of Ryan–Foster, that is to radically break the structure of the pricing problem.
P	The scheduling problem in such automated manufacturing systems is referred to as hoist scheduling problem in the literature and was proved to be NP-hard by Lei and Wang [3].
O	In 1976, Phillips and Unger [4] first formulated the problem as a mixed integer programming (MIP) model, where a single hoist repeatedly carries out a fixed hoist move sequence and the system returns to the initial status when this fixed sequence is completed. 
O	Kats and Levner [21] proposed a strongly polynomial algorithm for extended electroplating lines with fixed part processing times.
O	Liu et al. [18] formulated the problem as a comprehensive MIP model where the reentrant workstations and parallel workstations were considered together.
P	Che and Chu [19] developed an efficient branch and bound (B&B) algorithm to solve large-size problems.
P	Kuntay et al. [23] studied the cyclic hoist scheduling problem with both the economic and environmental objectives.
P	Subaï et al. [24] also considered the environment cost in their biobjective hoist scheduling problem where the non-linear environment cost as a function of the processing times is formulated.
P	Recently, Feng et al. [17] proposed a biobjective MIP model for the hoist scheduling problem to minimize the cycle time and the hoist traveling time simultaneously.
N	Second, an efficient hybrid DDE is proposed to deal with the mid- and large-size problems that cannot be solved by existing ε-constraint method proposed by Feng et al. [17].
N	With a given sequence X, its corresponding cycle time T(X) and cyclic schedule can be obtained by solving Problem P(X) using a graph-based polynomial algorithm proposed by Chen et al. [9], denoted as CCP algorithm in short.
P	Wang et al. [27] proposed a novel hybrid DDE to solve blocking flow shop scheduling problems.
O	Pan et al. [29] applied DDE for the permutation flowshop scheduling problem.
O	Pan et al. [25] applied a novel differential evolution algorithm to tackle a bicriteria no-wait flow shop scheduling problem.
O	Recently, Balaraju et al. [36] addressed multi-objective flexible job shop scheduling problems using DDE to minimize makespan, total machine load and critical machine load simultaneously. 
P	Let Kmax be the maximal level of WIP of the problem, which can be calculated by a polynomial algorithm proposed by Che and Chu [19] and k(X) be the corresponding WIP level of a hoist move sequence X, such that 0rk(X)rKmax.
O	For the problem without the reentrance, Yan et al. [33] gave the following three rules to construct a move sequence with a given WIP level k, k¼1, 2, …, Kmax.
O	In the first one, the move sequences of the selected individuals are directly operated by the discrete mutation and crossover operators proposed by Zhang et al. [32].
N	As the existing ε-constraint method does not consider the reentrant workstations, we first extend the model by adding two additional constraints formulated by Liu et al. [18] and solve the extended model by optimization software package CPLEX (V12.50) to obtain the Pareto optimal solutions.
O	The data for these benchmark instances can be found in Phillips and Unger [4], Leung et al. [39] and Manier and Lamrous [40]. 
P	The first five instances are without the reentrant workstations and their complete Pareto optimal fronts were first reported by Feng et al. [17].
P	Data envelopment analysis (DEA) was first introduced by Charnes et al. [10], and has been widely used in performance or productivity evaluation.
P	Furthermore, Banker and Chang [7] reported that Andersen and Petersen's [4] procedure using the super-efficiency scores for ranking efficient observations had poor performance.
P	The earliest work on anti-efficient frontier can be traced to “Inverted” DEA model proposed by Yamada et al. [30].
O	Paradi et al. [22] used DEA and Inverted DEA models, which are so called “Worst Practice DEA” in the paper, to identify the worst practices in banking credit analysis.
O	Entani et al. [14] employed both DEA and Inverted DEA models to obtain the upper and lower bound of interval efficiency of DMUs.
O	Cao et al. [9] uses the evidential-reasoning (ER) approach to construct a performance indicator for combining the efficiency and anti-efficiency obtained by DEA and inverted DEA models.
P	Zhou et al. [33] used the DEA model without explicit inputs (see, e.g., [19,20,31]) to combine the efficient and anti-efficient measures to rank the DMUs.
O	The data set of this example comes from Zhu [34], which is shown in Table 3.
P	Furthermore, we compare the results with those from the method proposed by Amirteimoori [3], which defines new combined efficiency measures based on the weighted L1–distances to both efficient and anti-efficient frontiers to rank DMUs.
P	Although traditionally DEA was regarded as non-statistical approach, Banker and Natarajan [8] provided a coherent Data Generating Process (DGP) for the two-stage analysis based on DEA.
O	Following the line of Banker and Natarajan [8], we employ two similar data generating processes in our simulation study.
P	In a different direction, Kristiansen et al. [11] proposed the first integer programming formulation for XHSTT timetabling problems.
P	Sorensen and Stidsen [15] presented some preliminary results in matheuristics for XHSTT timetabling problems.
O	The most recurrent classifications are High School Timetabling [16], University Course Timetabling [17] and Student Sectioning [18].
N	Detailed description of this format can be found in Post et al. [3] and Kingston [19].
P	The original Variable Neighbourhood Search algorithm was proposed by Mladenovic and Hansen [21].
P	This procedure is similar to the one proposed by Tuga et al. [24].
P	The IP formulation considered in this work was recently proposed by Kristiansen et al. [11].
P	The main idea of the algorithm is similar to the one proposed in Sorensen and Stidsen [15].
O	Other possible future works are (i) to improve the existing mathematical programming formulation for XHSTT and (ii) to make a deeper study about neighbourhood sizes and ILP based procedures, such as Local Branching [29] and Relaxation Induced Neighbourhoods (RINS) [30].
O	Swiler et al. [39] later proposed an automatic attack graph generation tool. Since then, many researchers have studied attack graphs and proposed a multitude of attack graph variations including attack trees [43], attack countermeasure trees [35], defense trees [9], and exploit dependency graphs [30].
P	For a detailed review of many other attack graph variations, refer to the studies by Lippmann and Ingols [23] and Kordy et al. [21].
P	Albanese et al. [1] propose a formal cost model that estimates the cost if a critical node is breached.
P	Alhomidi and Reed [3] use a genetic heuristic to find the minimum cut set in dependency attack graphs.
P	The study by Jajodia et al. [19] is another one that uses arc removal for network hardening.
P	Dewri et al. [12] propose a multi-objective optimization model to select a subset of security-hardening measures that minimizes the total damage and the total cost of security hardening.
P	Noel and Jajodia [29] develop a mechanism to cover (i.e., completely protect) an attack graph by placing the fewest number of Intrusion Detection Systems (IDS) sensors.
O	From the modeling perspective, our work is most closely related to the work by Pan and Morton [31]; in both their work and our work, the network is interdicted to prevent some flow from reaching a set of destinations.
P	However, Pan and Morton [31] interdict arcs in a nuclear material smuggling network to minimize a stochastic maximum reliability path, and our work interdicts arcs in a deterministic attack graph to minimize the maximum damage that can be caused through breach of critical assets (goal nodes) by an attacker.
O	Related to our work, Dewri et al. [13] propose a multi-objective optimization model that poses the interaction between the attacker and the defender as an arms race. 
P	Zonouz et al. [46] propose a new automated response approach called the Response and Recovery Engine.
P	Alderson et al. [2] provide a comprehensive discussion on the modeling and algorithmic strategies for quantifying the resilience of infrastructure systems to disruptive events. 
P	Our algorithm is based on the framework proposed by Brown et al. [10] and subsequently generalized by Alderson et al. [2].
P	Thus, to generate the lower bound, we adopt a model (MINATRISK) from a study by Nandi and Medal [27] on interdicting networks to prevent the spread of infections.
P	The fist LARP application was presented by Levy and Bodin [13] in 1989 to find the best location for postal carriers to park their vehicles so that they can perform a tour to deliver mail.
P	Ghiani and Laporte [7] approached the Location Rural Postman Problem (LRPP) by transforming it to a Rural Postman Problem (RPP) when there are no bounds on the number of depots and by using a branch and cut method to solve it.
O	Ghiani and Laporte [8] reviewed the common applications of LARP, such as mail delivery, garbage collection and street maintenance. 
P	This method was proposed by Levy and Bodin [13] for a mail delivery application.
P	The next two steps, allocation and routing, are based on the initial solution of the algorithm proposed by RiquelmeRodr´ıguez et al. [24] for the PCARP with inventory constraints.
P	The benefits of coordinating plants in other industries have been proven, especially when production and distribution costs are significant and inter-related (Dhaenens-Flipo and Finke [2]).
P	The annual beverage industry production budget has also motivated the integration of both processes in Guimaraes et al.[3].
N	The integration of production and distribution in a single decision model yields better results than optimizing separate models as shown in Chandra and Fisher [4].
O	There are several literature reviews on lot sizing and scheduling problems, such as Drexl and Kimms [5], Karimi et al. [6] and Robinson et al. [7].
O	A review on modeling issues for industrial lot sizing problems is presented in Jans [8] and another on metaheuristics applied to lot sizing problems in Jans and Degraeve [9].
P	Recent research has focused on problem-oriented methods (James and Almada-Lobo [13]), such as relax-and-fix (Beraldi et al. [14]), fix-and-optimize (Sahling et al. [15]) and Hamming-oriented partition search heuristics (Camargo et al. [16]).
P	For instance, Araujo et al. [17] apply the RF constructive heuristic to solve a real-world lot sizing and scheduling foundry problem.
P	A similar approach is applied by Toso et al. [18] and Clark et al. [19] for a problem in the animal nutrition industry, and by Ferreira et al. [20] and Ferreira et al. [21] for a soft-drink production problem.
P	This approach was applied by Sahling et al. [15] and Helber and Sahling [22] to solve variants of the multi-level lot sizing and scheduling problem.
P	Metaheuristic approaches can be applied to deal with this complexity and improve the quality of the solution within reasonable computational time (Boussaid et al. [23]).
P	Among these approaches, evolutionary computation has been widely applied (Goren et al. [24]).
P	A survey of evolutionary methods is presented in Gen et al. [25] for integrated manufacturing systems.
P	This population structure was introduced in Franca et al. [26] to solve the total tardiness single machine scheduling problem.
P	The group scheduling problem for manufacturing cells in Moghaddam et al. [27] is also solved using a GA and a memetic algorithm with individuals structured in ternary trees.
P	The two-level soft drink production problem is solved in Toledo et al. [28] with a multi-population GA where individuals are hierarchically structured in ternary trees.
P	A hybrid multi-population GA is applied to solve the short-term single-facility glass container problem in Toledo et al. [11] as previously mentioned.
O	Seeanner et al. [30] cross-fertilize the principles of the Variable Neighborhood Decomposition Search with those of the MIP-based Fix-and-Optimize heuristic. 
P	In Toledo et al. [31] and Toledo et al. [32], a GA is combined sequentially with the Fix & Op heuristic to solve the standard multi-level capacitated lotsizing problem with backlogging.
O	We note that the work Almada-Lobo et al. [1] also addresses the long-term MF-GCPP.
O	First, the authors in Almada-Lobo et al. [1] ignore the distribution decisions (transfers between plants).
O	Second, the duration of each color campaign was forced in Almada-Lobo et al. [1] to be multiple of entire days.
O	One way to address this issue is robust optimization where the constraints involving random parameters are satisfied for all realizations of the random events (see, e.g., Soyster [29], Ben-Tal and Nemirovski [5]).
P	For a comprehensive overview on robust optimization, we refer the reader to the book by Ben-Tal et al. [4], the survey by Gabrel et al. [12] and references herein.
P	For instance, Yu and Yang [33] studied the robust shortest path problem in a layered network under two robustness criteria; they proved that the problem is NP-complete and devised a pseudopolynomial algorithm.
P	Gabrel et al. [13] proposed an integer linear program formulation for the studied robust shortest path and analyzed the theoretical complexity of the resulting problems.
N	Provan [25] and Polychronopoulos and Tsitsiklis [26] studied expected shortest paths in networks where information on arc cost values is accumulated as the graph is being traversed, while Nikolova [23] maximized the probability that the path length does not exceed a given threshold value.
P	Nie and Wu [22] studied the problem of finding a priori shortest paths to guarantee a given likelihood of arriving on-time in a stochastic network and also provided a pseudo-polynomial approximation based on extreme-dominance.
P	Sen et al. [27] formulated a network flow multiobjective model where one objective function consists in minimizing the expected travel-time between given origin and destination nodes whereas the second objective function minimizes the variance of travel-time. 
O	Miller-Hooks and Mahmassani [17] addressed the problem of determining least expected time paths in stochastic, time-varying networks where the arc weights arc travel times) are random variables with probability distribution functions that vary with time. 
O	Xing and Zhou [32] investigated a fundamental problem of finding the most reliable path under different spatial correlation assumptions, and a Lagrangian substitution approach is used to get a lower bound.
P	Fu and Rilett [11] studied a dynamic and stochastic shortest path problem to come-up with the expected shortest path in a traffic network where the link travel times are modeled as a continuous-time stochastic process, and proposed a heuristic algorithm based on the k-shortest path algorithm.
O	In a recent paper, Mokarami and Hashemi [19] considered both robust and stochastic versions of the constrained shortest path problem, where an uncertain transit time was associated to each arc in addition to the arc cost. 
O	Delage and Ye [9] have previously studied the distributionally robust approach; they gave an equivalent deterministic formulation which we apply hereafter to DRSSPP
P	Garey et al. [11] showed that the problem of minimizing total flow-time with more than one machine belongs to the category of NP complete problems.
P	A comprehensive review of research on flowshop scheduling that appeared during the last 50 years is the one of Gupta and Stafford [14].
P	A review of scheduling problems that aim on minimizing makespan can be found in Ruiz and Maroto [28] and Gupta et al. [13]. 
O	The Permutation Flowshop Scheduling Problem with the objective of flow-time minimization was reviewed by Pan and Ruiz [22] and Framinan et al. [8], where the latter also reviewed works that consider makespan minimization.
O	Yagmahan [18] recently reviewed multi-objective flowshop scheduling problems
P	Framinan et al. [10] provided a framework to categorize heuristic algorithms for the Permutation Flowshop Scheduling Problem (PFSP) according to their structure.
O	Framinan et al. [9] categorized existing heuristics, which can address one or more of these phases, into two classes: simple and composite heuristics.
P	Composite heuristics are heuristics that contain at least one simple heuristic for conducting one or more of the three above-mentioned phases. Pan and Ruiz [22] showed that composite heuristics outperform simple heuristics in minimizing flow-time. 
P	A popular simple heuristic for minimizing makespan in the general PFSP was presented by Nawaz et al. [19] (we refer to this heuristic as NEH in the following), which outperformed other algorithms developed earlier, such as the heuristics of Palmer [20], Gupta [12], or Campbell et al. [2].
P	One example is the work of Framinan et al. [7], which tried to improve the performance of NEH for three objectives (i.e., makespan, idle time and total flow-time minimization) by applying 177 new ordering policies to the sorting phase of NEH.
P	A similar idea was presented by Rajendran [24], who optimized partial sequences by exchanging adjacent jobs pairwise with the objective to minimize total flow-time. 
P	Framinan and Leisten [6] combined this idea with NEH and performed pairwise exchanges at the end of each iteration to improve partial sequences.
P	Framinan et al. [9] evaluated different heuristic algorithms for the PFSP and concluded that the FL heuristic led to better solutions for the total flow-time criterion. 
O	Laha and Sarin [17] extended FL by allowing all jobs assigned to a partial sequence to change their respective position by checking all other k − 1 slots at the end of each iteration. 
O	Pan and Ruiz [22] reviewed the most promising constructive heuristics and indicated that LS is the best existing simple heuristic to minimize total flow-time in general PFSPs in terms of the quality of the results.
P	Framinan et al. [8] showed that minimizing total flow-time in a PFSP by using a modified version of the NEH heuristic, with jobs sorted in an ascending order of their total processing times, performs better than the original NEH.
O	Framinan et al. [7] named six attributes of the NEH heuristic that offer rooms for extensions:
O	Employ different indicator values in Step 1 (e.g. as by Framinan et al. [7]).
O	Choose a different sorting criterion in Step 2 (e.g. as by Framinan et al. [7]).
O	Restrict the insertion of job k in Step 4 to a subset of the k possible positions (e.g. as by Rajendran [24]).
O	Insert multiple unscheduled jobs into a subsequence in each iteration (e.g. as by Woo and Yim [32]).
O	Keep more than a single sequence within the iterations of Step 4 (e.g. as by Jena et al. [33]).
N	In addition to the suggestions of Framinan et al. [7], we propose the following additional options for extending NEH:
P	Following the notation of Framinan et al. [7], Heuristic/α/β describes an algorithm where α denotes the indicator value and β the sorting criterion employed.
N	Since Rajendran [24], Framinan et al. [8] and Laha and Sarin [17] used an ascending order of the indicator values in their algorithms for the PFSP with total flow-time objective, we restrict our analysis to the same sorting rule. 
N	It is important to note that TWFT proposed here is different from the decision criterion used by Rajendran and Ziegler [25].
O	In our heuristic, the weights of the flow-times are only determined by the position of the job in the current partial sequence, while in the heuristic of Rajendran and Ziegler [25], it is determined by considering holding costs in addition, which are part of their problem formulation.
P	As mentioned before, Framinan et al. [8] showed that using a modified version of the NEH algorithm with jobs sorted in ascending order leads to better results.
O	The number of jobs (the size of the problems) is either 20, 50, 100, 200 or 500, while the number of machines ranges from 5 to 20. Taillard's data sets have frequently been used in almost all PFSP papers to compare heuristics [21], for example by Reza Hejazi and Saghafian [27], Reisman et al. [26], Ruiz and Maroto [28], Pan and Ruiz [22], and Fernandez-Viagas and Framinan [5].
P	In a study of the literature, we found that ARPDs reported for identical heuristics applied on the same data set vary, e.g. in Pan and Ruiz [22] vs. Laha and Sarin [17] and Dong et al. [3] vs. Semančo and Modrák [29].
P	As in Pan and Ruiz [21] and Jarboui et al. [15], taking the best-known objective values from the literature is one alternative (in the following denoted as ARPDL).
P	Pan and Ruiz [22] and Fernandez-Viagas and Framinan [5] used this option in their computational experiment.
O	For example, both Pan and Ruiz [22] and Fernandez-Viagas and Framinan [5] considered some common heuristics in their computational analyses (i.e. Raj, LR(1), LR-NEH(5,10), ICi(i¼1,2,3) and PR(1,10,15)). 
P	The reported ARPDP-values for all common heuristics are higher in the paper of Pan and Ruiz [22].
P	This reveals that the best obtained solutions by the heuristics considered in Pan and Ruiz [22] are better than the best ones obtained by the heuristics of the other paper.
O	Thus it has attracted considerable attention ever since the newsvendor problem was pioneered by Arrow and Harris [2] and Morse and Kimball [21].
O	Several extensions of the newsvendor model are discussed, for example, in Khouja [13] and Qin [22].
P	Scarf [23] first develops the distribution-free method to deal with the difficulty about unavailable demand.
P	Gallego and Moon [7] propose a simpler proof for optimization of Scarf's ordering rule and extend the analysis by considering the factors of resource, fixed ordering and so on.
P	In a subsequent paper, Moon and Choi [19] extend the result of Gallego and Moon [7] to the situation in which customers may balk if the available inventory level is low.
O	Moon and Choi [20] examine the newsvendor problem with an integrated view, beginning with the raw materials to the final product stage.
O	Alfares and Elmorra [1] extend the results obtained by Gallego and Moon [19] by incorporating a shortage penalty cost.
O	Liap et al. [18] present distribution-free method for the classical single period newsvendor problem with possible customer balking and a linear lost sales penalty. 
P	Assuming that the probability density function of demand is given for each period, Keisuke [12] provides ordering plan that maximizes the expected profit. 
P	Bensoussan et al. [4] propose an estimation method based on observed demands using dynamic programming and probability theory.
O	Levi et al. [16] use the Monte Carlo simulations to estimate the demand distribution for both single-period and multi-period newsvendor problems.
P	Based on the assumption that newsvendor's environment is stationary, Levina et al. [17] apply an online learning method of prediction with expert advice (Weak Aggregating Algorithm, WAA [9]) to propose online ordering policy. 
P	Based on their result, Zhang et al. [27] adopt the switching idea to study the non-stationary newsvendor problem.
P	Recently, Kim et al. [14] extend the progressive hedging method to solve the multi-period newsvendor problem.
P	Levina et al. [17] and Zhang et al. [27] have applied it to study the single-product, multi-period newsvendor problem.
P	Thus, our model cannot be considered as two independent single-product models, as studied in Levina et al. [17] and Zhang et al. [27].
P	For the single-product, multi-period newsvendor problem, the online ordering policy obtained by applying the WAA to the fixed ordering quantity is proposed by Levina et al. [17].
O	Reviewing existing literature on paint batching problem, Epping and Hochstättler [5] attempted to solve the color batching problem in selectivity banks (an application of the re-sequencing problem for M-to-1 systems) by proposing a dynamic programming (DP) approach exploiting similarities between color batching and multiple sequence alignment.
P	Spieckermann et al. [20] proposed branch-and-bound algorithms to solve the same problem.
O	Escudero [6] defined a SOP as an asymmetric Hamiltonian path problem with precedence constraints.
O	Ascheuer [1] proved that the SOP is nondeterministic polynomial-time hard (NP-hard) and closely related to the asymmetric traveling salesman problem with precedence constraints
P	In particular, Chen and Smith [3] proposed a genetic algorithm (GA) solution, and Gambardella and Dorigo [9] applied an ant colony optimization (ACO) algorithm to the SOP. 
O	More detailed explanations of these algorithms are provided by Fox et al. [8] and Ying and Lin [22]. 
P	Lin et al. [15] formulated a 0–1 integer programming model for the color batching problem in selectivity banks and developed a nested ACO algorithm.
P	Recently, Sun et al. [21] also studied the color batching problem in selectivity banks.
P	They compared their algorithms (the arraying heuristic for 1-to-M conveyor systems and the shuffling heuristic for M-to-1 conveyor systems) and proved that their algorithms are comparable with both an exact branch-and-bound algorithm proposed by Spieckermann et al. [20] and a nested ACO algorithm developed by Lin et al. [15] with much quicker (less than 0.4 s) computation time.
P	Murata et al. [19] showed that a two-point crossover operation is effective for the flow shop scheduling problem.
O	An indication of its difficulty is given by the fact that the famous 10-job 10-machine instance formulated for the first time by Muth and Thompson [1] was exactly solved by Carlier and Pinson [2], with a branch and bound algorithm that required about 17895 seconds (about 5 hours) of computing time on a PRIME 2655 computer.
P	With regard to a literature review of JSS, Jain and Meeran [3] had provided a comprehensive overview of the progress, the techniques and the researchers involved in the classical JSS problem in 1990.
O	For example, Zhang et al. [4] developed a Tabu Search (TS) metaheuristic with delicate neighbourhood structure and fast evaluation strategies for JSS. 
P	Furthermore, Zhang et al. [5] developed a hybrid metaheuristic by combining Simulated Annealing (SA) with TS to obtain high-quality JSS solution within reasonable CPU times.
P	Udomsakdigool and Kachitvichyanukul [6] developed an Ant Colony Optimisation (ACO) algorithm with several specific features designed for JSS.
P	Huang and Liao [7] developed a hybrid metaheuristic for JSS by combining ACO and TS mechanisms. 
O	Rego and Duarte [8] developed an efficient algorithm for JSS by combining the Shifting Bottleneck Procedure (SBP) algorithm with a Filter- &-Fan search procedure. 
P	Hasan et al. [9] developed a memetic algorithm (MA) with some priority rules for solving JSS.
O	Lin et al. [10] developed a new Particle Swarm Optimisation (PSO) algorithm with a multi-type individual enhancement scheme for JSS.
P	Yin et al. [11] developed a discrete Artificial Bee Colony (DABC) algorithm for JSS.
P	Nasiri and Kianfar [12] developed an efficient TS metaheuristic with six types of neighbourhood moves for JSS.
P	Peng et al. [13] presented an efficient TS metaheuristic with a path relinking procedure for JSS. 
P	Amirghasemi and Zamani [14] developed a fast genetic algorithm with the use of elite pool to obtain the optimal solutions of some well-known benchmark JSS instances within several seconds.
P	An initial survey for this research area was given by Hall and Sriskandarajah [19].
O	Regarding the solution techniques, Mascis and Pacciarelli [20] studied the BJSS and NWJSS problems by means of alternative graph, which is an extension of the classical disjunctive graph. 
P	Furthermore, Meloni et al. [21] presented a rollout metaheuristic for the BJSS and NWJSS problems. 
P	Mati et al. [22] investigated a multi-resource BJSS problem, in which every operation simultaneously requires several resources such that deadlocks can be avoided.
P	Brucker and Kampmeyer [23] proposed a tabu search (TS) metaheuristic algorithm for a cyclic BJSS problem.
P	Gröflin and Klinkert [24] devised a TS metaheuristic algorithm based on an extended graph model to solve BJSS efficiently.
P	Samarghandi and ElMekkawy [25] developed a genetic algorithm to solve a two-machine NWJSS problem with a single sever and setup times.
P	Pranzo and Pacciarelli [26] developed an iterated greedy algorithm to solve to two variants of BJSS with swap allowed and without swap allowed.
P	The idea of FJSS was first introduced by Brucker and Schlie [27] has become an entity of intensive research due to its usefulness in real-world production environment.
P	Brandimarte [28] proposed a two-level hierarchical approach for FJSS by tackling machine assignment and operation scheduling separately.
P	Pezzella et al. [29] presented an efficient genetic algorithm with different selection and reproduction mechanisms for FJSS.
P	Gao et al. [30] used the special chromosome structure and advanced operators to develop a hybrid genetic and variable neighbourhood descent algorithm for FJSS.
P	Xing et al. [31] developed a Knowledge-Based ACO algorithm for FJSS by providing an effective integration between ACO and knowledge model. 
P	Bozejko et al. [32] developed a double-level parallel metaheuristic approach with machine selection module and operation scheduling module for solving FJSS.
P	Zhang et al. [33] solved FJSS by developing an effective genetic algorithm based on an improved chromosome representation and diverse crossover and mutation operators.
P	Yuan and Xu [34] developed a hybrid evolutionary-based memetic algorithm with harmony search and large neighbourhood search to solve large-scale FJSS instances.
P	Türkyılmaz and Bulkan [35] developed a hybrid genetic algorithm with variable neighbourhood search for FJSS with the objective of minimising total tardiness.
P	González et al. [36] developed a scatter search metaheuristic algorithm with effective neighbourhood structure to solve FJSS.
P	Jia and Hu [37] solved a multi-objective FJSS problem by a path-relinking TS algorithm with the mechanism of back-jump tracking.
O	For example, Toba [39] indicated from an application in semi-conductor industry that “Buffers accommodate lots (products) waiting at processing equipment units and supply the lots to the equipment units when they are ready to process them.
N	Brucker and Kampmeyer [23] indicated that a feasible solution may be represented by the machine sequences of the jobs but the buffers should be incorporated in the solution representation. 
O	Brucker et al. [40] investigated the LBJSS problem by classifying the buffering requirements into the following three types.
P	Introduced by Glover [42], TS metaheuristic consists of several elements called the initial solution, move, neighbourhood, searching strategy, memory, aspiration function, stopping rules, etc. 
P	SA metaheuristic introduced by Kirkpatrick et al. [43] also provides an effective mechanism to avoid getting trapped at a local optimum by using a probabilistic threshold to accept a neighbour that worsens the objective value. 
O	Under such circumstances, Yang et al. [66] show that the formulae derived for the standard M/M/C queueing models fail to provide satisfactory estimates of the system performance.
O	The earlier works on the M/G/C queues include Takahashi [53], which provides approximate formulae for the first two moments of waiting times. 
P	Hokstad [22] proposes an approximation based on Laplace transform for the steady-state queue length distribution.
P	Several approximations in both light and heavy traffics are introduced and tested by Boxma et al. [3], Tijms et al. [55], and Kimura [33,36].
O	Recognizing that most of the earlier approximations fail when both the number of servers (C) and the coefficient of variation (CV) are large, especially when traffic intensity is low, Ma and Mark [42] present a computationally efficient approximation for the mean queue length of M/G/C queues.
O	Mandelbaum and Schwartz [43] show that results on M/G/C queues are highly sensitive to the service time distribution.
O	Whitt [60,61] uses an infiniteserver approximation and notes that the quality of approximation varies across different performance measures.
P	This regime, also known as the Halfin-Whitt regime, is introduced by Halfin and Whitt [17] in the analysis of a G/M/C queue. Several extensions are developed by Puhalskii and Reiman [47], Whitt [63,64], Jelenkovic et al. [28], Mandelbaum and Momcilovic [44], and Reed [48].
O	Queueing theory dates back to the works by Erlang in the early 1900 s, and its theoretical development has grown substantially since 1950 s with the advances in Operations Research [18].
O	Readers can refer to Lakshmi and Iyer [39] for a comprehensive taxonomy of queueing applications in healthcare services and to Gans et al. [9] for applications in call centers.
O	While there are many studies on the single-server transient queues, such as Abate and Whitt [1], Bertsimas and Nakazato [2], Gong and Hu [10], Lee and Roth [41] and Wang [58], few studies exist on the multi-server transient queues.
O	Kelton and Law [31] and Murray and Kelton [46] use an embedded discrete-time Markov chain to calculate the probability of transient queuelength of M/Ph/C queues with exponential inter-arrival times and phase-type service times.
P	Chaudhry and Zhao [7] provide an analysis of a transient queue with finite queue capacity.
O	Whitt [62] analyzes the steady-state and transient performance of an M/G/C queue with a heavy-tailed service time distribution, and shows that the waiting time distribution is also heavy-tailed.
O	Using backward equations of Markov skeleton processes, Hou et al. [23] study the queue-length distribution of a transient G/G/C queue.
O	Ibrahim and Whitt [24] highlight that timevarying arrival rate can introduce significant errors in the estimation of queue length and waiting time when the system experiences alternating periods of overload and underload.
O	A pioneering work in this area is by Jagerman [27], which estimates the blocking probabilities using a non-stationary Erlang loss model by substituting the expected number of busy servers at specific time points obtained from a non-stationary infinite server model for the system load in a corresponding stationary finite server model.
P	Ingolfsson et al. [25] provide a survey and comparison of several exact and approximation methods for the non-stationary M/M/C systems. 
N	More recent studies by Jimenez and Koole [30] and Stolletz [50] address temporarily overloaded systems in which earlier models do not offer accurate results.
O	Margolius [45] derives an integral equation for the transient probabilities and mean queue-length of a non-stationary M/M/C queue.
O	Several studies also exist for queues with Erlang service times [8], general interarrival and/or service times [16,29]. 
P	Alternating Conditional Expectation (ACE) is an advanced nonlinear regression technique developed by Breiman and Friedman [5] who won the 1985 best JASA Theory and Methods paper award for their work.
P	The reader is referred to Sum et al. [51] for further details on the superiority of ACE over the standard Ordinary Linear Regression (OLR) technique.
O	A simulation model of a multi-server system with a single queue is built using the simulation software ARENA [32].
O	Inter-arrival times of customers are thus still often assumed to be exponential given the dearth of empirical evidence to suggest otherwise [40].
O	Following the results of a previous study [66], four independent factors are chosen to represent systems with opening and closing of the system: 
N	The results affirm the findings of Ho and Lau [21] that system performance is affected primarily by the coefficient of variation of its service time distribution but not by its skewness, kurtosis or other shape parameters.
P	Recently, Bertazzi et al. [5], Solyalı et al. [23] and Coelho et al. [10] have solved DSIRPs with the goal of minimizing the total inventory, distribution and shortage cost. 
O	Moin et al. [18] consider a deterministic scenario and develop a hybrid genetic algorithm for the multi-product multi-period IRP.
P	Salhi et al. [21] solved a multi-depot IRP using a giant tour for each depot which was later improved by local search. 
P	Bertazzi et al. [5] have formulated the SIRP as a dynamic program and have solved it by means of a heuristic rollout algorithm, sampling unknown demands and solving a series of deterministic instances.
P	Finally, Coelho et al. [10] proposed a rolling horizon algorithm to solve the DSIRP.
P	For the periodic review inventory system, Wensing [24] describes three policies. One is the OU which refers to a (t S, ) system
P	Simic and Simic [22] argue that for complex optimization problems such as the IRP, hybrid methods with techniques such as artificial neural networks, genetic algorithms, tabu search, simulated annealing and evolutionary algorithms can be successfully applied.
O	Genetic algorithms have been employed by Christiansen et al. [6] and Liu and Lee [15], who clustered customers in geographical areas to serve them together.
O	Local search operators were explored by Javid and Azad [13] and Qin et al. [19], who changed the delivery schedule for customers and adjusted the quantities delivered accordingly. 
O	Li et al. [14], Liu and Lin [16] and Sajjadi and Cheraghi [20] used simulated annealing to integrate location decisions into the IRP.
P	Adaptive large neighborhood search [8] and a hybrid of mathematical programming and local search [3] have also been used.
P	Finally, exact methods relying on branch-and-cut [2,7] and branch-cut-and-price [11] have also been developed.
P	We have used the large dataset of instances from Coelho et al. [10].
P	Note that the average solution provided by Coelho et al. [10] is 69 349.97, and hence all three new policies were able to outperform it
N	Obviously, this policy does not perform well and its costs are significantly higher than those of Coelho et al. [10] (Table 3).
P	The reduction of the average total cost from the 0.5Ci to 1.0Ci policies are very similar and yield the best comparison against the results of Coelho et al. [10].
O	The difference in these values arises in the average stockout: while in Coelho et al. [10] there is no stockout, in our policies low values are obtained. 
O	Coelho et al. [10] used transshipments and direct deliveries to mitigate stockouts after deliveries and demand realization (similar to a recourse function).
N	Overall, the fixed quantity policy does not outperform the solutions obtained by Coelho et al. [10].
O	Moreover, all three policies have outperformed the solutions of Coelho et al. [10], with an average total cost reduced by about 20%
N	These are reported in Table 7 and show that our algorithms can always find better solutions than those of Coelho et al. [10].
O	It is relevant to notice that the running times remain low even when the size of the instance increases, unlike the method of Coelho et al. [10].
P	Those of Coelho et al. [10] increase significantly faster, reaching more than 400 s.
O	These techniques range from studying centrality measures [13] to building complex probabilistic models describing network structure and formation [4,46,5].
O	The science of SNA encompasses a set of techniques for modeling network-based systems (see Wasserman and Faust [55] for SNA motivation and position statement).
O	In 2003, in a study of 816 organization founding teams, Ruef et al. showed that homophily and network constraints are the key factors defining team composition [48]. 
O	In a more recent study of 2349 open-source software (OSS) development teams, Hahn et al. reported positive correlations between the developers' decisions to join project teams, the collaborative ties with project initiators and the perceived status of other (non-initiator) members [29].
P	Zhu et al. investigated the impacts of personal and dyadic motives on team formation [60]. 
O	Kim et al. presented a Parallel Balanced Team Formation (PBTF) problem and employed MapReduce to solve PBTF variations considering the diversity of team members' skills including task-handling and communication abilities [34].
O	Agrawal et al. presented MaxTeam and MaxPartition to model problems in a similar setting [2].
P	A large branch of SNA literature develops Exponential Random Graph Models [46].
P	Stochastic actor-based models introduced by Snijders et al. [53] are also widely used; they also successfully utilize network structures based on individuals' attributes. 
P	Snijders et al. [53] were the first to focus research attention on individuals' local networks. 
P	Proposed by Lin and Kernighan [38], the idea was revised and implemented in an exceptionally effective heuristic Lin–Kernighan–Helsgaun (LKH) for symmetric TSP by Helsgaun [30].
O	In this study, a SGA with the following operators and parameter setting is implemented (for more details, see Chatterjee et al. [16]).
O	One of the reasons that has been stated by Kumar et al. [25] is: “the selected suppliers may impose several constraints on the supplying process to meet their own minimum or maximum order quantities that are based on their production capacity.”
O	Nonetheless, Costantino and Pellegrino [7] showed the advantages of adopting the multi-sourcing strategy especially in risky environments.
O	According to Aissaoui et al. [1], there are three major questions raised in VSP: (1) What product to order? (2) In what quantities and from which supplier(s) to order a product? In which periods to order a product? Here, the question arises what the relationship between VSP under the multi-sourcing strategy and the ordered quantity decisions is.
P	The review of the works on the VSP models presented by Deshmukh and Chaudhari [11] confirms the above gaps.
O	The VSP has been the area of focus since 1960 s when Dickson [13] published his work on the analysis of vendor selection systems and decisions. 
O	Weber et al. [46] reviewed 74 articles related to the vendor selection problem since 1966.
P	Recently, Deshmukh and Chaudhari [11] have provided a review on supplier selection criteria and methods.
O	Turner [41] introduced a single-objective linear programming model for British Coal in order to minimize the total discounted price under the vendor capability, maximum, and minimum order quantities.
P	Based on the concept of ownership's total cost, Degraeve et al. [10] developed a mathematical programming model as a basis to compare VSP models.
P	Kumar et al. [25] proposed a fuzzy programming model for a VSP with three important goals of quality-maximization, cost-minimization, and maximization of on-time delivery.
P	Moreover, a combinatorial optimization model for a two-stage VSP with outsourcing was 44 A.H. Niknamfar, S.T.A. Niaki / Computers & Operations Research 76 (2016) 43–59 presented by Cao and Wang [6].
P	Wang et al. [43,44] introduced two models for the VSP in order to maximize the total quality level, where the important issues such as quality, budget, and demand were fuzzy variables.
O	In addition, a VSP with fuzzy goals was modeled by Díaz-Madroñero et al. [12], in which the goals were to minimize the total ordering costs, the number of rejected items, and the number of late delivered items under several constraints.
P	Xu and Yan [47] formulated the VSP in a bi-fuzzy environment and utilized a chance-constrained programming and a particle swarm optimization to solve the problem.
P	Furthermore, the VSP under lean procurement environments was presented by Yu et al. [53]. 
P	More recently, Zerbini and Borghini [54] developed a relationship between the release capacity concept and the vendor selection process. 
O	Pan [29] presented a linear programming model in a multiple sourcing strategy to determine suppliers along with their allocated order quantities. 
P	Ghodsypour and O'Brien [19] developed a multi-criteria mixed integer non-linear programming model to solve a VSP under the multisourcing strategy and capacity constraints.
P	Basnet and Leung [3] presented a multi-period inventory lot-sizing scenario for the VSP where multiple products and vendors were assumed and each product could be sourced from a set of approved vendors.
P	A multi-objective optimization model for the VSP was introduced by Wadhwa and Ravindran [42].
P	Burke et al. [5] introduced a single period, single product sourcing decisions under demand uncertainty to explore the impact of minimum supplier order quantities on both single and multiple sourcing strategies.
P	An integrated VSP and order allocation problem was studied by Faez et al. [17], in which a fuzzy case-based reasoning was utilized as a solution approach. 
P	Keskin et al. [22] developed an integrated VSP and inventory optimization model and solved it using the generalized benders decomposition method.
P	Fang et al. [18] studied the performance of different sourcing strategies such as single, dual, multiple, and contingent sourcing in a supply risk management problem.
P	A coordination strategy was proposed by Yin et al. [50] to integrate business decisions and manufacturing planning in supply chain management.
P	Later, Yin and Nishi [49] presented quantity discount models that determine contracting suppliers, production quantities, and inventory.
O	To name of a few recent and relevant works, Ayhan and Kilic [2] proposed a two-stage approach for the supplier selection problem in multi-item/multi-supplier environment with quantity discounts. 
O	In addition, Yin et al. [51] studied a game-theoretic model for the supply chain coordination problem involving one manufacturer and multiple suppliers with quality variations under demand uncertainty. 
O	More recently, Ekici et al. [15] considered a duopolistic market of suppliers competing for the business of a retailer.
P	The model presented by Keskin et al. [22] is considered as a base to formulate this closer to reality problem.
P	Recently, Wang et al. [45] developed a probability estimation operator in DE to solve a binarycoded problem.
P	Moreover, GA proposed in Deb et al. [9] is one of the most credible algorithms developed based on genetic algorithm [36]. 
O	In order to demonstrate the applicability of the proposed model and to assess the performances of the solution methodologies, several problems with different numbers of vendors, stores, and products are considered based on the information given by Keskin et al. [22].
O	Besides, the lower and the upper bounds of some of these ranges are selected based on the presented case in Keskin et al. [22] as well as the one in Yu et al. [53].
P	Synchronous flow shops were first discussed by Kouvelis and Karabati [14].
P	Soylu et al. [17] present a branch-and-bound approach as well as several heuristics to minimize the makespan in synchronous flow shops.
O	In Huang [10], rotating production units with synchronous movement and a loading/unloading (L/ U) station are considered.
N	Our work is further motivated by a practical application studied in Waldherr and Knust [22].
O	In the practical application from [22] two of the eight machines are dominating.
P	For example, efficiently solvable cases can be found in Monma and Rinnooy Kan [16], Ho and Gupta [9], Xiang et al. [25].
O	In Wang and Xia [24] dominating machines in no-wait flow shops are investigated.
O	Complexity results for synchronous flow shop problems with dominating machines can be found in [23].
O	For a general survey on vehicle problems see Toth and Vigo [18], problems with unit demands were, for example, considered in Campos et al. [2] and Angel et al. [1].
O	The special arc costs in our application are the same as those considered by Gilmore and Gomory [8] for the traveling salesman problem.
P	This polynomially solvable case of the TSP has received a lot of attention in the literature, cf. Chandrasekaran [3], Kabadi and Baki [12], Kabadi [11], Kao and Sanghi [13], and Vairaktarakis [19,20].
P	With each sequence a corresponding (left-shifted) schedule is associated in which each operation starts as early as possible (cf. Waldherr and Knust [23]).
O	Since this constant is sequence-independent and adding a constant to the makespan objective does not change an optimal sequence, in this situation we may even assume that all processing times on the non-dominating machines are equal to zero (cf. Waldherr and Knust [23]).
O	If we additionally set a b 0 0 = ≔0, this TSP is a so-called “large TSP” (cf. van Dal et al. [21]) which can be solved in 6( ) n n log time by the algorithm of Gilmore and Gomory [8].
P	As described above, this can efficiently be done by the algorithm of Gilmore and Gomory [8].
P	At first we recall a MIP formulation proposed in Kouvelis and Karabati [14] for the general synchronous flow shop problem without dominating machines.
N	If we used the classical 6(2 ) n subtour elimination constraints from Dantzig et al. [5], the resulting models could not be solved for n > 20
O	In the MTZ-constraints (Miller et al. [15]), n additional variables ∈ [ ] κ u 1, i n are introduced denoting the position of node i N ∈ in its tour.
O	In the two-commodity flow model by Finke et al. [7] 6(n )2 additional variables u v, ij ij for ij V i , , ∈ ≠ 0 j are introduced.
O	The Team Orienteering Problem (TOP) was first mentioned in Butt and Cavalier [7] as the Multiple Tour Maximum Collection Problem (MTMCP).
P	Later, the term TOP was formally introduced in Chao et al. [9].
P	TOP is a variant of the Vehicle Routing Problem (VRP) [4].
O	The resulted problem is known as the Orienteering Problem (OP), or the Selective Travelling Salesman Problem (STSP) (see the surveys by Feillet et al. [13], Vansteenwegen et al. [32] and Gavalas et al. [16]).
O	For example in Bouly et al. [5], the authors used TOP to model the schedule of inspecting and repairing tasks in water distribution.
O	A very similar application was described in Tang and Miller-Hooks [29] to route technicians to repair sites. 
O	In Souffriau et al. [26], Vansteenwegen et al. [31] and Gavalas et al. [16], the tourist guide service that offers to the customers the possibility to personalize their trips is discussed as variants of TOP/OP.
O	Many other applications include the team-orienteering sport game, bearing the original name of TOP, the home fuel delivery problem with multiple vehicles (e.g., Chao et al. [9]) and the athlete recruiting from high schools for a college team (e.g., Butt and Cavalier [7]).
P	Many heuristics have been proposed to solve TOP, like the ones in Archetti et al. [2], Souffriau et al. [27], Dang et al. [12] and Kim et al. [20]. 
P	Butt and Ryan [8] introduced a procedure based on the set covering formulation.
P	In Boussier et al. [6], the authors proposed a branch-and-price (B–P) algorithm in which they used a dynamic programming approach to solve the pricing problem.
P	Later, Poggi de Aragão et al. [25] introduced a pseudo-polynomial linear model for TOP and proposed a branch-cut-and-price (B–C–P) algorithm.
P	Afterwards, Dang et al. [11] proposed a branch-and-cut (B–C) algorithm based on a linear formulation and features a new set of valid inequalities and dominance properties in order to accelerate the solution process.
P	Recently, Keshtkarana et al. [19] proposed a Branch-and-Price algorithm with two relaxation stages (B–P–2R) and a Branch-and-Cut-and-Price (B–C–P) approach to solve TOP, where a bounded bidirectional dynamic programming algorithm with decremental state space relaxation was used to solve the subproblems
O	These five methods were able to prove the optimality for a large part of the standard benchmark of TOP [9], however there is a large number of instances that are still open until now. 
P	Furthermore, according to the recent studies of Dang et al. [12] and Kim et al. [20], it appears that it is hardly possible to improve the already-known solutions for the standard benchmark of TOP using heuristics.
O	Recently, Pferschy and Staněk [24] demonstrated on the Travelling Salesman Problem (TSP) that such a technique which was almost forgotten could be made efficient now a day with the impressive performance of modern solvers for Mixed-Integer Programming (MIP), especially with a careful control over the reinforcing of the subtour elimination. 
P	Some of these cuts were introduced and tested in Dang et al. [11] yielding some interesting results for TOP, this encourages us to implement them in our cutting plane algorithm.
O	The first GSEC experiment with OP were reported in Fischetti et al. [14].
P	We adapted the GSEC version from Dang et al. [11] formulated to TOP with a directed graph as follows.
P	Dang et al. [11] proposed in their paper a set of efficient dominance properties that aims to reduce the search space by bounding the characteristics of each tour or subset of tours.
P	Based on the experimental report in Dang et al. [11], we focus exclusively on solutions in which the profits of tours are in ascending order.
P	This is similar to the approach of Pferschy and Staněk [24] which was developed in the context of TSP.
O	Then the graphs of incompatibilities between customers and arcs are initialized, and some early cliques and independent sets are extracted from them using the metaheuristic described in Dang and Moukrim [10].
P	A feasible solution is generated using a heuristic of Dang et al. [12] and provided to the MIP solver as a starting solution.
O	Thus, our branching rules prioritize yir first then xijr (see Boussier et al. [6], Poggi de Aragão et al. [25]).
O	These customers are identified based on Definition 1: the required LB is computed using a constructive heuristic from Dang et al. [12] while the required UB is computed with our CPA, but now formulated for the instances X⧹{ }i .
O	The graphs are then made more dense 26 R. El-Hajj et al. / Computers & Operations Research 74 (2016) 21–30 using their definition: lower bounds LB X( ) are due to the results of Dang et al. [12], and UB X i j ( ∪ {[ ∼ ]}) and UB X i, j u, v ( ∪ {[( ) ∼ ( )]}) are computed with the CPA, while adding constraints (19)–(22) to construct the desired models.
O	For each vertex in the associate incompatibility graph, we determine a large maximal clique containing the vertex using the metaheuristic from Dang and Moukrim [10].
O	We used the same two-hours limit of solving time as in Boussier et al. [6], Poggi de Aragão et al. [25], of which at most a one-hour limit is given to generate all the efficient cuts
P	We evaluated our approach on a set of TOP instances proposed by Chao et al. [9].
O	Since Poggi de Aragão et al. [25] did not report the detailed results of their algorithm, we restricted our comparison to the results of the B–P algorithm of Boussier et al. [6], the B–C algorithm of Dang et al. [11] and the B–P–2R and the B–C–P algorithms of Keshtkarana et al. [19].
P	For B–P (see Boussier et al. [6]) the reported CPU is the time spent on solving both the master problem and the subproblems until the optimality is proven.
O	For B–P-2R and B–C–P (see Keshtkarana et al. [19]), the CPU time is reported for the whole solving process.
O	The B–P–2R and the B–C–P algorithms of Keshtkarana et al. [19] only had problems with the set 6, while they obtained the best results for the set 5.
P	Moreover, we can notice from Table 3 that our CPA was able to improve the upper bounds of respectively 32 and 27 instances more than the two algorithms of Keshtkarana et al. [19].
O	Those could include variants of TOP on arcs, such as the Team Orienteering Arc Routing Problem (TOARP) which was addressed in Archetti et al. [3].
P	On the other hand, by taking into consideration the time scheduling of the visits, the CPA can be extended to solve other variants of TOP and VRP, such as the Team Orienteering Problem with Time Windows and/or Synchronization Constraints (e.g., Labadie et al. [21], Souffriau et al. [28], Guibadj and Moukrim [18], Afifi et al. [1]).
O	Meara et al. [33] recently conservatively estimated the annually needed number of surgeries in China at 57 million, of which they considered 27 million to be unmet.
O	For instance Subramanian et al. [40] consider an application which includes no-show, cancellation and overbooking.
O	For instance, Karaesmen and Van Ryzin [20] present a two-stage stochastic program to model no-show and overbooking, where cancellations have become known in the second stage (as is partially the case in our model).
O	Lai and Ng [25] propose a stochastic network optimization model for hotel revenue management and use robust optimization techniques to deal with cancellations, no-show and over-booking of hotel guests
O	For instance LaGanga and Lawrence [24] and Berg et al. [4] use overbooking to hedge against patient no-show and present simulation results showing a significant improvement in access and provider productivity, while increasing both patient wait times and provider overtime.
O	With regard to surgery scheduling, May et al. [32] conclude from a literature review that ‘it remains to be seen if the existing results and observations regarding manufacturing replanning and rescheduling would extend to surgery’ (where rescheduling refers to the possibility to adjust the initial schedule during execution).
O	Mancilla and Storer [31], Denton et al. [12] and Berg et al. [4] simultaneously consider patient waiting time, resource idle time, and overtime.
P	Xiao et al. [47] propose an adaptive scheduling approach for a problem that is closely related to the one considered in this paper, yet without considering cancellation.3947: 
N	Stepaniak et al. [39] present a simulation study on cancellation, which they refer to as ‘patient rejection’.
O	Laporte and Louveaux [26] propose modified L-shaped decomposition with adjusted optimal cuts for two stage stochastic program with integer recourse.
P	Angulo et al. [1] alternately generate optimal cuts of the linear sub-problem and the integer sub-problem, which improves the practical convergence (see also [15,8]). 
P	We use Jensen's inequality [17] to upper bound the minus second (and third) stage cost, a technique that was proposed by Batun et al. [3].
O	For instance, Kumar and Gandhi [23] (India) report that 17.6% of scheduled surgeries are canceled on the day of surgery.
O	Several authors, e.g., Kumar and Gandhi [23], Kolawole and Bolaji [22] (Nigeria), Chiu et al. [10] (China), Chalya et al. [9] (Tanzania), analyze causes of cancellation, citing variations and prolonged durations of previous surgeries as a prime source.
N	A Daily Briefing [11] report discusses a case study in the USA in which 6.7% of scheduled surgeries in 2009 are canceled, one-third of which was due to hospital related causes, such as poor scheduling.
O	In addition, Yoon et al. [49] (Korea), Hussain and Khan [16] (Pakistan), Perroca et al. [37] (Brazil) and Fernando et al. [14] (UK) explore cancellations.
N	Jiang et al. [18] report that 12.88% of children's elective surgeries are canceled in Hunan children's hospital in 2010 due to emergent infection (70.30%), inappropriate preoperative preparation (15.12%), poor scheduling and other factors (14.58%). 
O	Jie et al. [19] take a statistical analysis on Guangdong General Hospital, which is a large general hospital, and show that the cancellation rate is at 11.2%.
O	(Economic reasons refer to the patients inability to pay.) Li et al. [27] study cancellation at Zunyi Medical College, and report as main causes of cancellation: upper respiratory tract infection (18.39%), high blood pressure (12.86%), lack of preoperative preparation (11.79%), and economic concerns (9.64%).
O	Xiang et al. [46] report a cancellation rate of 5.1% caused by recent changes in health conditions (55.8%), patients' determination changes (23.1%), and poor scheduling.
N	Zhang et al. [50] report a 2010 case study and find that the cancellation rate is 13.9%, due to illnesses (68.7%), exogenous cancellations (20.3%), and preoperative preparations (7.7%). 
P	The reader may refer to Xu et al. [48] for related work.
O	These constraints extend the results in Batun et al. [3] for our problem.
O	The general method for obtaining upper and lower bound estimates from the SAA of two-stage stochastic programs has been discussed in Mak et al. [30] and Kleywegt et al. [21].
O	Our overall L-shaped algorithm follows the same general structure as the algorithm described in Angulo et al. [1].3964: 
O	All test problems have been pseudorandomly generated by using a generator proposed by Festa and Pallottino in 2003 ([8]).
O	More details about the history and main concepts of CP can be found in Pesant [46].
P	As stated in Russell and Norvig [47], if an existing CSP solving system is available, it is frequently more efficient to solve the problem using it rather than implementing a custom solution.
P	To mention a few applications, Caprara et al. [15] applied CP to solve the railway crew management problem, whereas De Backer et al. [20] successfully applied CP to a vehicle routing problem.
O	More recently, Hooker [33] suggested how an integrated modelling framework can retain, and even enhance, the modelling power of CP while allowing the full computational resources of mathematical programming.
O	The paper of Blum et al. [13] illustrates experiences in hybridising metaheuristics with CP.
P	More specifically, the 2-opt heuristic was applied in 1958 for solving the Traveling Salesman Problem (TSP) [19].
P	LS has proven to be a good heuristic for solving important family of problems like the above-mentioned TSP and a very difficult problem known as Job Shop Scheduling, Van Laarhoven et al. [51].
P	Many authors have proposed solutions for this situation, designing strategies for escaping from local optima, being one of the most prominent a method called Simulated Annealing [36,35].
P	An alternative mechanism that inherits the properties of LS is the J.H. Gutiérrez et al. / Computers & Operations Research 75 (2016) 150–162 155 so-called Variable neighbourhood Search (VNS) algorithm, proposed by Mladenović and Hansen [43].
O	The VNS algorithm as well as some variants and applications are discussed in Hansen et al. [30].
P	For an updated description of the numerous metaheuristic approaches please refer to the book of Xing and Gao [53].
O	The attack to telecommunication towers in Afghanistan occurred by Taliban [1], the assault on an ambulance station in Northern Ireland [2] and the possibility of disruptive threats to the electric grids [3] are examples of such attacks
O	In particular, critical components consist of those certain physical assets that, when lost, result in significant disruption or disorder in operational capabilities of the system [4]. 
O	Moreover, it also contains facilities for providing emergency services or locations to store gas and oil products [5].
O	In addition, protection of these facilities can be enhanced by hardening facility perimeters, limiting access, moving critical facility locations to interior and safe areas and developing backup power systems [6].
O	In a nested hierarchy, a higher-level facility provides all services supplied by a lower-level facility and at least one additional service while in a non-nested hierarchy, facilities on each level offer different services [7].
O	In this paper, we aim at developing the traditional r-interdiction median problem with fortification (RIMF) [8] for multi-flow nested hierarchical facilities.
P	Wollmer [9] was among the first authors who studied the arc interdiction model and presented an algorithm that minimizes the maximum amount of flow between facilities by removing a certain number of arcs. 
O	Given a limited interdiction budget, McMasters and Mustin [10] presented a mathematical model to determine the optimal interdiction plan for minimizing the maximal flow capacity from a source to a sink vertex of a given network. 
P	Numerous researchers have considered the network interdiction problem from which we can refer to the papers by Wood [11], Ghare et al. [12], Cormican et al. [13], Israeli and Wood [14] and Lim and Smith [15].
O	The first published paper in this area is that by Church et al. [16].
O	In a subsequent work, Church and Scaparra [4] incorporated the protection of facilities into the r-interdiction median problem to minimize the disruptive effects of possible intentional attacks to the system.
O	Accordingly, in another work, Scaparra and Church [6] reformulated the IMF problem as a maximal covering problem.
O	This makes the problem as a leader-follower game which is also known as the static Stackelberg game [17]
P	Scaparra and Church [8] are the first ones who proposed a bi-level programming formulation of the r-interdiction median problem with fortification (RIMF).
P	A similar problem was studied by Galvão et al. [26] in which prenatal health care system had been considered.
O	Aksen et al. [18] considered the budget constraint instead of a predetermined number of facilities to be fortified in the RIMF model
O	Losada et al. [19] addressed the facility recovery time in a planning horizon framework in cases of probable and improbable disruptions.
O	Liberatore et al. [20] presented the stochastic version of RIMF in which the extent of terrorist attacks and malicious actions are uncertain.
O	Keçici et al. [21] developed another formulation where the leader aims to locate new facilities, relocate existing ones if essential, and protect some of them by considering the budget constraint to ensure a maximum coverage of the customer zones.
P	Liberatore et al. [22] proposed a tri-level formulation for a facility protection model that considers the propagation of the disruptions from one element of the system to another.
O	In Aksen and Aras [23] the defender decides about facilities to be opened and protected conjunctly.
O	Aksen et al. [24] integrated the partial facility interdiction into the interdiction problem with capacitated facilities and outsourcing option.
P	For a recent literature survey of hierarchical facility location and its applications, we refer the reader to Farahani et al. [25].
P	Several papers have been dedicated to the application of heuristic algorithms in solving problems arising in single-level facility location problems (see, e.g. [27,28]). 
O	Moore and Bard [29] showed that mixed integer bi-level programming models are NP-hard.
P	Bard and Moore [30] extended their algorithm and solved instances with at most 35 integer decision variables in the leader problem.
N	The restriction in the number of integer variables in [29,30] indicates that the proposed branch-and-bound algorithm is not efficient for larger size problems.
O	There are recent advances in exact solution techniques for mixed integer bi-level programming such as dualization [22], cutting plane [31] and decomposition-based solution method [32], but their applicability is limited to small instances [24].
P	Keçici et al. [21] and Aksen and Aras [23] developed a heuristic solution procedure based on Tabu search heuristic in which the lower level problem is solved by CPLEX.
P	Aksen et al. [24] proposed two different methods for solving their developed bi-level programming model in which the first one is a progressive grid search which is not viable on large sized instances while the second one is a multi-start simplex search heuristic developed to overcome the exponential time complexity of the first method.
P	Introduced by Mladenovic and Hansen [33], the variable neighborhood search (VNS) is an algorithm that tries to achieve good quality solutions by gradually extending the size of the neighborhood.
N	Then, by using the roulette wheel selection method proposed by Holland [35], k1 facilities from P1 and k2 facilities from P2 are selected and removed from the sets of protected facilities.
O	For each protected facility at level 1 and level 2 equations (29) and (30) give the extraction probability of each facility, respectively, in which ScoreExtract (l) represents the score of each facility lAP1[P2 which is obtained by applying (31) and (32).
O	Then each demand is assigned to the closest appropriate facility and the score of each facility is calculated based on the total weighted distance between facilities and their allocated demands based on equations (33) and (34).
P	The SA is a probabilistic search algorithm proposed by Kirkpatrick, et al. [36] and Černý [37].
O	The coordinates of the customers and facilities are generated by following the same rule proposed in Aksen and Aras [23].
P	To obtain the best combination of the parameters in each algorithm, the resulting optimality gap over all tested instances is analyzed by the use of Wilcoxon signed-rank test [38].
O	We refer to [14] for a recent collection of chapters on different VRPs.
O	We refer to [6] for a recent work on a multi-compartment routing problem, with fixed size of the compartments, and to references therein.
O	In this paper we consider the Commodity constrained Split Delivery Vehicle Routing Problem (C-SDVRP) introduced in [2], where different commodities are distributed to customers with capacitated vehicles.
P	In [2] the C-SDVRP is introduced and compared with alternative ways of distributing multiple commodities, such as by allowing the splitting of individual commodities or by using vehicles dedicated to individual commodities.
O	We tested the algorithm on the small and mid-size instances tested in [2], with a time limit of 2 h.
P	The ESPPRC is NP-hard in the strong sense (see [8]).
O	We thus apply the ng-path relaxation of the problem that allows the generation of paths that may contain cycles (see [3]).
O	We solve the pricing problem by means of a label setting dynamic programming algorithm (see [10]).
O	The first one is the decremental state space relaxation technique [12].
O	The second technique we implemented is the 2-cycle elimination proposed in [5].
P	This branching rule implements the Ryan and Foster [13] rule and guarantees the correctness of the solution algorithm (see [4]).
O	In order to identify violated inequalities (29), we implemented the shrinking heuristic presented in [11].
O	Moreover, when zi is constrained to be greater than or equal to α41, then the right-hand side term of constraint (27) is set equal to jKi j ðα1Þ.
O	We tested the branch-price-and-cut algorithm on benchmark instances for the C-SDVRP proposed in [2].
O	As mentioned in the introduction, [2] proposed a branch-and-cut algorithm for the C-SDVRP which was able to solve 25 out of the 64 instances within a time limit of 30 min.
O	Finally, the last two columns report the value of the best known feasible solution, taken from [2], and the percentage gap between this value and the lower bound reported in column zn, respectively.
N	In parentheses, we report the number of improved feasible solutions with respect to the best known reported in [2].
P	Out of the 30 instances for which the algorithm finds a solution, this solution is better than the best known solution found in [2] on 20 cases.
P	In fact, the problem was solved heuristically by [2] by transforming it into a VRP while no ad hoc heuristic has been proposed.
O	By 2040, the Federal Highway Administration in the U.S. Department of Transportation expects that freight trucks on arterial roads including interstate and major highways will travel 662 million miles per day, which is an increase of 66% with respect to 2002 [13]. 
O	In order to respond to environmental and economic sustainability in the transportation sector, many logistics companies have a considerable interest in a transition to alternative-fuel (AF) trucks powered by natural gas, electricity, or hydrogen [6,10,14,33].
O	For example, Fedex, one of the major logistics companies, plans to replace 30% of their long-distance trucks with natural gas trucks over the next 10 years [11].
O	The availability of an AF refueling infrastructure on transportation networks is a key issue for logistics companies that want to bring in non-conventional or AF vehicles [29].
O	Transportation networks have two basic structures: circuit networks and branching networks [18,39].
O	Branching networks are characterized by their tree-like structures, which consists of sets of connected line segments without any complete circuit. A graph without cycles is called a forest and a connected forest is a tree [38].
P	Furthermore, the entire turnpike network in states such as Pennsylvania and Oklahoma, which include a number of independent roadway segments or expressways, can be represented as forests [16].
O	Hodgson [19] and Berman et al. [5] were the first to propose a basic flow-capturing location model (FCLM) for locating p service facilities with the objective of maximizing the passing flow rate captured by the facilities.
P	Hodgson et al. [20] applied the model to a real-world transportation network with over 20,000 flow pairs representing the morning-peak traffic in the Edmonton Metropolitan Area in Canada.
O	The model was extended to consider derivations from pre-planned routes to visit service facilities by Berman et al. [4]; to consider multi-counting by Averbakh and Berman [2], where customer service level is based on the number of facilities that customers encounter on their pre-planned tours; to incorporate customer preferences by Zeng et al. [40]; and to take into account facilities of different sizes by Tanaka and Furuta [34], where deviations from pre-planned routes depend on the facility size. 
P	Berman [3] proposed four new flow-demand location models, in two of them the objective was to maximize the coverage of potential customers whereas in the other two the emphasis was to maximize the number of potential customers that can become users. 
O	Furthermore, based on the distribution of the deviation distances to visit a facility from preplanned routes derived in [31], Miyagawa [32] developed an analytical model for determining the adequate density of AF refueling stations to achieve certain level of service.
O	By incorporating the key aspect that vehicles may need to refuel or recharge multiple times along their paths due to the limited driving range in the FCLM, Kuby and Lim [24] developed the flowrefueling location model (FRLM) to locate AF refueling stations that maximize the total path-flow that can be covered by combinations of refueling stations.
P	Upchurch and Kuby [35] compared the performance of the p-median and FRLM for locating AF refueling stations, and verified that the FRLM has a better performance in locating the stations, especially for inter-city trips.
P	Thus, to apply the FRLM to large real-world networks, Lim and Kuby [27] proposed three heuristic algorithms, and Kuby et al. [26] applied the heuristic algorithms to determine locations for hydrogen stations in the Orlando metropolitan area and the State of Florida
O	Also, for generation of optimal solutions and elimination of the feasible facility combinations step, Capar and Kuby [7] developed an efficient formulation of the FRLM by introducing decision variables that indicate whether a vehicle at a particular candidate site has enough fuel remaining to get to the next open refueling station.
O	In a similar manner, Capar et al. [8] resolved the complexity issue by developing a compact model with “cover sets” that cover the entire distance between any pair of nodes when a vehicle is filled up at a candidate site in the set.
O	Also, on an expanded network with dummy supply and demand nodes, MirHassani and Ebrazi [30] proposed a set covering version of the FRLM to determine a set of candidate locations that minimize the cost of building AF refueling stations on the paths that are generated by the flow-conservation property.
O	While the original FRLM assumed that all refueling stations have sufficient capacity to meet any refueling demand, Upchurch et al. [36] developed the capacitated FRLM that limits the number of vehicles that can be refueled at each station.
P	Also, while the original FRLM did not allow to deviate from preplanned trips for refueling, Kim and Kuby [21] proposed the deviation version of the FRLM with five types of distance decay functions to model the fraction of drivers that are willing to deviate from their shortest path.
P	Accordingly, Kim and Kuby [22] developed two heuristic algorithms to overcome the computational burden of their deviation-FRLM through network transformation. 
O	Kuby and Lim [25] pointed this out and used three methods for generating additional candidate sites.
O	The other two methods, developed by Kuby et al. [23], use the added-node dispersion problem to disperse nodes along the arcs of a network with the objective of minimizing the maximum separated arc length or maximizing the minimum separated arc length. 
P	A similar definition for this line segment is also provided in Kuby and Lim [25].
O	Then, the number of trees is reduced by adding edges ðvi; vjÞAE such that dðvi; vjÞrR until the final set of trees is formed. This approach takes O nð Þ log n time [38].
P	Rahoual et al. [8] solved the VRPTW by using the non-dominated sorting genetic algorithm (NSGA) with elitist and sharing strategy, which was the first work that employed the multi-objective evolutionary algorithm (MOEA)to solve MO-VRPTW.
P	Ombuki et al. [9] investigated the effectiveness of solving VRPTW by using a multi-objective optimization model and multi-objective genetic algorithm.
P	Tan et al. [10] introduced the heuristics methods into MOEA to perform local exploitation and proposed a hybrid multiobjective evolutionary algorithm for MO-VRPTW.
P	Geiger et al. [11] proposed an interactive multi-objective approach with variable neighborhood search to solve various components of general vehicle routing problems including MOVRPTW. 
P	Xu et al. [12] introduced a hybrid algorithm to solve tri-objective VRPTW.
P	Garcia-najera et al. [13] first considered the similarity of solutions in solving MO-VRPTW.
P	Gong [15] developed multi-objective particle swarm optimization (PSO) algorithms for solving MO-VRPTW.
O	Ghoseiri and Ghannadpour [16] investigated a bi-objective VRPTW and developed a multi-objective genetic algorithm for solving it. 
O	Chiang et al. [17] incorporated problem-specific knowledge into the genetic operators and developed an evolutionary algorithm for MO-VRPTW.
O	Melian-Batista et al. [18] considered both the total traveled distance and traveling time balance, and developed a solution approach based on the scatter search metaheuristic for real-world MO-VRPTW.
P	Banos et al. [19] combined evolutionary computation with simulated annealing and developed a hybrid algorithm for MO-VRPTW that considers both the traveled distance and the workload balance.
P	More recently, Zhang and Li [21] combined decomposition methods and the evolutionary computation together, and proposed a multi-objective evolutionary algorithm based on decomposition (MOEA/D).
O	In M-MOEA/D , a variable-length chromosome representation for VRPTW which is developed by Tan et al. [10] is employed.
N	As for the mutation, M-MOEA/D employs the remove and reinsert mutation operator proposed by Garcia-Najera and Bullinaria in [14].
O	According to Deb’s analysis on MOEA/D using the Tchebycheff approach [26], the optimal solution of the scalar subproblem with weight vector λ is located on the lowest contour line.
O	Solomon’s benchmark [27] which has been widely used in these years is adopted to illustrate the superiority of the proposed M-MOEA/D. Solomon’s problems consist of 56 data sets. 
O	Then, we compared M-MOEA/D with other two efficient and the latest developed multiobjective algorithms: MOGP by Ghoseiri et al. [16] and MOEA by GarciaNajera [28] After that, the effectiveness of the two innovative components in M-MOEA/D were analyzed in detail by comparing M-MOEA/D with its three variants. 
P	Table 10 employs the hypervolume indicator proposed by Zitzler and Thiele [48] to compare the non-dominated solutions obtained by M-MOEA/D and the other two comparing algorithm in view of multi-objective optimization
O	As presented by Chajakis and Guignard [3], the Set-up Knapsack Problem (SKP) occurs as a subproblem of a parallel machine scheduling problem with setups.
O	A thorough survey of the literature on two variants of knapsack problems with setups has recently been presented in the work of Michel et al. [4] namely 50 the multiple-class binary knapsack problem with setups (MBKPS) where item weights are assumed to be a multiple of there class weight and the continuous knapsack problems with setups (CKS) where each class holds a single item and a fraction of an item can be selected while incurring a full setup.
P	Michel et al. provided an extension of the branch-and-bound algorithm proposed by Horowitz 55 and Sahni [5] for problems with positive setup costs.
P	Dynamic Programming (DP) was introduced by Bellman [12].
P	Toth [13] and Horowitz and Sahni [5] presented an improved dynamic programming algorithm for solving the knapsack problem.
P	More recently, Pisinger [14] proposed a minimal algorithm for the 0-1 knapsack problem based on dynamic programming. 
P	A first hybrid approach combining DP with an enumerative scheme was proposed by Plateau and Elkihel [15].
O	Chajakis and Guignard [3] suggested a dynamic programming algorithm and two versions of a two-phase enumerative scheme for the SKP. 
O	McLay and Jacobson [9] have also provided a dynamic programming algorithm for IKPSW that produces an optimal solution in a pseudo-polynomial time but is not practical for solving large problem instances.
P	Chajakis and Guignard [3] presented a dynamic programming algorithm and two versions of a two-phase enumerative scheme consisting in converting the problem into a multiple choice knapsack problem (MCKP).
O	The second cost measure is based on distance (D) and consists of the fixed vehicle cost and the distance traveled by the vehicle, as is the case in the standard VRPTW (Solomon, 1987).
P	The first variant is FT, described by Liu and Shen (1999b) and the second is FD, introduced by Br¨aysy et al. (2008).
O	The third variant HT was defined and solved by Paraskevopoulos et al. (2008).
O	Hoff et al. (2010) and Belfiore and Yoshizaki (2009) describe several industrial aspects and practical applications of heterogeneous vehicle routing problems.
O	The most studied versions are the fleet size and mix vehicle routing problem, described by Golden et al. (1984), which considers an unlimited heterogeneous fleet, and the heterogeneous fixed fleet vehicle routing problem, proposed by Taillard (1999). 4091: 
P	For further details, the reader is referred to the surveys of Baldacci et al. (2008) and of Baldacci and Mingozzi (2009).
O	The FT variant has several extensions, e.g., multiple depots (Dondo et al., 2007; Bettinelli et al., 2011), overloads (Kritikos and Ioannou, 2013), and split deliveries (Belfiore and Yoshizaki, 2009, 2013).
O	There exist several exact algorithms for the capacitated vehicle routing problem (VRP) (Toth and Vigo, 2002; Baldacci et al., 2010), and for the heterogeneous VRP (Baldacci and Mingozzi, 2009).
O	Liu and Shen (1999b) proposed a heuristic for FT which starts by determining an initial solution through an adaptation of the Clarke and Wright (1964) savings algorithm, previously presented by Golden et al. (1984).
O	The algorithm was tested on a set of 168 benchmark instances derived from the set of Solomon (1987) for the VRPTW. Dullaert et al. (2002) described a sequential construction algorithm for FT, which is an extension of the insertion heuristic of Golden et al. (1984). 
O	Dell’Amico et al. (2007) described a multi-start parallel regret construction heuristic for FT, which is embedded into a ruin and recreate metaheuristic.
O	Br¨aysy et al. (2008) presented a deterministic annealing metaheuristic for FT and FD.
O	In a later study, Br¨aysy et al. (2009) described a hybrid metaheuristic algorithm for large scale FD instances.
O	An adaptive memory programming algorithm was proposed by Repoussis and Tarantilis (2010) for FT, which combines a probabilistic semi-parallel construction heuristic, a reconstruction mechanism and a tabu search algorithm.
O	In a re61 cent study, Vidal et al. (2014) introduced a genetic algorithm based on a unified solution framework for different variants of the VRPs, including FT and FD. 
O	To our knowledge, Paraskevopoulos et al. (2008) are the only authors who have studied HT.
P	The HEA combines two state-of-the-art metaheuristic concepts which have proved highly successful on a variety of VRPs: Adaptive Large Neighborhood Search (ALNS) (see Ropke and Pisinger, 2006a; Pisinger and Ropke, 2007; Demir et al., 2012) and population based search (see Prins, 2004; Vidal et al., 2014).
P	The second contribution is the introduction of several algorithmic improvements to the procedures developed by Prins (2009) and Vidal et al. (2012).
N	We also propose an advanced version of the Split algorithm of Prins (2009) capable of handling infeasibilities. 
O	The operators used within the HEA are either adapted or inspired from those employed by various authors (Ropke and Pisinger, 2006a,b; Pisinger and Ropke, 2007; Demir et al., 2012; Paraskevopoulos et al., 2008).
P	The heterogeneous fleet version of the ALNS that we use here was recently introduced by Ko¸c et al. (2014).
O	The general idea behind the SR operator, which was introduced by Shaw (1998), is to remove a set of customers that are related in a predefined way and are therefore easy to change. 
P	The average cost per unit (ACUT) is described by Paraskevopoulos et al. (2008) to measure the utilization efficiency of a vehicle Π(R) on a given route R.
O	The selection criterion is based on the historical performance of every operator and is calibrated by a roulette-wheel mechanism (Ropke and Pisinger, 2006a; Demir et al., 2012).
P	Vidal et al. (2012) proposed a new method, named biased fitness bf(C), to tackle this issue.
O	The algorithm selects the broken pairs distance (see Prins, 2009) to compute the distance β.
O	The OX operator is well suited for cyclic permutations, and the giant tour encoding allows recycling crossovers designed for the traveling salesman prob287 lem (TSP) (see Prins, 2004, 2009).
P	This algorithm was successfully applied in evolutionary based algorithms for several routing problems (Prins, 2004, 2009; Vidal et al., 2012, 2013).
O	This is achieved through a controlled exploration of infeasible solutions (see Cordeau et al., 2001 and Nagata et al., 2010), by relaxing the limits on time windows and vehicle capacities. 
N	This is in contrast to Prins (2009) who does not allow in feasibilities, and in turn solves a resource-constrained shortest path problem using dynamic programming to determine the best fleet mix on a given solution.
O	Our implementation also differs from those of Vidal et al. (2013) since it allows for infeasibilities that are not just related to time windows or load, but also to the fleet size in the case of HT and HD.
O	The method of Vidal et al. (2012), aims to ensure the diversity of the population and preserve the elite solutions. 
O	The benchmark data sets of Liu and Shen (1999b), derived from the classical Solomon (1987) VRPTW instances with 100 nodes, are used as the test-bed.
P	Liu and Shen (1999b) introduced three types of cost structures, namely large, medium and small, and have denoted them by A, B and C, respectively
O	The benchmark set used by Paraskevopoulos et al. (2008) for HT is a subset of the FT instances, in which the fleet size is set equal to that found in the best known solutions of Liu and Shen (1999a).
O	In total, there are 24 benchmark instances derived from Liu and Shen (1999a) for HT.
O	In our implementation, we initially used the parameters suggested by Vidal et al. (2012, 2013) for the genetic algorithm, but we have conducted several experiments to further fine tune these parameters on instances C101A, C203A, R101A, R211A, RC105A and RC207A.
O	In particular, we compare ourselves against LSa (Liu and Shen, 1999a), LSb (Liu and Shen, 1999b), T-RR-TW (Dell’Amico et al., 2007), ReVNTS (Paraskevopoulos et al., 2008), MDA (Br¨aysy et al., 2008), BPDRT (Br¨aysy et al., 2009), AMP (Repoussis and Tarantilis, 2010) and UHGS (Vidal et al., 2014).
O	The HEA outperforms all other algorithms in the literature for FD, including the UHGS of Vidal et al. (2014).
N	We have also developed an advanced version of the Split algorithm of Prins (2009) to determine the best fleet mix for a set of routes.
N	In the case of FT, our HEA clearly outperforms all previous algorithms except that of Vidal et al. (2014).
O	Dasci and Laporte [9], Jensen [36], Winerfert [52] discuss this issue.
O	Hotelling [33] analyzed a situation on a line with distance patronizing behavior from a game theoretic point of view.
O	Eiselt [24] and Eiselt and Laporte [25] extended the game theoretic approach to a tree environment.
P	The proximity rule was generalized to a utility function or random utility rule by [10,14,38]. Huff [34,35] introduced a patronizing model and investigated its validity with data.
O	He suggested that consumers distribute their patronage according to a probabilistic gravity model suggested by Reilly [44].
P	Drezner et al. [18,19] proposed a cover-based rule.
O	Drezner [11] assumed that the facilities' attractiveness are variables.
O	Plastria and Vanhaverbeke [40] combined the limited budget model with the leader–follower model. 
O	Aboolian et al. [1] studied the problem of simultaneously finding the number of facilities, their respective locations and attractiveness (design) levels.
O	The leader–follower model (Stackelberg [47]) considers a competitor's reaction to the leader's action.
O	A comprehensive discussion of this rule is presented in Section 2 of [18].
N	Equal division may not be accurate for a single consumer but the aggregated market share is estimated reasonably well as pointed out in [18].
P	Drezner [12] conducted interviews with consumers patronizing different shopping malls asking them to provide the zip code of their residence and whether they came from home.
O	Drezner [12] compared power and exponential decay on a real data set of shopping malls in Orange County, California and found that exponential decay fits the data better than power decay. 
P	In Drezner et al. [19], three strategies were investigated: In the improvement strategy (IMP) only the improvement of existing chain facilities is considered; in the construction strategy (NEW) only the construction of new facilities is considered; and in the joint strategy (JNT) both improvement of existing chain facilities and construction of new facilities are considered. 
P	The follower's problem is thus well-defined following the leader's action and can be optimally solved by the branch and bound algorithm detailed in Drezner et al. [19].
P	A branch and bound algorithm as well as a tabu search [29–31] were proposed in Drezner et al. [19] for the solution of each of these three strategies.
P	This was initially done by recoding the Fortran procedure for JNT problems from Drezner et al. [19].
P	The single-product multi-period inventory lot sizing problem has its origins at the end of the 1950s and was first proposed by Wagner and Whitin (1958).
O	Moreover, Zangwill (1969) has shown that this problem is a fixed charge network problem.
P	Wagner and Whitin (1958) have solved this type of problem optimally with a dynamic programming algorithm.
O	For instance, see Simpson (2001) who conducted a scrutinized and comprehensive study of nine well-known published rules. 
O	For example, De Bodt et al. (1984), Bahl et al. (1987), Kuik et al. (1994) and Wolsey (1995) are the first reviews on the history of the single product lot sizing problem.
P	In addition to well-known heuristic rules, the lot sizing problem has been also solved using other approaches, i.e. Hop and Tabucanon (2005) have developed a new and original approach to solve the lot sizing problem using an adaptive genetic algorithm.
P	Later, Cárdenas-Barrón (2010) discusses some features of the adaptive genetic algorithm in Hop and Tabucanon (2005).
P	He concludes that it is convenient to solve the lot sizing problem with Wagner and Whitin (1958) algorithm because this always obtains the optimal solution.
P	In addition, Jans and Degraeve (2007) have provided a comprehensive and complete review of metaheuristics for the lot sizing problem.
O	The research works of Kasilingam and Lee (1996) and Jayaraman et al. (1999) address this problem.
O	Also, Dahel (2003) develops a multi objective mixed integer programming approach to select the number of suppliers to use and the lot size of each product to place the orders to suppliers for a multiproduct, multi-supplier competitive sourcing environment. 
O	Basnet and Leung (2005) deal with the multi-product multi-period inventory lot sizing with supplier selection problem.
P	On the other hand, Moghadam et al. (2008) propose a new mathematical formulation for the multiperiod inventory lot sizing problem with supplier selection and solve it using a hybrid intelligent algorithm based on fuzzy neural networks and genetic algorithms.
O	Aissaoui et al. (2007) present an excellent and comprehensive review on supplier selection and lot sizing
O	In the same year, Wadhwa and Ravindran (2007) model the supplier selection problem as a multi-objective optimization problem in which one or more purchasers order multiple products from different suppliers in a multiple sourcing network.
O	Later, Ustun and Demirtas (2008) propose a model that integrates the well-known analytic network process (ANP) and achievement scalarizing functions to select the best suppliers and establish the optimal lot sizes between the chosen suppliers by considering tangible–intangible criteria and time horizon.
O	Afterwards, Ho et al. (2010) review the literature related to the multi-criteria decision making approaches for supplier selection and evaluation.
P	Tsai et al. (2010) develop an attribute-based ant colony system (AACS) which is a framework to study the critical factors to select the best suppliers for purchasing decisions.
O	Ware et al. (2012) present an exhaustive and well documented state of the art literature review.
O	Basically, Ware et al. (2012) critique the papers that deal with the supplier selection problem.
P	Afterwards, Ruiz-Torres et al. (2013) propose a mathematical optimization model to determine the optimal demand allocation over a set of suppliers considering the risk of supplier failures.
O	Ware et al. (2013) study the interrelationship of supplier selection criteria using the interpretive structural modeling (ISM) approach. 
O	Choudhary and Shankar (2013) propose an integer linear programming optimization model to jointly determine the timing of procurement, lot-sizes, and the selection of suppliers and carriers in order to minimize the total cost thru a finite planning horizon.
P	In a subsequent paper, Choudhary and Shankar (2014) develop a multiobjective integer linear programming model to make the best decisions on inventory lot-sizing, supplier selection, and carrier selection.
P	Ware et al. (2014a) propose a methodology for the flexible supplier selection problem in which both qualitative and quantitative factors are considered jointly. 
O	In the same year, Ware et al. (2014b) analyze the impact of demand variation on a multi-product, multi-source, multi-period model for the supplier selection problem.
P	Ware et al. (2014c) develop a mixed-integer non-linear program to address the dynamic supplier selection problem.
P	More recently, Karsack and Dursun (2015) present a fuzzy multi-criteria group decision making methodology which uses the well-known quality function deployment (QFD), and a fusion of fuzzy information and a 2-tuple linguistic representation model for supplier selection. 
N	This research paper mainly improves the work of Basnet and Leung (2005).
P	It is of fundamental importance to mention that the inventory lot sizing with supplier selection problem in Basnet and Leung (2005) is a complex combinatorial NP hard optimization problem.
P	Basnet and Lueng (2005) have left as a fertile area for future research the improvement of their solutions
P	TreviñoGarza (2009) has proved that the ROA is capable of solving other types of binary integer problems (i.e. single machine total weighted tardiness problem, set covering problem and set partitioning problem) in a reasonable time.
P	The noteworthy results achieved by Treviño-Garza (2009) have encouraged us to apply ROA to multi-product multi-period inventory lot sizing with supplier selection problem in the present research article. 
O	Basnet and Leung (2005) formally define the multi-product multi-period inventory lot sizing with supplier selection problem as follows.
O	Basically, Basnet and Leung (2005) propose a mixed integer linear programming (MILP) formulation which is presented in Subsection 2.2. 
P	Basnet and Leung (2005) propose the following MILP formulation for the multi-product multi-period inventory lot sizing with supplier selection problem (the notation and the mathematical model are placed here just for self-completeness of this paper).
O	This section presents a heuristic algorithm for solving the mixed integer linear programming (MILP) for the multi-product multi-period inventory lot sizing with supplier selection problem, along with, the computational results for the 150 instances solved in Basnet and Leung (2005).
N	It is important to mention that Basnet and Leung (2005) do not provide any valid inequality because their solution approaches do not require this.
P	In this paper option iv is preferred using the Wagner-Whitin (1958) algorithm.
P	Also, Y WW jt represents the solution found by the Wagner-Whitin (1958) algorithm.
O	The initial set is constructed applying the Wagner-Whitin (1958) algorithm. 
O	In other words, the Wagner-Whitin (1958) algorithm is used simply to determine which binary variables (i.e. Yjt) must be included in the initial set BR. 
P	This section presents the comparison of results of the proposed heuristic algorithm with the existing results in the literature reported by Basnet and Leung (2005).
O	To evaluate the performance of this new approach, the same instances for the multi-product multi-period inventory lot sizing with supplier selection problem proposed by Basnet and Leung (2005) were used.
O	Basnet and Leung (2005) there are two algorithms: enumerative and heuristic.
N	The results obtained by the proposed heuristic algorithm are compared with the best results obtained with the heuristic algorithm of Basnet and Leung (2005).
P	In detail, Table 2 shows Basnet and Leung (2015)’s solutions, the solutions of the proposed algorithm, the difference, the percent of improvement, the computational time, the number of optimal solutions achieved, and the gap.
N	In line with the results of Table 2 one can conclude that the proposed heuristic algorithm was able to find on average better solutions than those reported by Basnet and Leung (2005) for all instance sizes.
O	This indicates that the performance of the Basnet and Leung (2005) algorithm deteriorates for large instances. 
N	The performance of the heuristic algorithm is very satisfactory because it found better solutions than those reported by Basnet and Leung (2005).
O	Attempts to solve HCP have also been made by operations research or optimisation communities, such as nonlinear optimisation (e.g. see Filar et al. [10]) and importance sampling (e.g. see Eshragh et al. [9]).
O	Indeed, there is still a lot of interest in special properties of not only all cubic graphs, but even special classes of cubic graphs (e.g. see Horev et al. [17]).
O	In the TSP literature, some of the most successful theories and algorithms have been based on characterisations of facets of Q (e.g. see Grötschel and Padberg [14]).
O	Cubic graphs represent a natural test laboratory for this methodology, not only because HCP is still NP-complete, but also because of the availability of reliable public-domain generators (e. g. see Meringer [19]) that are capable of efficiently enumerating all nonequivalent connected cubic graphs of a given size.
O	Recently, in Baniasadi et al. [3], it was demonstrated that the set of all connected cubic graphs can be separated into two disjoint subsets, namely genes and descendants. 
O	The following two definitions are paraphrased from Baniasadi et al. [3].
O	In Baniasadi et al. [3], crackers were used to develop a decomposition theory for cubic graphs that exploits genes and crackers.
O	Since the majority of connected cubic graphs are Hamiltonian [20], the latter outcome is obviously the most common, and by itself does not provide certainty that the graph is Hamiltonian.
O	It was conjectured in [3] that mutants are extremely rare – indeed, only three such graphs containing 18 or fewer vertices exist.
P	Despite their rarity, their identification may still be considered important; one justification could be that the set of mutants is a superset to the more famous set of (nontrivial) Snarks [12].
P	A famous related approach is the, now classical, Dantzig– Fulkerson–Johnson relaxation of the TSP with subtour constraints (e.g., see Dantzig et al. [6] and Cook et al. [5]), which we will henceforth refer to as the DFJ relaxation.
O	Three previous attempts to construct a desirable polyhedron P*Q were included in the recent Ph.D. theses of Haythorpe [15] and Eshragh [8], and in Avrachenkov et al. [1].
O	In Haythorpe [15], two of these polyhedra P in variables corresponding to arcs were constructed, it was conjectured (based on empirical evidence) that both polyhedra are empty for any cubic bridge graph. 
O	Bridge graphs are always non-Hamiltonian, and it was conjectured in [11] that, asymptotically, almost all non-Hamiltonian graphs are bridge graphs. 
O	However, no other non-Hamiltonian graphs were detected using either of the polyhedra in Haythorpe [15].
O	In Eshragh [8], and later in Avrachenkov et al. [1], a related but somewhat different polyhedron was constructed. 
O	The following model is paraphrased from Cook et al. [5], where variable xia corresponds to the flow on the edge (i, a) in a tour.
O	Confirming the connectedness of a graph is also a polynomial-time process (e.g. see Siek et al. [21]).
P	When generated by GENREG [19] using the command genreg 18 3 3 it has the ID #4652. For this reason we refer to it as Γ18 4652.
N	This approach was tested for the Petersen graph and the two Blanuşa snarks, with the triangle replacing the first vertex in each graph as produced by GENREG [19].
O	According to Cross (1997), Revenue Management (RM) is the research area that focuses on the study of disciplined tactics for making product availability and pricing decisions, with the aim of maximizing revenue growth.
O	According to Weatherford and Belobaba (2002), ignoring the data censorship phenomenon can lead to demand underestimation ranging from 12.5% to 25%, and negatively affect revenue by 1% to 3%, a significant amount for major rail or airline operators.
P	Zeni (2001) and Queenan et al. (2009) have provided a comprehensive study of these methods, and have compared their respective impact on revenue. 
N	Their main drawback is that they cannot respond to sudden changes in customer behavior when a product becomes unavailable (see Sa (1987); Littlewood (2005); P¨olt (2000); Weatherford (2000); Lee (1990)).
N	On the other hand, there are many reseachers who have addressed the problem from optimization point of view (Escudero et al. (2013); Kunnumkal and Topaloglu (2010); M¨oller et al. (2008); Vulcano et al. (2012)).
O	Actually, authors such as van Ryzin (2005) have claimed that revenue management systems should focus on customer behavior and choice probabilities, rather than blindly estimating demand from historical booking data.
P	Choice-based models were introduced by Andersson (1998), and analyzed by Talluri and van Ryzin (2004) and Vulcano et al. (2010) within the framework of discrete choice theory.
P	In another research, Ratliff et al. (2008) have integrated historical demand data within a multi-flight heuristic procedure.
O	Also, Vulcano et al. (2012) have applied customer choice models to the estimation of product primary demand (firstchoice demand). 
O	Dealing with dependency yields a complex parameter estimation process that has been considered and tested by (Stefanescu (2009)) on small instances.
O	As the proportion of censored demand in historical data grows, the accuracy of the standard estimation methods decreases (see Talluri and van Ryzin (2004); Vulcano et al. (2012); Haensel and Koole (2010)).
O	For given utilities ui and choice sets Sj the choice probability pij of selecting product i on day j is computed according to the multinomial logit (MNL) formula (Liu and van Ryzin (2008)):
O	Whenever bilinear terms involve integer variables, this can be achieved by performing a sequence of Fortet inequalities to obtain a Mixed Integer Linear program (MILP) (Costa and Liberti (2012); Belotti et al. (2009)).
O	Alternatively, if variables are continuous, one must resort to solution techniques for nonconvex programs, such as McCormick inequalities, where a linear approximation of the bilinear terms involves additional variables and constraints (McCormick (1976)).
O	Then, an integer initial solution is obtained via a K-nearest neighbor algorithm to fix class membership variables, zjc (Cover and Hart (1967);Dudani (1976)).
O	In this paper, the main focus is on the algorithmic comparison with existing solvers, however, the impact of the choice of parametric or non-parametric demand models on revenue has been addressed in a separate article by the authors (Azadeh et al. (2014)).
O	The outcomes of our proposed model have been compared with those of two of the most acknowledged softwares: Knitro 8 (Ziena (2013)), a nonlinear solver (IPOPT (2013)), and Baron 11.0, a global optimization solver (BARON (2013)).
O	The software Baron was accessed through the NEOS server (see IBM (2013), NEOS (2013)).
O	According to Gershwin and Schor [20], minimizing the total buffer capacity under a minimum production rate constraint represents the primal problem, while the dual problem consists of maximizing the throughput rate of the line without exceeding a fixed amount of buffer size.
P	A pioneer research was conducted by Chow [5], who used the dynamic programming to tackle the buffer allocation problem in reliable production lines.
O	Later, Jensen et al. [26] coped with the problem of optimal location and sizing for buffer inventories in a serial production system that is subject to random variations of timing of stage failures, repair times and demand of finished goods.
P	Huang et al. [25] developed a design method including an approximate analytic procedure to evaluate the performance measure of an unreliable production line under a certain allocation strategy. 
P	Recently, other contributions adopted the dynamic programming for solving the buffer allocation problem on reliable production lines [16,22].
O	However, due to their restrictive assumptions, their applicability is rather limited [8].
O	Two heuristic algorithms, namely SEVA and Non-SEVA, based on the concept of local search, have been developed by Seong et al. [48] to maximize the line throughput rate.
P	Jeong and Kim [27] used a conventional gradient search method powered by a sophisticated algorithm to reduce the excessive computation time.
P	Gershwin and Schor [20] analyzed the two well-known problems concerning the BAP, namely the primal and the dual problem.
P	Solving the dual BAP was the objective of the research conducted by Kim and Lee [29], who proposed a new heuristic algorithm that preliminarily was validated through a numerical comparison with the Non-SEVA algorithm.
P	A significant paper in this field is that by Papadopoulos and Vidalis [43], which presented a heuristic in conjunction with a method able to determine a “good” initial solution that reduces the iterations to find the optimal solution. 
P	More recently, an improved local search technique named degraded ceiling algorithm was developed to maximize the throughput rate of unreliable production lines under avgiven number of total buffer slots [38].
N	A simple heuristic algorithm is proposed in Hillier and So [24] to estimate the buffer space in a production lines affected by failures, so that the buffer space is able to compensate for the negative effect of failures on the efficiency of the system.
O	A first approach to figure out a buffer allocation problem is that of Lutz et al. [31].
O	Spinellis and Papadopoulos [52] used both the exact numerical algorithm and the decomposition algorithm to solve the dual buffer allocation problem by means of simulated annealing and genetic algorithms.
O	Dolgoui et al. [17] developed a genetic algorithm where the tentative solutions are evaluated with an approximate method based on the Markovmodel aggregation approach. 
O	A first attempt in terms of hybrid optimization algorithms is that of Shi and Shuli [49], who incorporated the tabu search heuristic into the Nested Partitions framework.
P	A hybrid metaheuristics for solving the buffer allocation problem in unreliable tandem production lines was developed in (Dolgoui et al. [18]).
P	A genetic algorithm connected with a simulation-based evaluation method and an Artificial Neural Network, was proposed to investigate the buffer allocation of unbalancedunreliable flow lines [30].
O	The cross-entropy method is a new paradigm for stochastic optimization that Alon et al. [1] used along with a simulation approach to solve the throughput rate maximization for the BAP.
P	To determine the optimal buffer allocation for maximum line throughput rate and maximum line economic profit an artificial immune system has been recently presented [34].
O	Recently, five metaheuristics, also including a tabu search, have been compared in order to detect which search algorithm is more suitable to find an efficient solution of the BAP [41].
O	Differently from the previous tabu search procedures, as for example those proposed by Demir et al. [14] and Lutz et al. [31], PTS exploits the parallel configuration inherited from the parallel computing field to enhance the search strategy over the available space of solutions.
O	As for the exact optimization method, it consists of a MILP-based simulation/optimization approach derived from [35].
O	However, both approximation and simulation models make use of system states that, in case of lines with multiple machines and buffers, tends to become very large [51].
O	To this end, this section describes the mixed integer linear programming (MILP) model derived from [35], which both simulates the flow line and selects the optimal capacity to be allocated to each buffer.
P	Among the evaluative methods proposed in the literature, the most popular are Markov state [43,17,18], decomposition method [19] and simulation [31,35,47].
O	Tabu Search is a trajectory based metaheuristics algorithm for addressing combinatorial optimization problems ([3]).
P	It was first introduced by Glover [21] and basically consists of a neighborhood search method that keeps track of the up-to-now search path to avoid to be trapped into local optima or to try an explorative search of the solution domain.
O	Each neighbor solution is reached immediately from a given seed solution by a move [21].
N	Therefore, unnecessary and infeasible moves must be eliminated if it is possible [39].
O	The size of the tabu list has been fixed to 7, in accordance with most TS applications [45].
O	In this paper we consider an objective function composed of two functions (See Eq. (10)).
O	A common approach consists of the use of penalties [6,50,59].
O	Usually, a penalization is more efficient if its expression is related to the amount of constraint violation than to the violated constraint number [46].
P	Recently, real coded genetic algorithm has been used to solve integer and mixed integer optimization problems [11].
P	It consists of a regular real coded GA that was inspired to the HXNUM configuration presented by Deep and Thakur [12].
P	Then, a regular non-uniform mutation [36] was embedded into the proposed real-coded GA.
P	The algorithm structure is identical to the BS proposed by Demir et al. [14], the only difference being the initial solution generation mechanism that is based on the proposed ISG method.
O	Unlike the canonical PSO that uses a large inertia weight and static acceleration coefficients, namely cognitive and social parameters, the proposed PSO is inspired to the GLBest version [2], where the aforementioned parameters are adaptively managed in terms of global best and local best values.
P	Particle Swarm Optimization (PSO) is a modern evolutionary technique introduced by James Kennedy and Heberhart [28].
O	First, the three main assumptions of ANOVA were checked: normality, homoscedasticity and independence [37].
O	A KruskalWallis (KW) test [7] has been employed as it may be considered as the non-parametric counterpart of a one-way ANOVA and it may be used to test for a difference between the medians of 3 or more groups of data.
O	Therefore, it has been implemented according to the same setting used in the literature (See [14]).
P	The segmentation of the system in smaller systems could be another approach [58] to couple with the proposed PTS.
P	To the best of our knowledge, the PDPTWL has only been studied by Cherkesly et al. [10] who have developed three exact branch-price-and-cut algorithms which can solve instances with up to 75 requests within one hour of computation time.
O	In addition, a number of heuristics have been proposed for variants of the problem, namely the vehicle routing problem with time windows (VRPTW) (see [5,6,29] for a survey, and [30] for a stateof-the-art heuristic), the pickup and delivery problem with time windows (PDPTW) (see [4,19,20] for a survey, and [3,16,25] for recent heuristics), the traveling salesman problem with pickup and delivery and LIFO loading (see [7,8,17] for recent heuristics), the pickup and delivery problem with LIFO loading (see [1,9,15,17] for recent heuristics), and the traveling salesman problem with pickups, deliveries and handling costs (see [2] for a branch-andcut algorithm and [12] for a heuristic).
P	The first is the population-based heuristic of Vidal et al. [30] which can solve many variants of the VRPTW, namely the periodic VRPTW, the multi-depot VRPTW, and the sitedependent VRPTW.
P	The second is the adaptive large neighborhood search (ALNS) of Ropke and Pisinger [25] for the PDPTW.
P	In the second method, a diversification strategy inspired by that of Vidal et al. [28] is used to update the solution pool.
P	This concept was introduced by Nagata et al. [18] and extended by Vidal et al. [30] to be applied to all operators used in local search algorithms for routing problems, and is used when a vehicle arrives after the time window of a customer in order to reach the end of the time window.
O	A similar idea was used by Cordeau and Laporte [11] in the context of the dial-a-ride problem.
O	In fact, there are currently over one hundred thousand of commercial vessels actively operating throughout the world (AGCS, 2013).
P	For instance, a recent study by Aydogdu et al. (2012) indicates that while only 20% of all maritime accidents that occurred in Istanbul between 2000 and 2003 took place in the Ahırkapı Anchorage, this number increased to 56% between 2008 and 2011.
O	This high number of accidents in the Ahırkapı Anchorage has raised significant safety concerns among local maritime authorities (Aydogdu et al., 2012).
O	A variant of this problem is the NP-Hard circular open dimension problem (CODP) which is concerned with packing a given number of disks with known radii into a rectangular area with a fixed width so as to minimize the rectangle’s length (Akeb and Hifi, 2008).
P	Regarding algorithms for CODP and its variants, Stoyan and Yaskov (2004) used a combination of tree search and reduced gradient methods and Hifi and M’Hallah (2003, 2004) proposed evolutionary approaches.
P	On the other hand, Hifi et al. (2004) suggested a simulated annealing based approach whereas Rao et al. (2013) proposed nature-inspired meta-heuristics. 
P	Two specialized greedy heuristics were presented for CODP by Huang et al. (2005), and Kubach et al. (2009) proposed parallel versions of those heuristics.
O	Balaguer et al. (2011) provided a spatial analysis of recreational boats for anchorage capacity estimation based on several different sustainability scenarios.
O	Barros et al. (2011) modeled the problem of allocating anchorage positions for vessels in tidal bulk port terminals and proposed a simulated-annealing based solution where the objective was to minimize the total vessel demurrage.
P	Sasa and Incecik (2012) proposed a numerical model for simulation of anchored ships under the influence of environmental forces such as wind and waves.
O	Burmeister et al. (2013), on the other hand, analyzed risk of accidents in anchorages by using frequency models based on collision probabilities.
O	Bugaric and Petrovic (2007) studied a river terminal system for bulk cargo operations where the anchorage area was modeled as a simple first-in first-out queue with a fixed capacity independent of the size of the vessels.
P	A more general model for anchorage area capacity was presented by Fan and Cao (2000).
P	Huang et al. (2011) developed a simulation-based model to assess capacity and utilization of anchorages.
P	The simulation tool and algorithms introduced in Huang et al. (2011) were recently used in a marine traffic simulation system for hub ports (Huang et al., 2013). 
O	In fact, a simulation-based approach was also proposed in Shyshou et al. (2010) where the authors investigated the optimal number of anchor handling tug supply vessels required to move an oil rig from one location to another subject to weather and equipment related constraints.
O	There are numerous multi-objective optimization solution approaches in the literature, with the most commonly used technique being the weighted sum method (Marler and Arora, 2004, 2010).
O	The weighted sum method is a popular, easy to implement, and easy to interpret multi-objective optimization solution methodology; see, e.g., Hanaoka and Saraswati (2011); Naidu et al. (2014). Our MOAP strategy is also based on the weighted sum method.
P	Our first performance metric, Area Utilization, is aimed at assessing capacity when the anchorage is full (Huang et al., 2011).
O	Existing studies on disk packing problems give preference to corner and side locations while searching for candidate positions (Akeb and Hifi, 2008; Huang et al., 2005, 2011; Kubach et al., 2009).
O	Similar to existing studies on disk packing, we define a corner point CPi associated with circle i as the point where the circle is tangent to at least two items when its center is located at CPi (see, e.g., Huang et al. (2005)).
O	Similar to previous studies on disk packing, U tilization(CPi) is set to the “hole degree” of CPi (Akeb and Hifi, 2008; Huang et al., 2005, 2011).
O	Table 1 shows the number of arrivals in terms of vessel lengths recorded in the Ahırkapı Anchorage in 2013.
P	We compared MOAP against two recently introduced anchorage planning strategies, namely Maximum Hole Degree First (MHDF) and WALLPACK-MHDF algorithms (Huang et al., 2011).
O	Neuts [8] describes a matrix-geometric method to calculate the sojourn time distribution for a GI/G/1 system using phase type distributions.
O	Luh and Zheng [6] implemented Neuts' method in MATHEMATICA.
O	Sengupta [12] used a bivariate Markov process to model waiting time and queue length distributions in a GI/PH/1 queue.
O	His method is a “continuous analog” of the matrix-geometric method in Neuts [8].
P	Asmussen and O'Cinneide [2] verify the same result for a single-stage, multi-server queue, in a paper extending Sengupta's work to the GI/PH/c case.
P	Asmussen and Møller [1] show how to calculate the waiting time distribution in a GI/PH/c and MAP/PH/c queue, for both homogeneous and heterogeneous servers.
O	Whitt [16] also addresses single-stage, multi-server systems, but examines state dependent waiting time distributions. 
P	Rueda [10] develop an approximation for the waiting time distribution of single-stage queues with non-stationary interarrival and processing time distributions
P	Sojourn time distributions for queueing networks of singleservers have been addressed by Shanthikumar and Sumita [13], You et al. [18], and Yoon [17]. Shanthikumar and Sumita [13] approximate the sojourn time di.
O	Shanthikumar and Sumita [13] approximate the sojourn time distribution of an M/G/1 queueing system as one of three phase-type distributions (generalized Erlang, exponential, and hyperexponential), based on a “service index,” which is defined as the squared coefficient of variation of the total service time of an arbitrary job.
O	Neuts [8] showed that the convolution of phase type distributions is also phase type.
O	You et al. [18] used this observation to show how to calculate the sojourn time distribution for queueing networks, with general interarrival and service times.
P	In a paper published in Korean, Yoon [17] developed a method very similar to that of You et al.
P	Mandelbaum et al. [7] develop models for Markovian multiserver queueing networks in which customers can abandon and retry to enter
P	Gue and Kim [4] develop a state-dependent sojourn time distribution model for remaining time of a customer in a multiserver, multi-stage queueing system where the state of the system is the number of customers in each stage ahead of the customer of interest.
O	Our model is based on the work of Neuts [8], Asmussen and Møller [1], and You et al. [18]: we use the bivariate Markov process of Asmussen and Møller to extract the mean and variance of waiting times for each multi-server workstation.
P	We are interested in the time until absorption into state mþ1, whose distribution Neuts [8] defines as phase-type.
O	We use the fact that finite convolutions of phase-type distributions are also phase-type [8] to generate solutions for a queueing network, as in You et al. [18].
O	Sauer and Chandy [11], You et al. [18], and Tijms [14] showed different fitting methods that can convert a general distribution to a corresponding phase-type distribution based on the C2 of the distribution.
O	We follow the rule of You et al. [18]:
O	We now describe our model, which extends the work of Asmussen and Møller [1] and You et al. [18] to produce an approximation of the sojourn time distribution for a network of multi-server queues.
P	Compute the arrival rate and SCV of an arrival process at each workstation using the Queueing Network Analyzer (QNA) method of [15].
P	Asmussen and Møller [1] proposed a method to compute the steady-state waiting time distribution in a multi-server queue, with each server having a phase-type service time distribution.
O	They used matrix-analytic methods based on the method of Sengupta [12] and Asmussen and O'Cinneide [2].
O	We can approximate the interarrival time and three service time distributions as Erlangðm; μ) distributions according to the rule of [18] because all C2 are less than 1.
O	We use the Anderson–Darling test [5] to check the graphical agreement of the distributions between simulation and approximation results.
O	More than nine billion tons of goods are carried by ships annually (UNCTAD 2013), having an estimated range from 65% to 85% of the total weight transported in international trade (Christiansen et al., 2007).
O	Maritime transportation is the obvious choice for heavy industrial activities where large volumes are transported over long  distances since it has the lowest per unit cost of all transport modes (Christiansen et al., 2013).
O	Among the three basic modes of operation in maritime transportation (liner, industrial and tramp shipping) distinguished by Lawrence (1972), we are interested in tramp shipping which is comparable to a taxi service and follows the available cargoes (a mix of mandatory contract cargoes and optional spot cargoes).
O	Contracts between the shipping company and the cargo owner can be split into two main categories; More-Or-Less-Owner’s-Option (MOLOO) and More-Or-Less-Charterer’s-Option (MOLCO) (Brønmo et al. 2007b).
P	Sometimes, the planner can utilize this flexibility to achieve better fleet schedules and higher profits (Fagerholt and Lindstad, 2007).
O	There are mainly two types of problems that have been considered in ship routing and scheduling problems; cargo routing and inventory routing (Al-Khayyal and Hwang, 2007).
P	For instance, Brønmo et al. (2007a) presented a multi-start local search heuristic for a ship routing and scheduling problem.
P	Afterwards Korsvik et al. (2010) proposed a tabu search heuristic and Malliappi et al. (2011) developed a variable neighborhood search heuristic to solve almost the same problem.
P	Recently Hemmati et al. (2014) presented a wide range of benchmark instances for the ship routing and scheduling problem. They proposed an adaptive large neighborhood search heuristic to solve the problem. 
O	The maritime inventory routing problem is also considered by many researchers (Christiansen, 1999; Song and Furman, 2010; Engineer et al., 2012; Agra et al. 2013; Papageorgiou 2014; Andersson et al. 2015). 
O	Some of the works are based on real-life problems, for example Grønhaug et al. (2010), who worked on a maritime inventory routing problem in the liquefied natural gas business and Christiansen et al. (2011) who presented a case study from the cement industry.
N	Recently, Stålhane et al. (2014) introduced a new problem that combines traditional tramp shipping with a vendor managed inventory (VMI) service.
P	The method in (Stålhane et al., 2014) can only solve small instances, and it is therefore interesting to develop a method to solve realistically sized instances and to evaluate the contribution of introducing a VMI service for such instances.
P	Our contributions in this paper lies in 1) presenting a powerful novel heuristic to solve the problem introduced in (Stålhane et al., 2014) which enables the solution of realistically sized instances in reasonable time.
O	Often the contracted cargoes of shipping companies are based on COAs, however, as argued in (Stålhane et al., 2014), these COAs may, in some cases, be replaced by a VMI service.
P	For a mathematical model of the problem, we refer the readers to (Stålhane et al. 2014).
P	The ALNS heuristic was first introduced by Ropke and Pisinger (2006) and is an extension of the large neighborhood search (LNS) heuristic of Shaw (1997).
O	ALNS provides the possibility of having multiple destroy and repair methods within the same search process as in LNS (Ribeiro and Laporte 2012).
P	In Algorithm 3.5 a general pseudo-code for our ALNS implementation based on Hemmati et al. (2014) to solve the cargo routing problem is presented. 
P	For more details we refer the readers to Hemmati et al. (2014).
O	The algorithm is expressed in the following pseudo-code (MacQueen, 1967):
P	The algorithm starts with equal weights for all clusters and then automatically adjusts the weights using statistics from earlier iterations in accordance with the idea of adaptive weight adjustment in (Ropke and Pisinger, 2006).
N	To evaluate the ICGR heuristic, we have tested it on the instances proposed by Stålhane et al. (2014), and also generated a set of new and larger instances using the same instance generator.
O	As in (Stålhane et al., 2014), the revenues obtained from transporting mandatory cargoes and inventory cargoes have been set to zero.
P	Stålhane et al. (2014) presented both an exact column generation method and a heuristic alternative.
P	However, our heuristic can exploit this additional flexibility which is allowed by the arc-flow model in Stålhane et al. (2014), but that was not exploited by the column generation methods.
O	Table 3 shows profit loss incurred when comparing the COA solution of MOLCO and MOLOO models presented in (Stålhane et al., 2014) with VMI solution
P	Previous work (Stålhane et al., 2014) presented formulations and exact solution methods for the tactical planning problem of a shipping company offering both COAs and VMIs.
O	Andradottir [2] categorizes simulation optimization problems in two groups: continuous and discrete
O	A metamodel can be either global or local [5].
O	For example, a metamodel called Golden Region is utilized by [18].
O	Evolutionary algorithms are combined with a metamodel called Taylor Kriging [27].
O	For instance, Wang [40] uses a hybrid genetic algorithm-neural network strategy in order to determine four critical parameters of a pressure vessel.
O	Duvigneau and Visonneau [9] integrates a flow solver, which is used in computational fluid dynamics, with a hybrid optimization procedure to determine the shape of an airfoil in 2D and a wing in 3D.
O	Poloni et al. [32] use exactly the same procedures to determine the best yacht fin keel geometry using 3D simulations in a multiobjective environment.
P	For instance, Andradottir and Prudius [3] develop a strategy to balance exploration, exploitation and estimation, which are key elements in discrete problems, whereas Angun and Kleijnen [4] consider the optimization of simulation programs with multiple, stochastic outputs.
O	Also, it is shown that an MLP with a single hidden layer is a universal approximator by [8].
P	Any interested reader can find further details in a standard textbook such as [16].
O	However, as is the case with the ordinary generalized reduced gradient algorithm (GRG) [6], the newly generated point can be infeasible and a correction step may be needed.
O	Similar to MLPs, it is proven that RBFN with a single hidden layer is a universal approximator [31].
P	This unsupervised learning process can be accomplished using one of the well-known clustering algorithms such as k-means [14] or expectation-maximization [10].
O	As noted in [26], skin friction is responsible for 50%, 90% and 100% of the total drag on aircrafts, underwater vehicles, and pipelines, respectively.
P	It has some important applications in physics, chemistry, microbiology, control systems and heat management, energy generation, and display technology [34].
O	Microfludic systems allow sophisticated medical tools to be miniaturized [44].
P	Koklu and Baysal [23] deal with finding the best geometry of an actuator including four important parameters such as orifice width, orifice height, cavity height and frequency, to maximize momentum flux using simulation.
O	Kim [21] provide a survey on the control of turbulent boundary layers.
O	This method, also thoroughly discussed by [25], takes advantage of neural networks to find the correlation between wall actuation and skin-friction drag since this relation is complex due to Navier–Stokes equations.
O	Genetic algorithm-based optimization methods are used by [29] and [45] for drag reduction. 
P	For instance, Kampolis et al. [19] suggest an enhancement of this method using important factors and obtain interpolants for two applications although it is not used to solve simulation optimization problems. 
P	Ong et al. [30] use Hermite radial basis function networks and combine it with genetic algorithms and trust region approaches to solve airfoil aerodynamic design optimization problems.
O	Here, Fmin and Fmax are respectively the global minimum and maximum values of F, where the former is available from the literature (see Appendix A) and the latter is computed by global solver BARON [37].
O	As mentioned by Miltenburg [1], proper design and machinery layout is one of three important elements that help increase flexibility to respond to changes in demand.
O	The first U-shaped assembly line design was brought to the attention of the academia by Miltenburg and Wijngaard [5].
O	Some exact (e.g. an integer programming formulation by Urban [6], a branch and bound procedure by Scholl and Klein [7]) and heuristic/meta-heuristic (e.g. a simulated annealing approach by Erel et al. [8], a shortest route formulation by Gökçen et al. [9], a genetic algorithm approach by Hwang et al. [10] and an ant colony optimisation based approach by Sabuncuoglu et al. [11]) solution approaches were developed to solve the U-shaped assembly line balancing problem in its traditional form where there is only one line on which a single model is produced.
O	Among these studies, Scholl and Klein [7] and Hwang et al. [10] also aimed at minimising cycle time and obtaining a smooth workload distribution, respectively, as well as minimising the number of workstations as a common objective. 
O	Variability in processing times due to human factors (stochastic task times) on U-lines was studied by Chiang and Urban [12], Urban and Chiang [13] and Celik et al. [14].
P	Urban and Chiang [13] developed a chance-constrained piecewise-linear programme to optimally solve the U-shaped line balancing problem with stochastic task times.
P	Hybrid heuristic and ant colony optimisation approaches were proposed by Chiang and Urban [12] and Celik et al. [14], respectively. 
O	While Chiang and Urban [12] aimed at minimising the number of workstations, Celik et al. [14] considered total cost of rebalancing as the primary goal. 
O	Learning effect in processing times is considered by Toksari et al. [15] when minimising the number of workstations. Kara et al. [16] and Jayaswal and Agarwal [2] proposed integer programming model and simulated annealing algorithm, respectively, to minimise operating costs. 
O	Manavizadeh et al. [27] considered human efficiency for mixed-model Ushaped assembly lines and set labour assignment policy after balancing the line using simulated annealing algorithm based on two types of operators: permanent and temporary.
O	Yegul et al. [22] proposed multi-pass random assignment algorithm for U-shaped two-sided assembly line balancing problem with the aim of minimising the number of workstations.
P	To help multi-criteria decision making process Gökçen and Apak [17] and Kara et al. [21] proposed goal programming and binary fuzzy goal programming approaches considering two or more conflicting objectives.
O	In the former study, Gökçen and Apak [17] also considered the number of tasks assigned to each workstation as an additional goal. 
O	Therefore, line balancing and model sequencing problems have been simultaneously considered by some of the researchers to find suitable balances for mixed-model U-lines. Kim et al. [18] and Hamzadayi and Yildiz [24] proposed evolutionary based algorithms while Kara et al. [19] and Hamzadayi and Yildiz [26] proposed simulated annealing algorithm based methods to solve the problem.
O	Hamzadayi and Yildiz [24, 26] minimised number of workstations needed while Kim et al. [18] minimised absolute deviation of workloads (ADW) as primary goal.
O	Hamzadayi and Yildiz [26] illustratively showed that ADW is an insufficient performance criterion for evaluating the performance of the solutions although it is a frequently used performance measure in the literature. 
O	Kara et al. [19] aimed at optimising part usage rate and cost of setups as well as ADW in a multiobjective manner.
O	Özcan et al. [23] and Dong et al. [29] proposed genetic algorithm and simulated annealing based approaches, respectively, to solve the mixed-model assembly U-line balancing problem with stochastic task times. 
P	While ADW is considered as the performance measure by Özcan et al. [23], Dong et al. [29] aimed at minimising expectation of work overload time.
P	Please refer to Özcan et al. [23] for a detailed review of the literature on balancing and sequencing of mixed-models on U-shaped lines.
P	Also Battaïa and Dolgui [28] presented a taxonomy of line balancing problems and their solution approaches.
P	Parallel line system was first proposed by Gökçen et al. [34] and have been hybridised with various types of other line configurations successfully; see for example Ozcan et al. [35], Kucukkoc and Zhang [36-38] and Kucukkoc et al. [39] for parallel two-sided lines; Özcan et al. [40] for parallel mixed-model lines; and Kucukkoc and Zhang [41-43] and Zhang and Kucukkoc [44] for mixed-model parallel two-sided lines.
O	Gökçen et al. [34] showed that locating two straight lines in parallel to each other requires less number of workstations in comparison with performing the same tasks on individually balanced straight lines.
O	Ozcan et al. [35] and Kucukkoc and Zhang [41] showed this advantage for the parallel two-sided lines and mixed-model parallel two-sided lines, respectively. 
P	Chiang et al. [20] introduced and formalised the multiple U-line balancing problem.
P	Rabbani et al. [25] studied the mixed-model two-sided assembly line balancing problem with multiple U-shaped layout and proposed a genetic algorithm approach for the solution of the problem with the aim of minimising both the cycle time and the number of workstations.
O	Gökçen et al. [34] used the Least Common Multiple (LCM) approach, the main steps for which are given as follows: 
O	The proposed algorithm employs modifications of a well-known heuristic, namely Ranked Positional Weight Method [46], and a variant of another well-known heuristic, namely Maximum Number of Immediate Successors [47], in specific to the characteristics of the parallel U-shaped assembly line balancing problem. 
P	As mentioned by Otto and Otto [48], the quality of the solutions obtained by a priority rule-based method can be improved by ‘applying several passes of this priority rule with some kind of random influence’.
O	Test problems solved by Balakrishnan et al. [50] (and available at [49]) are taken as input data. In Balakrishnan et al. [50], one test problem is solved in each test case due to the nature of the assembly line structure.
P	However, as we consider two U-shaped lines in parallel to each other, each test problem solved by Balakrishnan et al. [50] is accommodated on each of the parallel U-shaped assembly lines in our cases.
P	Following the three-field notation of Graham et al. [13], we refer to this problem as Rjrj j Cmax; PwjTj. 
P	SThe ATCR rule was originally proposed by Lin and Lin [26] for the Rjrj j PwjTj problem.
O	The main steps of the proposed TSA are described as follows, with some notations similar to those used by Logendran et al. [27].
O	The method of computing crowding distance is based on Pasupathy et al. [36].
O	In order to obtain the parameters for the TSA we used a response surface methodology as described in Myers and Montgomery [30].
O	Release dates and due dates were generated in a manner similar to that of Mönch et al. [29].
O	In order to measure the results, we used number of non-dominated solutions, computation times, and a modified μd distance based on Dugardin et al. [9].
O	(The time-indexed formulation for single scheduling problems was originally proposed by Sousa and Wolsey [41] and extended for parallel machine scheduling problems with release dates by Lin and Lin [26].)
O	According to T’Kindt and Billaut [46], this is the ϵ constraint approach in bicriteria optimization; it is written as ϵð PwjTj j CmaxÞ in the γ part of the α|β|γ notation. 
O	To generate the set of non-dominated points, we use a similar method as described by Balasubramanian et al. [3].
O	Lourenço et al. [28] mentioned that a greedy initial solution combined with a local search can often result in high-quality solutions, and a local search from a greedy solution takes fewer improvement steps on average, which, in turn, has the local search requiring less CPU time.
P	Carlyle et al. [5] stated that a “good approximation (solutions) typically consists of a set of diverse solutions that are uniformly distributed along the efficient frontier, and which are also close to the efficient frontier."
P	We compare the proposed TSA with four existing GAs, namely the GA and PGA described by Lin et al. [25], the MPGA described by Cochran et al. [7], and the GA (denoted as GAT) described by Tavakkoli-Moghaddam et al. [45].
O	The GA and PGA [25] variant was originally designed to minimize makespan and TWT for the unrelated parallel machine scheduling (Rj j Cmax; PwjTj) problem
P	We choose the GA and PGA [25] as a basis for comparison since they perform quite well in [25] and they are easy to implement.
O	We used the same parameter settings for the GA and PGA as in [25], for the MPGA as in [7], and for the GAT as in [45].
O	Unrelated parallel machine scheduling problems exist in industries, such as scheduling operations of a printed wiring board (PWB) manufacturing line [47], scheduling dicing operations of semiconductor wafer manufacturing [21], scheduling drilling operations in a printed circuit board (PCB) industry [18], scheduling the spinning units of acrylic fibers in the textile factory [39], and scheduling the chip attach operations in the back-end semiconductor assembly line [48], etc
O	According to Brundtland (1987), sustainable development is to meet the needs of the present without compromising the ability of future generations to meet their own needs.
O	In turn, to operationalize sustainability, Elkington (1998) defined the three pillars of sustainability as economy, environment, and society. In this way, business sustainability is defined as the ability to do business with the aim of economic, environmental, and social well-being (Hassini et al., 2012).
O	Regarding the environmental aspect of sustainability, governmental legislations and policies — for instance, in Europe, Japan, and North America — are obliging the corporations to minimize their environmental impacts (Fleischmann et al., 2002; Robeson et al., 1992).
O	As our focus in this paper is on supply chain management (SCM), we use the following definition of sustainable supply chain management (SSCM) taken from Seuring (2013): “SSCM is the management of material, information and capital flows as well as cooperation among companies along the supply chain while taking goals from all three dimensions of sustainable development, i.e., economic, environmental and social, into account which are derived from customer and stakeholder requirements.” 
O	Supply chain network design (SCND) and supplier management are considered as the most crucially important strategic decisionsin SCM that play a role in overall sustainable performance of the supply chain (Pishvaee and Razmi, 2012; Seuring and Müller, 2008; Shen et al., 2012). 
O	SCND contains the determination of locations, numbers, and capacities of network facilities and the material flow between them (Pishvaee and Razmi, 2012).
P	Sustainable SCND (SSCND) tries to define the best supply chain configuration that enables an organization to maximize its longterm economic profitability as well as environmental and social performance (Chaabane et al., 2012).
P	Also, supplier management is a source of increasing concern in connention with training managers in sustainable management, legislation, expanding of organizations responsibility, and paving the way to support sustainable managerial decision making (Dao et al., 2011).
O	In the supplier management process, the potential suppliers are reviewed, evaluated, and selected to become part of the company’s supply chain, and subsequently the optimum order quantity of each supplier is determined (Weber et al., 1991).
O	Sustainable supplier management is clearly a critical activity in purchasing management due to the environmental, social sustainability, and also ecological performance that can be demonstrated by suppliers (Godfrey, 1998).
O	In other words, companies must cooperate with their suppliers about socially and environmentally friendly practices for purchasing and materials management(Hsu and Hu, 2009).
O	Accordingly, supply chains are shifting toward SSCM with various motivations, such as gaining public image (Fombrun, 2005), satisfying activists’ requirements (Spar and La Mure, 2003), and maintaining customers over the long term (Bhattacharya and Sen, 2004).
P	In this regard, there is a gap in the previously published papers in the area of sustainable network design (Chaabane et al., 2012; Elhedhli and Merrick, 2012; Hu and Li, 2009; Pishvaee and Razmi, 2012; Shen and Daskin, 2005; Srivastava, 2007; Vachon and Klassen, 2008; Wang et al., 2011). 
O	it should be noted that logistics network design is considered as an NP-hard problem (Jo et al., 2007; Syarif et al., 2002).
O	In recent years, sustainability has fascinated practitioners and scholars based on simultaneous pressure from various stakeholders including consumers, managements, nongovernmental organizations (NGOs), governmental legislation, community activists, and global competition (Govindan et al., 2013; Hassini et al., 2012).
O	Along the way, Tang and Zhou (2012) indicated that at the mercy of recent considerations to sustainability, the operations research and management science development lies within the domain of environmentally and socially sustainable operations.
P	Also, Ageron et al. (2011) have pointed out that the implementation of sustainability in supply networks is considered as a critical factor for the success of the whole supply chain.
P	SSCM aims to minimize environmental concerns and improve social impacts while, at the same time, improving long-term economic performance(Erol et al., 2011; Seuring and Müller, 2008).
P	TBL suggests that in addition to economic performance, organizations need to engage in activities that positively affect the environment and society (Elkington, 1997).
O	Among different publications about SSCM, our paper is about sustainable SCND which aims at considering the above-mentioned three pillars of sustainability in the design phase (Frota Neto et al., 2008).
O	SCND is a new, emerging approach that seeks to embed economic, environmental, and social decisions into supply chains during the design process (Chaabane et al., 2012).
P	Despite the fact that the field of SSCM is considered as being quite new, the interest in SSCM has increased rapidly over recent years (Ageron et al., 2011).
O	Seuring and Müller (2008) comprehensively reviewed a total of 191 papers about SSCM published from 1994 to 2007 and outlined major streams of research in this field.
O	Successively, Seuring (2013) has reviewed in detail 36 papers which have utilized quantitative models for SSCM among more than 500 papers published until 2010.
P	A large number of papers on sustainability and SSCM have recently been published in different journals (e.g. Bai and Sarkis, 2010; Basurko and Mesbahi, 2012; Baumgartner, 2011; Bergenwall et al., 2012; Gimenez et al., 2012; Gunasekaran and Spalanzani, 2011; Linton et al., 2007; Pagell et al., 2008; Seuring and Gold, 2013; Smith and Ball, 2012; Tseng et al., 2013).
P	For instance, Carter and Rogers (2008) concentrated their research on preparing a literature review with a conceptual framework for SSCM through reviewing journal papers and related publications from previous years.
P	Teuteberg and Wittstruck (2010) published a systematic review of SSCM.
O	Only four out of 36 surveyed papers (Cruz and Matsypura, 2009; Cruz, 2008; Cruz, 2009; Hsueh and Chang, 2008) have stated that social aspects as well, as economic and environmental aspects are considered.
P	Consequently, we are obliged to narrow down our literature review to green supply chain management (GSCM). For a more detailed and complete review of SSCM, readers are referred to Srivastava (2007), Carter and Rogers (2008), Seuring and Müller (2008), Ageron et al. (2011) and Brandenburg et al., 2014. 
O	According to reviews conducted by Srivastava (2007) and Zhu and Sarkis (2004), GSCM includes green selection of raw material suppliers, product shipment through the chain from suppliers to plants to distributors to customers, and even reverse logistics.
O	The neighboring field of our paper is the reverse SCND problem, which deals with decisions about the number,location, and capacity of collection, recovery, recycling, and disposal centers along with decisions about material flows between them (Fleischmann et al., 2004).
P	Fleischmann et al. (1997) provided the first review on quantitative models for reverse logistics. 
P	For a more detailed and complete review of reverse and closed loop supply chain management, readers are referred to Govindan et al., 2014 and Sasikumar and Kannan (2008a-b, 2009).
O	Reverse SCND has attracted a lot of researchers so far (Du and Evans, 2008; Ko and Evans, 2007; Lieckens and Vandaele, 2007; Pishvaee et al., 2010a; Salema et al., 2007; Srivastava, 2008; Soleimani and Govindan, 2014; Govindan and Popiuc, 2014; Kannan et al., 2012).
O	Closed-loop SCND is another topic that is closely related to our study (Chaabane et al., 2012; Pishvaee et al., 2010b; Qiang et al., 2013; Wang and Hsu, 2010; Kannan et al., 2009).
P	Among various proposed quantitative models of closed-loop SCND, the most relevant ones are those proposed by Pishvaee and Razmi (2012) and Pinto-Varela et al. (2011), as they both consider a trade-off between environmental impacts and economic performance of network configurations.
P	As mentioned above, there is a large body of literature about reverse and closed-loop SCND; however, according to Chaabane et al. (2012), collecting used products and re-processing might not only increase operating costs but also contribute to an increase in greenhouse gas (GHG) emissions, which defeats long-term sustainability.
P	Among the plethora of studies of forward GSCND, in terms of supply chain configuration, the most relevant one is a SCND proposed by Bachlaus et al. (2008), as it consideres five echelons and utilizes the cross-dock concept as an intermediate level between distribution centers and end customers.
P	In terms of modeling approach, objectives and the trade-off between objectives, the most relevant work is a mixed integer linear programming model proposed by Wang et al. (2011) which has two objectives: 
P	Different methods can be used for ranking suppliers, such as analytic hierarchy process (AHP) firstly proposed by Nydick and Hill (1992)for vendor selection, analytical network process (ANP) proposed by Sarkis and Talluri (2002) for supplier selection, and data envelopment analysis (DEA) firstly proposed byWeber (1996) for vendor selection.
O	In supply chain strategic design problems, different production technologies/production lines are available to be established at plants as a strategic decision which affects both costs and environmental impacts (Pishvaee et al., 2012; Shen, 2007),
O	The proposed model is based on the following common assumptions in the literature (Syarif et al., 2002; Yao and Hsu, 2009):
P	The novel method proposed in this paper is expected to fill the gap between the single solution and the Pareto optimal set by providing decision-makers with a medium-sized set of solutions (several representative solutions) from a holistic view (Li et al., 2009). 
P	Furthermore, to examine the proposed method, some experiments are designed and conducted, in which the results of the hybrid algorithm are compared to the Nondominated Sorting Genetic Algorithm (NSGAII) as one of the best multi-objective evolutionary genetic algorithms based on methods (Deb et al., 2002) and multi-objective particle swarm optimization algorithms (MOPSO) as one popular swarm intelligent algorithm (Coello Coello and Lechuga, 2002).
P	The construction of hybrid metaheuristics is motivated by the need to achieve a good trade-off between the global exploration and the local exploitation during the search (Behnamian et al., 2009).
O	Accordingly, the combination of AMOVNS as an algorithm that gives the search process a time-varying and ultimate zero probability of jumping (Zhang et al., 2007) and AMOEMA which utilizes the electromagnetism theory of physics by considering each particle to be an electrical charge (Tsou, 2009) paves the ground to avoid falling into the local minimum and eventually find the global optimum. 
P	We expect that hybridization of such two different algorithms enriches the search process and enhances the overall or local search capability and efficiency (Behnamian and Ghomi, 2010). 
P	Electromagnetism mechanism algorithm (EMA) is a metaheuristic method introduced by Birbil and Fang (2003) for global optimization.
O	It is a population-based metaheuristic structure to search for the optimal solution in continuous optimization problems (Tsou and Kao, 2006) and has rarely been used for discrete optimization problems (Vahdani and Zandieh, 2010).
O	EMA utilizes the attraction–repulsion mechanism of electromagnetism theory, which is based on Coulomb’s law to determine the optimal solution (Su and Lin, 2011).
O	It seems a natural progression to extend EMA to multi-objective optimization problems (MOOP) (Tsou et al., 2008).
P	Tsou and Kao (2006) restructured EMA to solve a MOOP and called it MOEMA.
P	As MOEMA is considered as a continuous optimization, the random-key (RK) technique as an encoding scheme is utilized with the aim of enabling the approach to solve discrete problems(Chang et al., 2009; Naderi et al., 2010; Tavakkoli-Moghaddam et al., 2009).
O	The initial value is assumed to be uniformly distributed between the corresponding upper and lower bounds (Su and Lin, 2011).
O	Each particle’s charge is calculated based on goodness of the corresponding solution that is similar to the fitness function concept in genetic algorithm (Khalili and Tavakkoli-Moghaddam, 2012).
P	The electrostatic force between the charges of two particles is proportionally calculated based on magnitudes of the particles’ charge, and it is inversely proportional to the distance between the charges (Vahdani and Zandieh, 2010).
O	For a minimization problem, the charge of each particle is estimated as follows (Tsou and Kao, 2006):
O	Obviously, particles with lower objective value have higher charge (Su and Lin, 2011).
O	In contrast, a bad solution obliges the other particles to move toward this region (Vahdani and Zandieh, 2010).
O	The forces exerted on i x by each of the other points are calculated by vector summation as illustrated in Fig. 7 (Tsou and Kao, 2006).
O	The random step length λ is generated with uniform distribution between (0, 1) (Tsou, 2009).
O	In this equation, as the imposed force is normalized and only the direction of the movement is identified, it can be guaranteed that candidate particles do not move to the unvisited particles along this direction with random step length (Jamili et al., 2011).
P	VNS is one of the most recent metaheuristics developed to solve the problems in an easier way (Hansen and Mladenovi, 2001).
P	The first multi-objective VNS algorithm(MOVNS) was proposed by Geiger (2008).
O	During the VNS process, the local optimum in each neighborhood is discovered iteratively and paves the ground for reaching the global optimum at the end (Vahdani and Zandieh, 2010).
O	MOVNS-related wealthy literature can be found in Geiger (2008) and Arroyo et al. (2011). 
P	This metaheuristic has been known and widely applied as an attractive algorithm for many optimization problems, such as transportation problems (Polacek et al., 2004), scheduling (Arroyo et al., 2011; Liang et al., 2009), etc.
O	In this paper, we propose AMOVNS with both intensification and diversification as a reinforced local search (Van  Hentenryck and Vergados, 2007).
O	AMOVNS profits from the ε -domination concept (Mostaghim and Teich, 2003).
O	In this case, the unit in the second position is located immediately after the unit in the first location, and the other units are shifted towards the right accordingly (Rabiee et al., 2012). 
O	Through this policy a part of a solution with predetermined length is selected, and then the values of its positions are exchanged with others randomly(Vahdani and Zandieh, 2010). 
O	To compare the quality of different Pareto-optimal fronts produced by multi-objective optimization methods, some performance assessment metrics are used to make quantitative comparisons between multi-objective metaheuristic algorithms (Behnamian and Fatemi Ghomi, 2011). 
O	As these metrics assess the quality of approximated Pareto sets from different perspectives and due to their conflicting and incommensurable nature, various metrics should be used simultaneously (Behnamian et al., 2009). 
O	Is considered as a diversity measure and evaluates the standard deviation of distance of an ideal point from a/an non-dominated set (Behnamian and Fatemi Ghomi, 2011).
O	Data envelopment analysis (DEA) developed by Charnes et al. (1978) is usually applied to evaluate the performance of some choices with some attributes compared to each other (Wu and Blackhurst, 2009).
P	A more detailed theoretical introduction of DEA can be found in Cooper (2004).
P	Subsequently, all non-dominated solutions obtained by the algorithms are combined and the efficiency of these points is calculated by the DEA model, which introduced by Amin (2009); Amin and Toloo (2007).
N	Parameter setting is identified as an important milestone to achieve robust metaheuristic algorithms by not producing functional variance under external environmental influence (Gholami et al., 2009).
O	This process intends to find good values for the parameters before the run of the algorithm, which remains fixed during the run (Behnamian et al., 2010).
P	Generally speaking, the efficiency of the algorithms is heavily based on miscellaneous factors, such as the assigned values to parameters (Vahdani and Zandieh, 2010).
O	In this regard, this paper employs the response surface method (RSM) as a well-known technique that is widely applied in a variety of industrial settings and parameter optimizations, to determine the optimal values of the parameters (Rabiee et al., 2012).
P	RSM is a collection of mathematical and statistical techniques developed by Box and Wilson in the early 1950s (Myers et al., 1971 and 2009).
O	Multiobjective label-setting algorithms are generally extensions of single objective ones, and particularly of Dijkstra's algorithm and An [14].
P	Hansen [13] first extended Dijkstra's algorithm to the biobjective problem, and showed that even with two objectives thenumber of Pareto-optimal solution paths can grow exponentially with solution depth in the worst case.
O	However, there are interesting classes of MO search problems where this worst-case behavior does not appear [25,28].
O	Martins [27] provided an exact label-setting algorithm for the general case, i.e. each label scanned by the algorithm is Pareto-optimal.
O	These include MOAn [36], TC [38], and NAMOAn [26].
O	Tung and Chew also proposed using the ideal point of each node n as a lower bound estimate of the cost of all paths from n to the goal [38].
O	More details about this algorithm can be found elsewhere [26].
P	For the case with two objectives the label sets GopðnÞ; GclðnÞ and COSTS can be ordered lexicographically, allowing for efficient dominance checks [33].
O	If the estimate function h !ðnÞ returns an optimistic estimate (lower bound) of the cost of any path from n to the goal, then it is guaranteed to terminate with the set of all non-dominated solution paths to the problem, i.e. NAMOAn is an exact algorithm (see Theorem 4.9 in [26]).
O	If the lower bound estimate satisfies the so-called monotone property, then NAMOAn explores only non-dominated labels (i.e. it is a label-setting algorithm) and is optimal in the class of admissible algorithms (see Theorem 6.4 [26])
P	Assumption 1 is satisfied by the precalculated ideal-point lower bound proposed by Tung and Chew [38].
P	All versions of NAMOAn employ the ideal point lower bound proposed by Tung and Chew [38] for all problems.
O	This set is determined solely by the quality of the lower bound estimate, as long as a nondominated label is always selected for expansion (see Theorem 5.9 [26]).
O	An economic cost criterion was further introduced in [20].
O	The NY city map is considerably larger. Previously reported runtimes of NAMOAn solving a set of random biobjective problems on this map (minimizing c2 and c3) were up to several days [20].
P	This procedure has been previously used to compare exact algorithms on difficult road map problem instances [31,32,22].
P	Regarding the New York city road map, we selected the first twenty problems proposed in [18].
O	This is in accordance with recently reported results over biobjective problems [15,21].
O	Usual additional hypotheses include the simultaneous availability of all jobs and all machines and deterministic processing times, among others (see e.g. Framinan et al. [10] for a complete list of assumptions).
P	Several criteria can be established to measure the performance of the different schedules (see e.g. Sun et al. [21]).
O	Among them, the maximum completion time of a sequence or makespan is related to resource usage (see e.g. Ruiz and Maroto [19] and Fernandez-Viagas and Framinan [7]), while tardiness refers to the delay of the completion time of a job with respect to its committed due date (see e.g. Ruiz and Maroto [5] and Fernandez-Viagas and Framinan [9]).
O	According to the notation by T'Kindt and Billaut [24], the problem described in the previous paragraph can be denoted as Fmj prmuj ϵðCmax=TmaxÞ.
P	In this regard, the works by Daniels and Chambers [2], Chakravarthy and Rajendran [1], Framinan and Leisten [8], and Ruiz and Allahverdi [18] develop different heuristics either for the problem, or for general cases of the problem.
O	As mentioned in Section 1, the problem is NP-hard since the minimisation of each individual criterion is already an NP-hard problem for the permutation flowshop (see e.g. T'Kindt and Billaut [24] for a detailed proof).
P	Given the clear connection between our problem and that of makespan minimisation, most of the algorithms to solve the problem are based on the best heuristic for makespan minimisation: i.e. the NEH heuristic by Nawaz et al. [14].
P	In Taillard [22], a mechanism – named in the following Taillard's acceleration – is proposed so the computational complexity of evaluating all insertions is equivalent to that of evaluating one sequence.
O	Among the contributions on the Fmj prmuj ϵðCmax=TmaxÞ problem, Daniels and Chambers [2] were the first in proposing a constructive heuristic
P	Chakravarthy and Rajendran [1] propose a simulated annealing algorithm to solve the Fmj prmuj ϵðZ=TmaxÞ where Z ¼ λ  Cmax þ ð1 λÞ  Tmax, λA½0; 1.
P	Framinan and Leisten [8] propose a constructive heuristic, denoted in the following as FL, based on the NEH algorithm to solve the Fmj prmuj ϵðCmax=TmaxÞ problem.
O	The heuristic is compared with those of Daniels and Chambers [2] and Chakravarthy and Rajendran [1] for small and big instances.
P	Finally, Ruiz and Allahverdi [18] propose an iterated optimisation algorithm to solve the Fmj prmuj ϵðZ=TmaxÞ problem.
P	More specifically, they proposed a high-performance Genetic Algorithm (GA in the following) where the selection procedure is based on n-tournament (see Ruiz and Allahverdi [17]).
O	Recall that the completion times of the jobs in Π placed after the insertion of a job σ must increase at least mini pi;σ  ; iA½1; …; m (see also Fernandez-Viagas and Framinan [4]).
O	The pseudocode of this local search method is shown in Fig. 6, which is similar to those in Pan et al. [16] and in Fernandez-Viagas and Framinan [4].
P	The temperature parameter has been generated following the suggestions by Osman and Potts [15] (see e.g. Fernandez-Viagas and Framinan [4], Pan et al. [16] and Ruiz and Stützle [20] for similar approaches).
O	ANPA is tested following the same calibration test as in Vallada and Ruiz [25], which in turn is based in Vallada et al. [26].
P	In this section, the performance of the proposed algorithms BICH and ANPA is compared with the best algorithms so far for the problem, i.e. the GA by Ruiz and Allahverdi [18] and the constructive heuristic FL by Framinan and Leisten [8].
O	The algorithms are tested using the set of instances of the benchmark of Vallada et al. [26], which includes n ¼ f50; 150; 250; 350g jobs and m ¼ f10; 30; 50g machines.
P	The same stopping criteria as in Ruiz and Allahverdi [18] are applied for the iterative improvement algorithms. 
O	Nevertheless, we also include it in the analysis since it is the usual way in which this analysis is carried out (see e.g. Framinan and Leisten [8] and Ruiz and Allahverdi [18]).
O	The p-value of each hypothesis is calculated using a nonparametric Mann–Whitney test (see Pan et al. [16]).
O	Regarding the computational time requirements, note that the average CPU time is both instance- and instance-size-dependent indicator (see Fernandez-Viagas and Framinan [6]).
O	Particularly, the average relative percentage computation time described in Fernandez-Viagas and Framinan [6] has been tested with similar results.
P	The above analyses have been performed with processing times following a uniform distribution, as it is usual in the literature for the PFSP (see e.g. the benchmarks of Taillard [23] and Demirkol et al. [3]).
O	The procedure to generate the three benchmarks is the same as in Hopp and Spearman [26], with the exception of the distribution of the processing times, which follow exponential (positive and negative) and normal distributions, respectively.
O	In the case of the normal distribution, the standard deviation is chosen to achieve a moderate variation of the processing times which means, according to Hopp and Spearman [13], a coefficient of variation between 0.75 and 1.33.
O	About 30 years ago ZoBell (1946) reported some observations, parts of which are quoted in Table 3-1 and which might aptly serve as a starting point for the present discussion.
O	Although they seem to thrive better at lower salt concentrations (Brock, 1975) they apparently grow and reproduce quite well in the hypersaline brine of the northern basin.
O	This is interesting in view of the fact that in the southern and less saline basin of Great Salt Lake D. viridis is found to be the principal planktonic alga, reaching levels as high as 2 x lOS/ml, i.e. a biomass of about 150 g/m3 (Stephens and Gillespie, 1976).
O	According to Post (1977) nitrogen seems to be a key element in the ecosystem of the northern basin. Nitrate and nitrite could not be detected in the brine. 
N	The chemical nature of the soluble organic material has not been determined (Post, 1977). 
O	Post (1977) reported that when simply keeping lake water from the northern basin in the laboratory at 28OC for 6 months, the bacteria and algae counts rose by one or more orders of magnitude above the highest levels observed in the lake.
O	According to Nissenbaum (1 975) one should from an ecological point of view distinguish between the upper water mass, from the surface to about 80 m, which is aerobic and has temperatures in the range 23-36"C, and the lower water mass below 80 m which is anoxic, contains H,S (0.5-1 mg/l), and has a fairly constant temperature in the range 21-23°C. 
O	The Dead Sea was traditionally believed to be completely barren of indigenous life until Volcani (1940, 1944) published his observations almost 40 years ago. 
P	Since that time little was done to gain further understanding of the life in the Dead Sea until the work of Kaplan and Friedman (1970) who were the first attempting to quantify the biota of the Dead Sea (Table 3-4). 
O	The microbiology and biogeochemistrv of the Dead Sea was ably reviewed and discussed by Nissenbaum (1975). 
P	This corresponds to a biomass of the same order as that observed by Post (1977) for D. salina in the northern basin of Great Salt Lake.
O	The indications are that the bacterial sulfate reduction is extremely slow in the water column, but considerably more intense in the sediment (Nissenbaum, 1975).
N	In addition NH, (2-6 mg/l) and organic N (0.3-3 mg/l) are quite high, but phosphate is very low and may possibly be a limiting factor (Nissenbaum, 1975).
N	Measurements have shown that the light intensity is reduced to 1% at 30 m depth (Kaplan and Friedman, 1970).
O	Baas Becking (1928) reported the occurrence of red-colored bacteria and cyanobacteria from such localities in western United States.
O	The red-colored bacteria may be present in such numbers that they color the brine, and Jannasch (1957), studying “die bakterielle Rotfarbung” in the alkaline salines of Wadi Natrun in Egypt, found that the dominating red-colored bacteria there were phototrophic sulfur bacteria (Thiorhodaceae).
O	Raymond and Sistrom (1967, 1969) described an organism isolated from Summer Lake, Oregon, which they named E. halophila; Imhoff and Triiper (1977) described an organism isolated from Wadi Natrun, Egypt, which they named E. halochloris.
O	At present Triiper and his collaborators are engaged in an extensive study of the ecology of the alkaline hypersaline lakes of the Wadi Natrun area in Egypt (Imhoff et al., 1978).
O	Imhoff et al. (1978) report on the chemical composition of six of the lakes of the Wadi Natrun. 
O	None of the six lakes investigated by Imhoff et al. (1978) contained animals ranlung above unicellular protozoa.
P	Imhoff et al. (1978) reported biologically important compounds of carbon, sulfur, nitrogen and phosphate to be present in abundance in all the Wadi Natrun lakes studied,and these elements can therefore not be considered growth limiting, although certain compounds of certain of the elements, e.g. CO, and H,S, may be limiting at certain times.
O	The biological productivity in the dilute brine of the first evaporation pond(s) is quite high (Carpelan, 1957).
P	The number of species able to develop decreases rapidly as the brine becomes more concentrated, and so does the biological productivity of the ecosystem (Copeland and Jones, 1965).
O	The brine algae, Dunaliella viridis and D. salina become the dominating primary producers (Gibor, 1956a, b; Nixon, 1970).
O	The red-colored bacteria of the genera Halobacterium and Halococcus have been considered the dominating types of concentrated brines in marine salterns (Larsen, 1962).
O	Dundas (1977) reported recently that he had found the majority of bacteria in redcolored brines from the marine salterns to be colorless.
O	Nixon (1970) reports 3500 individuals/mz from a marine saltern of 35% salinity in Puerto RICO.\
O	Nixon (1970) reported figures of about 100 mg/l of organic C, which should roughly correspond to 200 mg/l of organic matter. 
N	The organic components of the brines of marine salterns have not been further identified, but Nixon (1970) remarks that excretion products of the algae may make up a substantial part.
N	Nixon (1970) reported that the levels of inorganic phosphate and nitrate were very low in brines from marine salterns.
P	The red-colored bacteria of strong brines have attracted considerable attention, and some insight in their physiological and biochemical peculiarities has been gained (Larsen, 1973; Dundas, 1977).
O	In addition to ionic interactions hydrophobic phenomena of the proteins seem to play an important role in relation to the salt (Lanyi, 1974). 
O	Halobacterium volcanii (Mullakhanbhai and Larsen, 1975) isolated from Dead Sea mud has an optimum requirement for NaCl of only about lo%, and this is less than half of that required optimally by other halobacteria.
P	In all cases investigated the halobacteria and the halococci have been found to have a considerable requirement also for Mgt+ (1-5% MgCI,), and this has been reported as possibly a general property of these organisms (Larsen, 1967).
O	Recently has been described mass occurrence of halobacteria in the lakes of Wadi Natrun which are extremely low in Mg" and Ca+' (Table 3-2, Imhoff et al., 1978).
O	It has been suggested that the halobacteria use this system as an auxiliary device for ATP production in the cell under oxygen limiting conditions where a limited amount of ATP is synthesized via the respiratory chain (Oesterhelt, 1976).
O	When freshly isolated from nature the halobacteria often produce gas-filled vacuoles which provide buoyancy to the cells (Walsby, 1975).
O	Petter (1932) who first described gas vacuoles in the halobacteria, put forth the reasonable hypothesis that a function of these structures is to lift the obligate, aerobic bacteria, which live in an environment of low oxygen solubility, towards air.
O	Biochemical considerations on the basis of our present knowledge imply an extremely large number of mutations in order to convert a non-halophile to an extreme halophile or vice versa (Larsen, 1962, 1967, 1973).
O	These findings have nourished an idea that the extremely halophilic character might come about by some special gene-transfer mechanism (Dundas, 1977).
P	At lower salt concentrations correspondingly lower contents of glycerol were found inside the algal cells (Ben-Amotz and Avron, 1973; Borowitzka and Brown, 1974).
O	As pointed out by Imhoff et al. (1978) a mass development of phototrophic sulfur bacteria cannot be expected in the Dead Sea because of the lack of H,S in the photic zone.
P	They are therefore not so frequently encountered, and appear unhealthy, in the strongest brines (Nixon, 1970).
O	There is, however, one outstanding exception to this rule, namely the mass occurrence of apparently healthy cyanobacteria in the strongly saline lakes of Wadi Natrun (Imhoff et al., 1978).
O	When grown at 3 M NaCl Aphanothece hahphytica accumulates K+ to a concentration of somewhat less than 1 M. Little or no Na+ is taken up (Miller et al., 1976).
O	An organic solute of a polyol nature, and possibly free amino acids in additon, seem to be the main osmoregulators, thus displaying a regulatory mechanism reminding of Dunaliella and other eukaryotes (Tindall et al., 1977).
O	Very little is known about these bacteria, but in view of the findings that about half of the organic matter in marine sediments may be mineralized by anaerobic sulfate respiration (Fenchel and Jorgensen, 1977), such a process may possibly be of considerable importance also in the element cycling of the hypersaline ecosystems.
O	In his seminal paper of 1994, “Needed: an empirical science of algorithms”, Hooker [1] challenged the OR community to augment its theoretical focus on worst-case or average-case analysis of algorithms with a more empirical approach to algorithmic analysis: one that enables better understanding of the likely performance of algorithms on diverse test instances.
P	As such, he was proposing a paradigm shift in OR towards an experimental mathematics approach [2].
P	In a follow-up paper in 1995, “Testing heuristics: we have it all wrong” , Hooker [3] argued that randomly generated test instances lack diversity and rarely resemble real-world instances.
O	Obtaining a sufficient number of real-world or real-world-like instances for statistical inference can be difficult, especially since synthetically generated instances of problems often have quite different properties and underlying structure to real-world instances [4–6].
P	The opportunity now exists to challenge and extend these benchmarks, and to generate new test instances that enable strong inferences to be made about algorithm strengths and weaknesses to support objective algorithmic testing [8].
O	Examples include the DIMACS challenge which created new graph colouring benchmarks [4]; efforts to generate more real-world-like instances of well-studied problems [9–12]; new instances that are more challenging for particular algorithms [13,14]; and some new instances with controlled characteristics to support experimental mathematics [15,16].
O	In the field of graph colouring, Culberson states on his webpage [17] about his graph generators, “my intention is to provide several graph generators that will support empirical research into the characteristics of various colouring algorithms”. 
O	Greater awareness now also exists for the importance of rigorous testing of algorithms [18].
O	The design of experiments approach randomly generates instances by varying easily controlled parameters (usually just a subset of a more comprehensive feature space) to create a Latin hypercube design [15].
O	The idea of using a genetic algorithm (GA) to evolve instances with desirable characteristics has been around for a decade, starting with creating instances that are hard or “worst-case” for an algorithm [13,14].
O	Extending these ideas to create new instances that are easy or hard, and uniquely easy or hard for particular algorithms, new Travelling Salesman Problem instances have been evolved that have helped to learn the strengths and weaknesses of algorithms more effectively than relying on random or benchmark instances alone [19,20].
O	Typically this is done by studying a small set of real-world instances and making small variations to some key parameters [11,12].
O	New timetabling instances were generated using this method, and shown to produce instances that are more similar to real-world instances than those of the seed instance generator [22], while simultaneously eliciting different performance behaviours from competitive algorithms.
O	Our recent work [8] has developed powerful new methodologies to enable objective assessment of optimization algorithm performance within such an instance space.
O	We have applied this methodology to assess the power of state-of-the-art optimization algorithms in an objective manner [8,25,26], and shown that existing benchmark instances only populate a small portion of the available instance space [8,21,24], and often do not coincide with the location of real-world instances [21].
O	The first paper [27] identified the properties of instances that affect difficulty, for broad classes of combinatorial optimization problems, and provides the starting point for constructing instance spaces for a new problem. 
P	The second paper [8] demonstrated how to construct an instance space, and how visualizing and measuring the area of algorithm footprints in the instance space can provide the objective assessment of algorithm strengths and weaknesses we seek.
N	Assuming that we have an adequate set of features, we can then use dimension reduction techniques to visualize the instances in a 2-d instance space, after verifying that the inevitable loss of information has not destroyed too much of the topology [29].
P	This framework has been developed in our previous work [8], by extending the Algorithm Selection model of Rice [30,31] (shown in the blue box in Fig. 1) to consider visualization of instance spaces and the insights that can be learned within them.
P	Then the instance space can be used to visualize the regions where an algorithm is predicted to perform well based on previous empirical evidence, and machine learning methods can be used to define the boundary of good performance, known as the algorithm footprint [24,8].
O	The ideal instance space is one that maps the available instances to a 2-d representation in such a way that the easy instances and hard instances are well separated and the topology of instance (dis)similarity is preserved [29].
O	This description is a summary of the more detailed discussion found in [8].
O	Let e be the energy of a graph [33], defined as the sum of absolute values of the eigenvalues of the graph's adjacency matrix.
O	Further bounds are provided in Yu et al. [37], but inequalities (2)–(4) are the only ones that directly relate the properties of interest, namely the energy or its mean in terms of the density and size of a graph.
O	The use of inequalities in feature spaces to empirically support theoretical analysis of upper and lower bounds has already been proposed [39], and our proposed methodology of evolving new instances at target locations in the instance space can clearly lend weight to this effort, with augmented collections of instances strengthening empirical evidence.
O	For both heuristics we used the implementation available for download from Culberson's website [17].
O	We will need to examine how the fitness function can be adapted to ensure evolved test instances are discriminating, challenging and include real-world-like instances, adapting ideas from our previous work [21].
P	Because of the practical importance of this family of problems and the difficulty to solve them optimally (they are NPhard; see, e.g., [30]), a continuous research activity has been done over the last decades.
O	Recent surveys are Becker and Scholl [4], Scholl and Becker [26], Boysen et al. [5,6] and Battaïa and Dolgui [2].
O	Among these problems, the simple ALBP (SALBP) has been traditionally the most studied [3].
P	For instance, among others: parallel workstations [18], parallel tasks [15], multiple products [25], mixed-models [12], U-shaped lines [21], stochastic task times [13], setup times between tasks [20], task times depending on the sequence [9], incompatibility between tasks [24], constrained resources [11] and ergonomics considerations [10].
O	For example, it can be found in the manufacturing of PCBs [22] on specific robotic lines.
O	As a result, automated machines are required which are capable of placing small-sized surface-mount devices with the necessary precision onto the board [17].
O	A more detailed description of the AWALBP can be found in Calleja et al. [7].
P	Most of the related work has been inspired by throughput optimisation in PCBs lines and several procedures have been proposed to solve the levels L2 [28,27] and L3 [19,14].
P	More general approaches to solve the AWALBP-L2 are proposed in Müller-Hannemann and Weihe [22] and Calleja et al. [7,8] but always under the assumption that each task can be processed in only one workstation.
P	2 A. García-Villoria et al. / Computers & Operations Research 62 (2015) 1–11 and Weihe [22] solve AWALBP-L1 heuristically and Calleja et al. [8] propose an efficient mathematical model called Task model to solve the problem optimally (again, for the case that each task can be processed in only one workstation); both works assume that each task can be processed in only one workstation.
O	Tazari [28] and Tazari et al. [29] deal a case of AWALBP-L1 in which each task can be processed in several workstations.
P	Thus, according to the classification proposed in Boysen et al. [5], the problem to solve is classified as [spec, fix| |c].
P	To do the assignment, the Task model proposed in Calleja et al. [8], which is very efficient, is used.\
O	With respect to the second model (Task model), Calleja et al. [8] reported average computing times of few miliseconds, so we expect that the 10 s limit will rarely be active in practice (in our computational experiment, we confirmed the assumption).
O	We generated a set of test instances based on those 1200 instances of AWALBP-L2 used in Calleja et al. [7,8], which are based on the description of real-world cases.
P	Recall that whereas in the AWALBP-L2 solved in Calleja et al. [7,8] it is assumed that each task can be performed only in one workstation, in this work the tasks can be performed in multiple workstations. 
P	While a significant number of heuristic approaches have been proposed in the literature for tackling the TOPTW (for a survey see [10,26]), to the best of our knowledge, the only TDTOPTW heuristic has been recently proposed by Garcia et al. [9].
P	The performance of the TD_CSCR, TD_SℓCSCR and AvgCSCR algorithms has been compared against a time dependent extension of the most efficient known TOPTW heuristic (Vansteenwegen et al. [27]) as well as the approach proposed by Garcia et al. [9] using precalculated average travel times between POIs.
P	Tricoire et al. [23] are known to yield the highest quality solutions.
P	Erkut and Zhang in [6] considered the Maximum Collection Problem with Time Dependent Rewards (MCPTDR) where each node's profit decreases linearly over time, and the objective is to maximize the sum of the rewards collected in a single tour.
P	The Orienteering Problem with Variable Profits was introduced by Erdogan and Laporte [5] as a variant of the OP in which the percentage of the collected profit at each node depends either on the number of discrete passes or in an alternative model, on the continuous amount of time spent at the node.
O	Yu et al. recently [30] presented a mixed integer programming approach for solving a more general problem that allows multiple starting nodes (depots) and the profit collected at each node is characterized by some non-decreasing function over the time spent at this node.
P	Verbeeck et al. [29] suggested a mathematical formulation of the TDOP and proposed a fast local search based metaheuristic to tackle the problem.
P	Abbaspour et al. [1] investigated a variant of the Time Dependent OP with Time Windows (TDOPTW) in urban areas, and proposed a genetic algorithm for solving the problem.
O	The work of Garcia et al. [9] is the first to address algorithmically the TDTOPTW.
P	Similar to the approach proposed by Garcia et al. [9], AvgCSCR is based on average travel times to handle time dependent travel costs among locations and integrate public transportation.
O	Hence, relevant algorithmic solutions should unavoidably be tested upon real transit network data (for instance, Garcia et al. [9] used timetabled data of the San Sebastian bus network, provided by the local transportation authority), to validate their solutions.
O	Using the method of Dibbelt et al. [4], we compute offline pairwise full (24 h range) multimodal timedependent travel time profiles.
P	The AvgILS refers to the average travel time approach proposed by Garcia et al. [9], wherein the standard ILS algorithm [27] is used to construct routes based on pre-computed average travel times.
O	The class of problems referred to as fixed interval scheduling problems considers the scheduling of resources for machine scheduling operations within fixed time windows, but without any sequence-dependence(for example, see Kroon, Salomon & Van Wassenhove [1997]).
P	For example, the traveling salesman problem (TSP) with time windows (TSPW) has been discussed in many seminal papers including Savelsbergh [1985], Padberg & Rinaldi [1991], Gendreau, Hertz, Laporte & Stan [1998], and others
O	Such problems have been studied under the context of history dependent scheduling (see Andersson, Chaki, de Niz, Dougherty, Kegley & White [2012], Lee, Lei & Pinedo [2012]).
O	Cargo container terminals often hold thousands of containers at a time and the daily throughput may exceed 100-500 containers (see AAPA rankings [2010], World Shipping council [2013] for details).
O	Our primary motivation behind the GPSP is to optimize the scheduling of cranes used in cargo container terminals to handle the internal movement of containers (see for example [ Dayama, Ernst, Krishnamoorthy, Narayanan & Rangaraj, 2013]).
O	In machine scheduling terminology (defined by Graham, Lawler, Lenstra & Kan [1979]), Aj constitutes the release time and Bj the deadline for job j.
O	Christofides, Mingozzi & Toth [1981] gives early (exact) approaches that are applicable in the discussion of GPSP. Within this context, variants relevant to GPSP include the multiple TSP which allows for multiple resources (see Bektas [2006]) and TSP with time window bounds (see Dumas, Desrosiers, Gelinas & Solomon [1995], Ascheuer, Fischetti & Gr ¨otschel [2000], Ascheuer, Fischetti & Grotschel [2001]).
P	Many diverse applications (see Ernst, Krishnamoorthy & Storer [1999]) address cases where sequence dependence, multiple resources and time windows are all involved in the same problem.
O	Neglecting immediate-precedence, history-dependence has also been studied in the literature (see Andersson et al. [2012], Lee et al. [2012], Yin, Ruan & Sun [2010]).
P	Effectively, history dependence can be transformed to the feedback arc-set problem (see Karpinski & Schudy [2010]).
O	Biskup & Herrmann [2008] and Cheng, Lee & Wu [2010] discuss history dependence along with time window bounds for single machine scheduling cases.
O	Yin et al. [2010] further generalizes this result by considering only a fixed number of jobs in history.
O	Vehicle routing problems consider scheduling of multiple vehicles, including cases where jobs have time window restrictions (see Desaulniers, Desrosiers, Erdmann, Solomon & Soumis [2002], Dumas, Desrosiers & Soumis [1991], Savelsbergh & Sol [1995] for details on VRP with pickup and delivery).
O	Finally, fixed-job scheduling problem or fixed interval scheduling are a class of problems where a set of resources fulfills tasks within time restrictions without any precedence considerations (see Dantzig & Fulkerson [1954], Kroon et al. [1997] and Krishnamoorthy & Ernst [2001]).
O	Figure 1 depicted a situation which appears quite similar to CBWP as discussed in Gupta & Nau [1992] and in Slaney & Thi´ebaux [2001].
O	Assignment of multiple parallel resources to time bound tasks (interval scheduling problem) is often modeled on a ”conflict graph” or ”interval graph” (for example, see Kroon et al. [1997] or Krishnamoorthy & Ernst [2001]).
O	Some impacts of the time windows on feasibility of paths have been modeled in the literature using directed acyclic graphs called as Precedence-Graphs (for example, see Ascheuer et al. [2001]).
O	Any sub-tours in ISCG are avoided by Equation ( 33) which acts as a sub-tour elimination constraint similar to th MTZ constraints introduced by Miller, Tucker & Zemlin [1960].
O	From this, if we were to neglect all the δ equations, the HDSG aspect and the availability of multiple resources, then the remaining equations in this formulation closely resemble the standard formulation for TSP with time window (see Ascheuer et al. [2001]).
O	For example Ascheuer et al. [2001] attempt to identify and preclude time-infeasible paths by developing constraints only on the immediate precedence variable (like U i j variable in HPF).
O	We take the problem data instances used by Solomon [1987], Potvin & Bengio [1996], Pesant, Gendreau, Potvin & Rousseau [1998] and others. 
N	Third, we improve the Lagrangian heuristic proposed by Fiorotto and de Araujo (2014) to obtain better upper bounds.
O	Areas of production that consider parallel machines are the pharmaceutical industry (De Matta and Guignard, 1995), plastic sheet production (Mergaux and van Wassenhove, 1984), tile production (De Matta and Guignard, 1994), the tire industry (Jans and Degraeve, 2004b), bottling of liquids and others (Carreno, 1990) and packaging (Marinelli, 2007).
P	Considering the problem with identical parallel machines, Lasdon and Terjung (1971) propose a heuristic for a lot sizing and scheduling problem with no machine setup time
O	Carreno (1990) proposes a heuristic for the Economic Lot Scheduling Problem (ELSP), i.e. with a constant demand rate, with setup times for parallel machines and solves problems with one hundred items and ten machines in fast computational times.
P	Jans (2009) proposes new constraints to break the symmetry that is present due to the identical machines and tests his approach using a network reformulation for the problem. 
P	Tempelmeier and Buschkuhl (2009) consider the multi-stage problem with setup carry-over (a setup is maintained between adjacent periods) and develop a Lagrangian heuristic.
P	For the unrelated parallel machines case, Toledo and Armentano (2006) relax the capacity constraints and propose a Lagrangian heuristic to solve the problem.
O	The authors use a strong reformulation of the problem and instead of the capacity constraints, they relax the demand constraints using Lagrangian relaxation.
P	They also propose a heuristic to find feasible solutions and compare their results with Toledo and Armentano (2006).
P	Multi-stage problems with unrelated parallel machines were studied in Ozdamar and Birbil (1998), who present a generic model in which the multistage case can be considered.
P	Stadtler (2003) and Helber and Sahling (2010) also analyze the multi-stage problem.
P	Stadtler (2003) proposes a period decomposition heuristic and, to solve each subproblem, a reformulation based on the facility location problem is used.
P	Helber and Sahling (2010) propose a fix-andoptimize approach and obtain better results than those obtained by Stadtler (2003).
O	Salomon et al. (1991) study the Discrete Lot Sizing and Scheduling Problem (DLSP) with parallel machines, and analyze the complexity for the cases of identical and non-identical machines.
O	Kang et al. (1999) propose a method based on column generation and branch-and-bound.
P	Belvaux and Wolsey (2000) describe a generic model and an optimization system that is capable of solving a wide range of lot sizing problems including special cases with different items and parallel machines. 
O	Meyr (2002) present a general model that consists of an extension of the General Lot Sizing and Scheduling Problem (GLSP) model for the case in which both setup cost and time are sequence-dependent.
P	Fandel and Stammen-Hegener (2006) also present a model based on the GLSP model and consider the multi-stage case.
P	Marinelli (2007) proposes a solution approach for a real capacitated lot sizing and scheduling problem with parallel machines and shared buffers, arising in a packaging company producing yoghurt.
P	Finally, Meyr and Mann (2013) propose a heuristic for the lot sizing and scheduling problem on parallel machines. 
O	Before the seminal paper of Dantzig and Wolfe (1960), Manne (1958) had already implicitly applied the ideas of decomposition for the lot sizing problem with dynamic demand considering several items and capacity constraints.
O	Lambrecht and Vanderveken (1979), Bitran and Matsuo (1986) and Degraeve and Jans (2007) further discuss the formulation proposed by Manne (1958).
O	Degraeve and Jans (2007) show that the decomposition proposed by Manne, while valid to calculate a strong lower bound, has a structural deficiency when it aims to solve the problem with integrality constraints.
P	Dzielinski and Gomory (1965) use column generation to handle the formulation with the large number of variables proposed by Manne (1958).
O	Indeed, Manne’s formulation is the full master problem obtained when one applies Dantzig-Wolfe decomposition (Dantzig and Wolfe, 1960) to a formulation with a smaller number of variables.
P	Dzielinski and Gomory (1965) also note that the subproblems that must be solved to generate columns are equivalent to the problem studied by Wagner and Whitin (1958).
P	Lasdon and Terjung (1971) develop a column generation approach to handle large problems
O	Algorithms of this type are also addressed by Bahl (1983), Cattrysse et al. (1990), Salomon et al. (1993) and Huisman et al. (2005).
O	Hindi (1995) presents a heuristic including variable redefinition and column generation.
O	Hindi (1996) combines the ideas of linear relaxation, column generation, minimum cost flow network and Tabu search in a hybrid algorithm.
P	Haase (2005) also solves the lot sizing problem by column generation and finds improved lower bounds.
P	Considering that both Lagrangian relaxation and Dantzig-Wolfe decomposition have advantages and disadvantages, Huisman et al. (2005) discuss two different ways to combine these two methods in hybrid algorithms to solve the linear relaxation of the master problem.
O	Pimentel et al. (2010) consider the lot sizing problem with setup time and apply the Dantzig-Wolfe decomposition to the classical formulation in two different ways: item decomposition and period decomposition.
O	de Araujo et al. (2014) present a transformed reformulation and valid inequalities that speed up column generation and Lagrangian relaxation for the capacitated lot sizing problem with setup times (CLST) and show theoretically how both ideas are related to dual space reduction techniques.
P	Finally, the authors propose a combination of the two methods proposed by Huisman et al. (2005).
O	The aim of this paper is to find good lower and upper bounds for the single stage problem with unrelated parallel machines extending the ideas proposed in Huisman et al. (2005), de Araujo et al. (2014) and Fiorotto and de Araujo (2014).
O	This formulation is based on the formulation of Trigeiro et al. (1989) for the single machine problem, and has been studied in Toledo and Armentano (2006).
P	Next we present a reformulation of the model (1)-(5) using the variable redefinition approach proposed by Eppen and Martin (1987), producing a formulation based on the shortest path problem.
P	Note that this model is an adaptation of the shortest path reformulation that was originally proposed by Eppen and Martin (1987) for the case without capacity constraints.
O	The ideas proposed by Jans and Degraeve (2004a), Huisman et al. (2005) and de Araujo et al. (2014) are used and were extended for the problem being addressed.
P	These subproblems can be solved by the branch-and-bound method proposed by Jans and Degraeve (2004a).
O	Indeed, this is always the case if the dualized constraints in the Lagrangian relaxation and the linking constraints of the Dantzig-Wolfe decomposition are the same (Huisman et al., 2005).
O	The Lagrangian multipliers pit are updated by the subgradient optimization method (Camerini et al., 1975).
O	Several papers from the literature have shown that the combination of these two techniques is a promising tool in the resolution of integer programming problems (Huisman et al., 2005).
P	To counter this behavior, strategies for stabilizing the dual solutions have been proposed in the literature, leading to more efficient variations of the column generation method (see for example, Ben Amor et al. (2007) and Gondzio et al. (2013)).
O	Cattrysse et al. (1993), Jans and Degraeve (2004a) and de Araujo et al. (2014) apply this technique to solving variants of the capacitated lot sizing problem with a single machine.
O	With this approximation for the vector πit it is possible to calculate an approximation for the vector μtj that represents the optimal multiplier for the convexity constraints (16) (Huisman et al., 2005):
O	The advantage of approximating the optimal dual variables by the Lagrangian relaxation, is that in the case of alternative dual solutions, column generation algorithms tend to converge more quickly using dual variables produced by interior point methods than with extreme point dual variables calculated by the simplex method (Bixby et al., 1992; Barnhart et al., 1998).
O	Computational experiments performed in Jans and Degraeve (2004a) indicate that the use of the Lagrangian multipliers indeed speeds up the convergence and reduces the problem of degeneration. 
O	Following the ideas proposed in de Araujo et al. (2014), this approach is based on the observation that when the Lagrangian relaxation is obtained by dualizing exactly those constraints that are the linking constraints in the Dantzig-Wolfe decomposition, the same subproblem results.
P	To obtain a feasible solution (upper bound) we extended the feasibility heuristic proposed by Fiorotto and de Araujo (2014) by adding an initialization and an improvement stage and by making several changes.
O	Note that feasibility heuristics based on production transfer have been applied to make the capacity constraints feasible (for instance, Trigeiro et al. (1989) and Toledo and Armentano (2006)).
O	In order to establish a ranking, we calculate an approximate average unit cost per item and per machine, assuming that we use the Economic Order Quantity (EOQ) as production quantity (Andriolo et al., 2014).
P	The algorithms described in the previous sections were tested on a total of 2160 instances proposed in Toledo and Armentano (2006).
O	To generate the normal capacity (NC), Toledo and Armentano (2006) use as a base a lot-for-lot policy and afterwards, an adjustment is made to reduce the capacity in order to generate instances which use about 80% of the capacity.
P	The tight capacity (TC) is obtained by multiplying this capacity by 0.9. Further details can be found in the paper Toledo and Armentano (2006).
O	For the method LR/CF proposed in Fiorotto and de Araujo (2014): they start the subgradient optimization method fixing the dual variables to zero; the size of the initial step is equal to 1 and decreases by multiplying by 0.6 if the Lagrangian solution is not improved in the last 50 iterations; 2500 iterations are made in total.
O	For the method T A proposed in Toledo and Armentano (2006): the size of the initial step is equal 1.75 and decreased by 2 if the Lagrangian solution is not improved in the last 25 iterations; 150 iterations are made in total.
O	We note that despite the higher computation times of the hybrid methods, mainly the method LR/EF/CF, they present better gaps than the Lagrangian heuristics T A and LR/CF proposed in Toledo and Armentano (2006) and Fiorotto and de Araujo (2014), respectively.
N	Note that for Table 2 and the following tables, we are not able to give the results for TA, since the results were not provided according to this classification in the paper of Toledo and Armentano (2006).
N	This was also observed by Trigeiro et al. (1989) for the single machine lot sizing problem with set up times.
N	The results show that we improved the heuristic proposed by Fiorotto and de Araujo (2014) considering that we found better upper bounds for all classes.
P	A reformulation of the problem using the variable redefinition approach proposed by Eppen and Martin (1987) was used.
O	According to recent survey research presented in De Snoo et al. [2] focused on manufacturing firms, the three most frequently cited performance criteria for a production schedule (and associated information produced by that process) are the (i) fulfillment of constraints made to external parties, (ii) fulfillment of resource utilization constraints, and (iii) schedule robustness and information completeness, in that order. 
O	As De Snoo et al. [2] acknowledge “schedulers indicate that responsiveness in [requests for rush orders] is highly important”.
P	Recently, rescheduling and the more general topic of scheduling under uncertainty has attracted the interest of many researchers, as reviewed by Mehta and Uzsoy [3], Davenport and Beck [4], Vieira et al. [5], Herroelen and Leus [6] (who include project scheduling as well), Aytug et al. [7], Ouelhadj and Petrovic [8] and Sabuncuoglu and Goren [9].
P	To the best of our knowledge, the first article to use this measure was Wu et al. [14], followed by O’Donovan et al. [15], Mehta and Uzsoy [11], and Hall and Potts [16], among others.
O	For example, Wu et al. [14] use start times for this purpose, whereas Watatani and Fujii [17] and Hall and Potts [16] use distance metrics between two sequences.
O	According to Aytug et al. [7], the purposes of a predictive schedule (preschedule) include the following: serving as a capacity check; providing visibility for the rest of the organization, external suppliers, and customers; evaluating the performance of the shop-floor personnel; and avoiding any further problems by serving as a feedforward control tool.
O	In terms of the main issue being addressed and the way we model stability, the previous work most related to our efforts is Wu et al. [14].
P	Hall and Potts [16] and Hall et al. [21] are also closely related to the problem we define here, though in terms of the way the issue is modeled, there are significant differences.
O	Hall and Potts [16] develop two classes of models: (i) the cost of disruption is included as a constraint in the model, and (ii) a total cost function is taken as the objective function, which includes the cost function (e.g., the maximum lateness, or the sum of the completion times) and the disruption cost. 
O	Hall et al. [21] model the rescheduling of a single machine due to a set of new orders as a problem of minimizing the maximum lateness of all jobs, subject to a limit on the maximum time change of the prescheduled jobs (jobs are all available at time zero; preschedule is not necessarily optimal and may include idle time).
P	1 rj Lmax has been proven to be NP-hard by Lenstra et al. [22].
P	Erschler et al. [24] developed dominance properties that do not depend on the processing times and that we use in this research as well.
O	Garey et al. [25] provide both an Oðn2Þ and an Oðn log nÞ algorithm for the timing problem (for both cases without rj; ~ dj and with these constraints).
O	Kanet and Sridharan [27] provide a comprehensive review of the key results up to the late 1990s for the problems 1 rj Lmax and 1 rj PEj þTj. \
O	To the best of our knowledge, the only algorithm designed for the 1jrj; ~ dj j PϵjEj þτjTj problem and extensively tested is that of Esteve et al. [29].
P	The BBGLS and CNSTR heuristics use the dominance theorem developed by Erschler et al. [24] for 1jrj j Tmax (and 1jrj j LmaxÞ (also discussed in detail in [30], as Theorem 1).
O	Erschler et al. [24] defined the top of an interval structure and a t-pyramid related to the top as follows:
O	Briand et al. [30] define two sequences that lay the foundation for calculating the bounds discussed here.
P	To overcome this limitation Li and Quan in [17] recently proposed an encoding of MCP into MaxSAT assuming the input graph to be colored (definition 1, proposition 2).
O	In this paper we propose a new approach for the one-to-one multi-commodity pickup and delivery traveling salesman problem (m-PDTSP) introduced by Hernández-Pérez and Salazar-González [23].
O	We first consider the TSP [24], or more precisely the asymmetric version since all the problems discussed here are defined in a directed graph G ¼ ðV; AÞ
P	Many formulations have been presented for this problem (see, for instance [30], as probably the latest such reference) and we also refer the reader to the well known formulation by Dantzig et al. [9] (DFJ) that will be stated in Section 3 as a subformulation for all the formulations presented and discussed in this paper.
P	We refer the reader to the papers by Balas et al. [6], Ascheuer et al. [5], and Gouveia and Pesneau [16]. 
P	A second variant of the TSP is the so-called many-to-many onecommodity pickup and delivery traveling salesman problem (1- PDTSP) and has been introduced by Hernández-Pérez and SalazarGonzález [20].
O	It is NP-hard to find a feasible solution for the 1-PDTSP as shown by Hernández-Pérez and Salazar-González [20].
O	The papers by Hernández-Pérez and SalazarGonzález [21,22] present several models and valid inequalities for the 1-PDTSP and branch-and-cut algorithms to solve it.
O	As pointed out by Hernández-Pérez and Salazar-González [23] the m-PDTSP generalizes the 1-PDTSP
O	Again, and as also pointed out in Hernández-Pérez and Salazar-González [23], if one ignores the vehicle capacity in the m-PDTSP, one obtains the SOP since the precedence between source and destination for each commodity must be maintained.
O	The m-PDTSP is NP-hard since it generalizes all the variants described here which are also known to be NP-hard. HernándezPérez and Salazar-González [23] present two solution approaches, both based on Benders decomposition of a path and a multicommodity flow model.
P	Rodríguez-Martín and Salazar-González [31] also propose several heuristic approaches for the m-PDTSP to obtain high-quality solutions for larger instances for which exact approaches cannot obtain satisfying results within reasonable time.
O	We conclude this literature review by pointing to the overview on further pickup and delivery problems given in Berbeglia et al. [7]
O	The precedence relations are ensured separately by adding valid inequalities from the SOP, see Balas et al. [6] and Ascheuer et al. [5].
O	Especially for tightly capacitated instances with a large number of commodities we are able to outperform the approaches by Hernández-Pérez and SalazarGonzález [23].
O	To calculate the load bounds for all other arcs ði; jÞAA; ia0; janþ1, we use some ideas from HernándezPérez and Salazar-González [23] and extend them in the following way.
P	These constraints are also used in models for the 1-PDTSP and m-PDTSP in previous papers (e.g., [21–23]).
O	Note that constraints (4), although exponential in number, can be easily implicitly included in the model by a cutting plane approach based on finding violated inequalities with max-flow computations (see, e.g., [28]).
O	The flow model by Hernández-Pérez and Salazar-González [23] is based on the generic scheme mentioned before.
P	Also and as mentioned by Hernández-Pérez and SalazarGonzález [23] the LP relaxation of the model can be improved by replacing constraints (8) and (9) with the following well known modeling strengthening constraints of multi-commodity flow models extended by information obtained in preprocessing:
N	Weaker related relations have been pointed out by Hernández-Pérez and SalazarGonzález [23] stating that the two problems: (i) the 1-PDTSP using net demands ρ (without considering any precedence relations) and (ii) the TSP with precedence constraints defined by the commodities (with unlimited vehicle capacity) are relaxations of the m-PDTSP.
O	According to Balas et al. [6] we consider cut-like inequalities known from the SOP, i.e., the simple ðπ; σÞ-inequalities which are described as follows: for each commodity kAK we define a set of relevant nodes Vk ¼ V=Vout k and the corresponding inequalities are efined as follows:
P	For details we refer to Balas et al. [6].
P	Balas et al. [6] have also proposed a different set of inequalities, the so-called precedence cycle breaking constraints (PCB).
O	Note also that sequences including transitive precedence relations are dominated by the ones consisting only of non-transitive relations, as shown by Balas et al. [6] for the PCB inequalities.
O	We adopt the notation from Balas et al. [6] and write πðjÞ≔fi : ði; jÞARg for the set of predecessors for node jAV. 
O	In this subsection we show that the model by Picard and Queyranne [29] (PQ) can be easily readapted to model the capacity constraints of the aggregated SCF system (17) and (18).
O	Similar to what has been done in Gouveia et al. [17] and Godinho et al. [13] to redefine cut inequalities in the layered graph, we can also redefine the SOP cuts (21) in the load-based layered graph GL to improve the LP relaxation of model LCUTR.
P	However, the strong models in Godinho et al. [12,13] explicitly use the fact that the associated layered graphs are acyclic (as in the original PQ system) since the layers correspond to the positions of the nodes in the solution.
P	Together with the exact separation algorithms described in the previous sections, we apply in each cutting plane iteration the heuristic by Hernández-Pérez and Salazar-González [23] to identify further violated inequalities: essentially, we perform a restricted enumeration of node sets S and check for violated π-, σ-, and ðπ; σÞ-inequalities, and capacity cuts (30).
P	To further improve a created solution, we run a generalized variable neighborhood search (GVNS) [19].
O	We used three different classes of instances introduced by Hernández-Pérez and Salazar-González [23]:
O	Similarly, the Benders decomposition approach (BE) based on the MCF model by Hernández-Pérez and Salazar-González [23] (HS) also contains heuristic elements.
P	We adopted all results of BE from Hernández-Pérez and Salazar-González [23].
O	Tables 4 and 5 show the results of our branch-and-cut algorithms in comparison to the Benders decomposition approach (BE) by Hernández-Pérez and Salazar-González [23].
O	A position-dependent model as shown e.g., in Godinho et al. [12,13] may be appropriate to model the precedence relations.
O	In Table 7 we compare two variants of our branch-and-cut algorithms to the state-of-the-art results for the SOPLIB instances by Montemanni et al. [27] which consist of 200–700 nodes.
P	In this paper we study the mobile facility location problem (MFLP) which was originally proposed by Demaine et al. [1].
O	Using this observation, we present a novel integer programming (IP) formulation for the MFLP with significantly fewer nonzero constraint coefficients and integer variables than an earlier (and perhaps more natural) formulation in the literature by Friggstad and Salavatipour [2].
P	The MFLP was introduced by Demaine et al. [1] as one of a class of movement problems
O	Friggstad and Salavatipour [2] presented an 8-approximation algorithm for the MFLP when travel times satisfy the triangle inequality and the facility weight wj equals one for every facility j ∈ F.
O	Friggstad and Salavatipour [2] seem to suggest that local search is not a good heuristic for the MFLP. 
O	Gendreau et al. [7] described a system for the real-time relocation of ambulances to maintain coverage after an ambulance responds to a call.
P	Kolesar and Walker [8] considered the redeployment of fire companies in New York City while some companies are responding to a call.
O	A bicriteria model for the repositioning of response units in a probabilistic network after a change in the state of the network was presented by Sathe and Miller-Hooks [9].
O	Nair and Miller-Hooks [10] applied this model in determining relocation policies for ambulances. 
O	The location and relocation of servers in a stochastic network to minimize the expected travel time of clients to servers and the expected relocation costs was studied by Berman and LeBlanc [11] where they presented a heuristic that uses the Hungarian algorithm to minimize relocation costs.
P	This model was generalized by Berman and Rahnama [12], enabling the transition of network states to be a Markovian process.
O	Friggstad and Salavatipour [2] described a collection of local search operations defining a solution neighborhood and demonstrated that a local search heuristic allowing only these operations (which we now describe) can produce an arbitrarily large locality gap.
O	Friggstad and Salavatipour [2] demonstrate how to construct an MFLP instance where any local search heuristic allowing only the above two operations may exhibit an arbitrarily large locality gap.
O	Floyd’s algorithm [21] was used to determine the shortest path between all pairs of vertices of the network.
O	Avella et al. [23] also previously used the TSP-Lib data sets for studying large scale p-median problems
O	Friggstad and Salavatipour [2] produced an example where a local search heuristic that uses two operations to define a local search neighborhood can produce a solution with an arbitrarily large optimality gap.
O	Recently (and subsequent to our work) Ahmadian et al. [6] showed that n-OptSwap is a 3- approximation algorithm when n is large. When n = 1, they show the approximation ratio of 1-OptSwap is 499.
O	These computational results confirm the effectiveness of our local search heuristics and show that the approximation ratio experimentally is much tighter than the worst case approximation ratio obtained in the theoretical analysis of Ahmadian et al. [6].
P	QMSTP was proven to be NP-Hard by Assad and Xu [1].
O	A common procedure for computing lower bounds for constrained quadratic 0-1 problems is that of Gilmore [10] and Lawler [11].
P	The QMSTP was first addressed by Assad and Xu [1].
O	Assad and Xu [1] implemented a branch-and-bound (BB) algorithm based on the Lagrangian lower bounds and managed to solve instances defined over complete graphs with up to 12 vertices.
P	Öncan and Punnen [6] also introduced a procedure based on Lagrangian relaxation
P	A modified version of the exact algorithm in [1] was introduced by Cordone and Passeri [9].
P	Assad and Xu [1] also addressed the AQMSTP particular case.
O	A bi-objective version of AQMSTP was investigated by Maia et al. [14].
P	Pereira et al. [15] proposed a stronger reformulation for AQMSTP that is naturally linear.
O	Pereira et al. [15] also investigated the formulation obtained after projecting the extended variable space on the x space.
O	Many facet defining inequalities for the BQFP were studied by Lee and Leung [17].
O	One of the known algorithmic alternatives to deal with exponentially many inequalities candidates to Lagrangian dualization is the relax-and-cut approach [28].
P	We denote by CP the first set, introduced by Cordone and Passeri [9].
P	The second set, denoted by OP, was introduced by Öncan and Punnen [6].
O	We report lower bounds for the Lagrangian relaxation schemes of Assad and Xu [1], which we denote LAGAX92, Öncan and Punnen [6], which we denote LAGOP10, LAGRLT , and LAGSEC.
O	We found differences between the bounds we evaluated and those reported by Öncan and Punnen [6].
P	Assad and Xu in [1] proposed a lower bounding procedure and two heuristic approaches.
P	Öncan and Punnen [3] introduced a Lagrangian relaxation procedure to obtain an improved lower bound and an efficient local search algorithm.
P	Cordone and Passeri [4] have developed two heuristics and an exact approach. 
P	Lee and Leung [5] studied the Boolean quadric forest polytope and proposed several facet defining inequalities.
P	Buchheim and Klein [6] proposed complete polyhedral descriptions of the QMSTP with one quadratic term and provide an improved version of the standard linearization by means of cutting planes.
P	Pereira et al. [7] proposed some new formulations using a particular partitioning of the spanning trees, and provided a new mixed binary formulation for the problem by applying the first level of the reformulation-linearization technique (RLT).
P	Assad and Xu in [1] iteratively applies an adaptation of the Gilmore–Lawler procedure, originally proposed for the Quadratic Assignment Problem [11,12], to a sequence of equivalent QMSTPs.
P	Öncan and Punnen in [3] introduced an extended formulation based on the addition of two sets of valid inequalities to the linearized formulation of [1].
P	Pereira et al. [7] proposed a new mixed binary formulation for the problem and developed a Lagrangian relaxation approach to obtain a linear programming based lower bound.
P	The GL procedure was proposed by Gilmore [11] and Lawler [12] in the context of QAP and has been adapted to many other quadratic 0–1 problems [13,14].
P	In order to improve the GL bound for the QMSTP, Assad and Xu in [1] proposed a method that generates a monotonic sequence of lower bounds.
O	Assad and Xu [1] show that the relaxation of (19) in QAX formulation using Lagrangian multipliers θf yields a Lagrangian dual whose optimal value is precisely LBAX.
P	The level-1 RLT representation of the QMSTP proposed by Pereira et al. [7] applies two sets of operations.
P	In our implementation, each spanning tree problem is solved with Prim's algorithm [24].
P	We considered the benchmark sets CP and OP introduced in [4] and [3], respectively.
O	In all tables the first three columns indicate the problem size (n), the density (d), and the objective value of the best known solutions (Ub) obtained from [4].
N	Note that, the revised form of the Öncan and Punnen Lagrangian relaxation approach does not always yield tighter lower bound values for all the data sets than the ones obtained by the Assad and Xu's leveling procedure, as claimed in their paper [3].
O	Using the typology of cutting and packing problems proposed by Waescher, Haußner, and Schumann [31], this problem belongs to the “open dimension” class, characterized as
P	We will address the following specific versions of the problem, as classified by Caprara, Lodi, Martello and Monaci [8]:
P	Problem PSS was proved to be strongly NP-hard by Leung, Tam, Wong, Young and Chin [23], hence the same holds for PRSO and PRSR. Caprara, Lodi, Martello and Monaci [8] introduced some simple lower bounds for the three problem versions, and analyzed their worst-case performance.
O	Bansal, Correa, Kenyon and Sviridenko [3] gave a polynomial time approximation scheme (PTAS) for problem PRSO.
P	The result was extended to PRSR by Correa [12].
N	Picouleau [29] proposed simple heuristics for PSS, and determined the worst-case performance of some of them.
O	Different, but similar, open dimension problems have been addressed by Korf [20, 21], Huang and Korf [17] and Korf, Moffitt, and Pollack [22], who considered the problem of packing a given set of rectangles into a rectangle of minimum area.
P	More different open dimension problems have been also studied, concerning, e.g., the packing of cylinders (Birgin, Martinez, and Ronconi [7]), of polygons (Stoyan and Patsuk [30]) or of irregular shapes (Costa, Gomes and Oliveira [13]).
P	An extensive literature is available for these topics, for which we refer the interested reader to the survey by Lodi, Martello, Monaci and Vigo [24].
P	A different modeling technique, based on the enumeration of all possible relative placements of each pair of items, was proposed by Onodera, Taniguchi, and Tamaru [28] for a special twodimensional block placement problem. 
P	Their method can lead to models of polynomial size, but with known low efficiency in practice (see, e.g., Chen, Lee, and Shen [10]), so the research in this area mostly concentrated on the former approach (see, e.g., Hadjiconstantinou and Christofides [16]).
O	In order to reduce the number of binary variables, we can make use of a consideration made by Christofides and Whitlock [11] for the two-dimensional knapsack problem.
P	A classical heuristic approach for two-dimensional packing problems is bottom-left, introduced by Baker, Coffman and Rivest [2].
O	These can be implemented exactly as in Chazelle [9], which preserves the O(n2) overall time complexity of the algorithm.
O	(The worst-case performance ratio of LG has been analyzed in Caprara, Lodi, Martello and Monaci [8])
P	Two main variants of the problem are usually considered in the literature (see Lodi, Martello, Monaci, and Vigo [24]): following the three-field notation proposed by Lodi, Martello, and Vigo [25], 2BP|O|F denotes the case in which the items are oriented (as in PRSO and PSS), while 2BP|R|F denotes the case in which the items may be rotated by 90◦ (as in PRSR).
O	These item sets have also been used by Korf [20, 21] for testing his algorithm for packing a set of rectangles into a rectangle of minimum area.
P	In 2004 Korf [21] answered the question in the negative, running an adaptation of his algorithm for 5 days (and generating about 20 billions decision nodes).
O	More recently, Korf, Moffitt, and Pollack [22] determined the minimum square for packing the items up to n = 27.
P	We included in our benchmark the instances proposed by Korf, Moffitt, and Pollack [22] for the problem of packing rectangular items into a rectangle of minimum area.
P	A permutation-based metaheuristic for a two-dimensional packing problem arising in VLSI placement was proposed by Murata, Fujiyoshi, Nakatake, and Kajitani [27], through a totally different kind of permutations.
P	They represented indeed a packing through a pair of permutations of n rectangles each, determining their horizontal and vertical partial orders, and searched the solution space through simulated annealing. Imahori, Yagiura, and Ibaraki [19] gave a dynamic programming algorithm to obtain the best solution corresponding to a given pair of permutations.
N	However, for the three GARD instances (with n equal to 18, 24, and 26) for which L was smaller than the value reported by Korf, Moffitt, and Pollack [22], L was set to the latter value.
O	The problem considered is NP-hard [10] and was first studied by Huygens et al. [9] who only consider Lr4 and K¼2. 
P	Recently, an extended formulation combined with a Benders decomposition approach to efficiently handle the large number of variables and constraints of the formulation has been proposed and tested in Botton et al. [1].
P	Zotkiewicz et al. [14] present a polynomial time algorithm to solve the particular case of this variant where K¼2.
O	A weak cut (12) can then be generated if no integral feasible x can be found.
O	Cardoen et al. [21] and Guerriero and Guido [22] provide the most recent surveys on the contribution of operational research to operating room planning and scheduling.
O	For instance, Fei et al. [23-24] developed a set-partitioning integer-programming model to define the date for surgery and used a column-generation-based heuristic.
P	Marques et al. [20] and Roland et al. [29] provide integer and mixed-integer programming models and solve the optimization problem using genetic algorithms.
O	Min and Yih [27] propose a stochastic dynamic programming model to study how the consideration of patient priority affects the surgery scheduling policy. 
O	The problem is usually divided into three interrelated sub-problems [1]: horizontal alignment optimization, vertical alignment optimization, and earthwork optimization.
O	This creates a complicated, but deterministic optimization problem that is generally solvable using modern MILP solvers (assuming reasonable road lengths and time allowances) [15].
P	Hare et al. [16] and de Lima et al. [10] developed two mixed integer linear programming models for earthwork operation in road construction.
P	Unlike Burdett and Kozan [6] who developed a model for the earthwork allocation problem considering earthwork as discrete 3D blocks, in the present paper we use a section-based model, noting that section-based models achieve similar precision as 3D block based model when section lengths are less than 30 m [9].
P	In 2009 Moreb [29] developed a linear programming model combining the vertical alignment and earthwork allocation optimization
P	In 2010, Koch and Lucet [26] advanced Moreb's model by removing unnecessary errors in slope constraints. 
P	More recently, Hare et al. [14] incorporated the vertical alignment in the earthwork allocation model, resulting in a mixed integer linear programming model that can be solved efficiently in practice.
P	Jong et al. [20,21] developed a horizontal alignment optimization model which was solved by a genetic algorithm.
O	In 2008, Easa and Mehmood [11] developed an optimization model incorporating safety constraints, which were quantified as the expected collisions for an alignment.
P	In 2009, Lee et al. [28] presented a heuristic based method to optimize the horizontal alignment that works in two stages
P	Tat and Tao [33] proposed a three-dimensional alignment optimization model, which they solved using a genetic algorithm.
P	Akay [2] developed a model for three dimensional alignment optimization for forest roads and solved it using a simulated annealing algorithm
O	Aruga [3] used a tabu search method to optimize three dimensional alignments of forest roads.
P	Jong and Schonfeld [22] presented an evolutionary model for optimizing the vertical and horizontal alignment simultaneously.
P	The previous two models [17,22] were improved in [18] by considering accessibility, proximity, and land-use changes, and further improved in [25] to consider incorporating bridge and tunnel costs.
P	Cheng and Lee [8] also proposed a heuristic-based model for three dimensional alignment optimization.
O	Most similar to this work, Kang et al. [24] developed a bi-level optimization model for road alignment design.
P	Recently, Kang et al. [23] also proposed a three dimensional alignment optimization model based on genetic algorithm and geographic information system (GIS).
O	Thus Laporte et al. [12] have compared a number of measures on various generic metro configurations (see Figure 1): 1) network complexity (number of edges divided by number of nodes); 2) connectivity (ratio of the number of edges compared to the maximal number of edges that could exist in a planar graph for a given number of nodes); 3) directness (proportion of origin-destination paths that can be traveled without transfers); 4) passenger/ network effectiveness (ratio of travel time with transfers to travel time disregarding transfers).
P	In this paper we work with the modular approach first suggested by Bruno and Laporte [2].
O	(In Bruno and Laporte [2] an interactive system allows planners to make a selection among a menu of predefined configurations and to define their own.)
P	Tizhoosh maps this theory to machine learning and proposes to use opposite numbers instead of random mutations to quickly evolve the EA population [1].
P	OBL was first proposed in 2005 [2] and was applied to a popular reinforcement learning algorithm, Q-learning.
O	The research was extended to empirical analysis on an extensive collection of benchmark functions and the experimental results illustrated that opposition-based DE outperforms fuzzyadaptive DE and standard DE [5].
P	Different opposition-based PSO algorithms have been introduced [11] with velocity clamping [12] or with Cauchy mutation [13].
O	More recent opposition-based EAs include appending artificial bee colony algorithm [14] with opposition to form generalized oppositionbased ABC [15]; CODEQ, a parameter-free algorithm that combines chaotic search, opposition-based learning, differential evolution and quantum mechanics for optimizing constrained problems [16]; and opposition-based gravitational search algorithm [17].
P	The central opposition theorem proves that the opposite point has a higher probability of being closer than a random guess to the solution [22].
P	An intuitive analysis can be used to show that the distance to the optimal solution is less with opposite sampling than random sampling [23].
O	The effects of population size, problem dimensionality and opposition jumping rate (Jr) on opposition-based differential evolution are studied [5].
O	Empirical studies on 30 benchmark functions indicate that quasi-oppositional optimization outperforms opposition [24].
O	Mathematical properties of quasiopposition are given in [25].
P	Fitness-based opposition (FBO) and quasi-based reflection are introduced as a single algorithm in [7].\
P	Empirical analysis of FBO with center-based sampling against four other opposition-based algorithms is performed in [28].
O	An opposition-based PSO algorithm was previously published [41].
O	BBO has many state-of-art variations [44–47]. In this section, we incorporate opposition to a variant called linearized BBO (LBBO) [48].
N	LBBO did not compete in CEC 2013; however, it is included in this study as a newer algorithm and due to its promising performance on the CEC 2005 and CEC 2011 test suites [48].
O	Our simulations focus on four algorithms. The first EA is particle swarm optimization (PSO) [40].
O	x^ Kr is based on our earlier empirical results in [7] and is characterized by assigning a reflection weight proportional to a solution candidate's relative fitness.
O	We assumed that the solution and the estimate have uniform distributions as in [32] and that the problem domain is symmetric such that b ¼ a to simplify the resulting mathematical expressions.
O	This paper focuses on the computation of a good representation of the efficient set, the so-called representation problem, where the quality of the representation is measured with respect to some property of interest [1,2].
O	Two widely accepted ways of measuring the quality of a representation [3–6] were introduced by Sayin [2], and are explored in this paper: (i) uniformity, the representation points are as spread as possible and (ii) coverage, the representation points are close to the remaining non-dominated points.
O	Note that this problem corresponds to a particular case of the k-dispersion problem in facility-location; see, for example, Ravi et al. [10].
O	A second property proposed by Sayin [2] is coverage, which measures the quality of the representative subset by considering the distance of the unchosen elements to their closest elements in the subset.
O	This problem is known in the literature of facility-location as the k-center problem; see Kariv and Hakimi [11] for an early reference as well as Hassin and Tamir [12] and Schöbel [13] for a problem closely related to the coverage representation problem considered here.
O	Also note that the factor ϵðr; bÞ defined above corresponds to the approximation ratio ð1þϵÞ typically used in the context of approximation schemes; see, for example, Papadimitriou and Yannakakis [14].
O	This leads to the time complexity of Oðk nÞ described by Wang and Kuo [15].
P	In this section we describe two algorithms to solve the uniformity representation problem: An adaptation of the dynamic programming algorithm proposed by Wang and Kuo [15] and a threshold algorithm.
P	The dynamic programming algorithm for problem (UR) is based on the approach proposed by Wang and Kuo [15], which we briefly describe as follows.
O	As reported by Wang and Kuo [15], it is possible to further improve this algorithm by applying a modification which removes the need to check the optimal value for every subproblem, obtaining a time complexity of Oðnkþnlog nÞ
O	This leads to the time complexity of Oðk nÞ described by Wang and Kuo [15].
P	Our approach to the uniformity representation problem is based on the threshold algorithm described in Ponte et al. [16] for a related problem with the ϵ-indicator
P	By following the structure and improvements proposed by Vaz et al. [17] we have that TPðnÞ ¼ Oðn2log nÞ and TSðnÞ ¼ Oðlog nÞ.
P	Wang et al. [15] suggest a speed-up technique that can be adapted for this algorithm.
O	The algorithm presented in Ponte et al. [16] can be applied to the coverage representation problem, since all the required properties are met.
P	It uses the algorithm proposed by Schöbel [18] to solve set cover problems with C1P.
O	Moreover, the improvements presented by Vaz et al. [17] may also be applied, yielding a time complexity of Oðn2log nÞ.
P	This reformulation of the feasibility problem for coverage representation is closely related to the approach described in Ponte et al. [16] for the ϵ-indicator, although it retains some of the characteristics of the threshold algorithm described in Section 3.2.
O	The set covering procedure presented in Section 4.2.2 may also be adapted for the ϵ-indicator problem; see the discussion on related approaches in a different context in Ponte et al. [16] and Vaz et al. [17].
O	This result, as well as the analogous case for rows, is demonstrated by Vaz et al. [17].
O	However, we remark that a better time complexity may be achieved by the approach of Bringmann et al. [19].
O	This time complexity can be realized by using the technique described in Kung et al. [20] with an AVL binary balanced tree to keep the non-dominated vectors.
P	As in Section 6.1.1, we also use the technique of Kung et al. [20] and an AVL tree to store the solutions found for each subproblem.
O	Then we could proceed according to the operations described in Schöbel [18].
P	Since this algorithm still finds the solution in linear time, the complexity of this procedure is still Oðnlog nÞ, considering the overhead caused by the generation of the compressed covering matrix (see Vaz et al. [17]).
O	The technique of Kung et al. [20] for the three-dimensional case, an extension of the technique described in Section 6.1.1, can be used to maintain a set of non-dominated vectors in linearithmic time complexity.
P	A possible solution to this problem is to use an AVL tree as described by Kung et al. [20], which results in a similar approach to that used for the dynamic programming algorithm.
O	Tighter time-complexity bounds for dynamic programming should be obtainable, for instance, by using an average-case or smooth analysis argument on the number of states that are kept at each iteration; see, for example, Beier and Vöcking [21].
O	For example, the Global e-Sustainability Initiative estimated the overall network energy requirement for European telecommunication is around 35.8 TWh in 2020 [13].
O	To this extend, the backbone networks and more precisely IP routers, consume a majority of energy [5].
O	While the traffic load has only a marginal influence, the most contribution of energy consumption on router is the number of active elements such as ports, line cards, base chassis [32].
P	From energy savings perspective, RE has a drawback since it increases energy consumption of routers [23].
P	To find a good trade-off, in our previous work, we proposed GreenRE - a model that combines EAR and RE to increase energy efficiency for backbone network [23].
O	Previous studies have considered robustness either on traffic volumes [30] or on redundancy rates [18].
O	However, this problem is known to be NP-Hard [22], and currently exact solutions can only be found for small networks.
O	Spring et al. [38] developed the first system to remove redundant bytes from any traffic flows.
O	However, Anand et al. [4] have shown that on a desktop equipped with a 2.4 GHz CPU and 1 GB RAM, the prototype can work at 2.2 Gbps and 10 Gbps respectively for encoding and decoding packets.
P	In next sub-section, we recall the GreenRE model - the first model of energy-aware routing with RE support [23].
O	As shown in literature [16,30], the traffic for a commodity can be split among multiple paths between a source and a destination.
O	Let a RE-router consume 30 Watts [23] and a link consume 200 Watts [16].
O	Each active link fu; vg and router u is respectively associated with a power consumption value PEfu;vg ¼ 200 Watts [16] and PNu¼30 Watts [23].
P	Over the past years, robust optimization has been established as a special branch of mathematical optimization allowing to handle uncertain data [6,7].
O	A specialization of robust optimization, which is particularly attractive by its computational tractability, is the socalled Γ-robustness concept introduced by Bertsimas and Sim [8,9].4969: 
O	Fig. 6 shows real traffic traces of the three source-destination pairs: (a) Washington D.C. - Los Angeles, (b) Seattle - Indianapolis, and (c) Seattle - Chicago in the US Abilene Internet2 network in intervals of 5 mins during the first 10 days of July 2004 [30].
P	This confirms an assumption that the number of simultaneous demand peaks is bounded [30].
O	As there is a duality gap between the primal and dual models due to the integrality of the primal model, we cannot simply get the optimal solution using the compact formulation based on Bertsimas and Sim methodology like in existing works [30,18].
O	Energy-aware routing problem is known to be NP-Hard [22].
O	Since the power consumption of a link (200 Watts [16]) is much more than an enabled RE-router (30 Watts [23]), the heuristic gives priority to the minimization of the number of active links.
O	We solved the Robust-GreenRE model with IBM ILOG CPLEX 12.4 solver [29].
N	However, this result does not invalidate the benefit of Γ-robustness because in real-life traffic, only a few demands will vary their traffic simultaneously [30].
P	Yet the problem was only recently introduced, first appearing in [5], where meta-heuristic approaches were explored. 
O	Strong NP-hardness of the problem is established in [5] using a reduction to a network with only a single transshipment node (node other than the source or sink).
O	For example, Tawarmalani and Li [22], motivated by a problem in highway maintenance, consider a multicommodity flow variant, providing complexity results, combinatorial algorithms, and integer programming models.
O	The only other work combining network flow and scheduling that we are aware of is that of Nurre et al. [16], who schedule arc restoration tasks in the wake of a major disruption so as to maximize weighted flow over time.
O	In the problem of [16], the arc is closed from the start of the planning horizon until its restoration task is completed, from which time it is always open.
P	The latter is an important feature of real applications: in the export supply chain studied in [4], the port terminal has stockyards for holding stockpiled material, which can provide outbound flow from the system even while maintenance shuts down inbound flow.
O	In addressing these questions, this paper can be viewed as contributing to a rapidly increasing body of work exploring exact models based on coarser time discretizations, for example big bucket models in lot-sizing [18] and discretizations based on job release dates and deadlines in machine scheduling [2], as well as approximate integer programs, for example for traveling salesman problems with time windows [10,23] and for solving continuoustime dynamic network flows [14].
P	In order to tackle a real problem in which maintenance jobs must be timed to within 15-min intervals over a planning horizon of a year, the integer programming model presented in [4] is formulated in terms of a sparse set of possible start times selected heuristically for each job.
P	So by the fundamental theorem for Mixed Integer Linear Programs [15] the convex hull of the set of feasible solutions for CTIP is a rational polyhedron, with all its extreme points rational.
O	For our computational study, we use a subset of the randomly generated test instances from [5].
O	For each network, we consider the 10 instances where all jobs have a time window in the range ½25; 35 (the second, harder, instance set in [5]).
O	We also investigate the performance of the formulations and heuristics on the two instances derived from a problem arising in the scheduling of maintenance for a coal supply chain, the Hunter Valley Coal Chain (HVCC), studied in [5].
P	MIP-based heuristics that consider only a sparse subset of possible maintenance job start times are used in [4] to obtain practical schedules, tested on 2010 and 2011 HVCC annual maintenance schedules
P	Seeking to combat the challenge posed by the very large number of possible job start times, matheuristics are developed and tested on two instances of the problem we consider here, derived from the same 2010 and 2011 HVCC schedules [5].
O	However these matheuristics exploit the decomposable structure of the network flow problem that occurs if storage is ignored: the methods in [5] cannot be applied to the problem with storage and the instances tested in [5] disallow storage at the stockpad nodes.
P	Very recently, an estimation of distribution algorithm based hybrid algorithm named HEDA has been proposed by Wang et al. [11] to cope with large-scale problems by using a new repair operator.
P	More recently, a scalable adaptive strategy was developed by Wang at al. [31] in an improved adaptive binary harmony search (ABHS) algorithm.
P	To circumvent the above weakness, a new pitch adjustment operation was presented by Wang at al. [30] and a novel discrete binary HS algorithm was developed to solve the discrete problems effectively.
P	Based on this observation, a novel global harmony search algorithm (NGHS) derived from the swarm intelligence of particle swarm was developed by Zou et al. [33] for solving the 0-1 knapsack problems.
P	For example, the new pitch adjustment with neighboring values introduced by Lee et al. [35] helps HS to optimize the structures with discrete-sized members.
P	The scheduling problem we tackle in this paper is motivated by large-scale integrated circuit manufacturing as explained by Lee et al. [1].
O	Kashan et al. [4] focus on minimizing the makespan through a genetic algorithm that outperforms the simulated annealing approach suggested by Melouk et al. [5].
O	Angel and Bampis [20] consider a time-dependent version of the well-known single-machine total weighted tardiness scheduling problem extending the work by Congram et al. [21], they develop a multi-start local search algorithm showing the superiority of dynasearch neighborhoods over traditional ones
P	A recent survey on multi-start methods is provided by Martí et al. [28].
P	The lower bound is that of the SPT-EDD-dynamic batch schedule as proposed in Possani [24].
P	Eglese et al. [24] examine the issues involved in constructing a database of timedependent traversal times for a road network and assess the benefits of time-dependent vehicle routing and scheduling systems in a distribution application in the Northwest of England.
P	The reader interested in routing in public transportation networks is referred to the recent papers by Delling et al. [20] and Dibbelt et al. [22].
O	As stated by Malandraki and Daskin [60], potential causes of variability in travel times have two main components. 
O	To the best of our knowledge, the first reference to a timedependent travel time model is due to Beasley [10].
O	Later, Hill and Benton [47] proposed a modeling approach where each node was provided with a time-dependent piecewise constant speed function.
O	Malandraki and Daskin [60] discussed the adaption of the best successor heuristic for the vehicle routing problem with time windows and piecewise constant travel times.
O	In particular,Fleischmann et al. [28] presented a general framework for the implementation of time-varying travel times in various vehicle-routing algorithms
N	Recently, Ghiani and Guerriero [37] have shown that the travel time model proposed by [49] does not suffer from the drawback pointed out by Fleischmann et al. [28].
O	Indeed, Cordeau et al. [16], Arigliano et al. [3] and Calogiuri et al. [13] have shown that there is an interesting relationship between the time dependent versions of TSP, TSP with time windows and quickest path problem and their static counterparts.
P	A widely adopted FIFO modeling approach based on timevarying travel speeds was proposed by Ichoua et al. [49] (IGP model, in the following).
O	Fleischmann et al. [28] pointed out that the IGP model relies on constant edge distances, which is an hypothesis suitable for road networks, but not for the VRP where links between customers represent fastest paths that might change due to traffic congestion.
O	Later, Ghiani and Guerriero [37] proved that this issue is not a drawback since one can always express the VRP's travel time functions with respect to dummy reference distances.
O	Fleischmann et al. [28] also demonstrated that a continuous piecewise linear travel time function satisfies the FIFO property if its slopes are strictly greater than 1.
O	Recently Jabali et al. [50] proposed a time-varying model where the vehicle speed can be prescribed in a range whose upper bound is the IGP speed (aka congestion speed).
P	Kok et al. [53] proposed a speed model on real road networks that reflects the key elements of peak hour traffic congestion. 
P	Finally, Lecluyse et al. [56] presented a method to generate time-dependent travel times that are both spatially and temporally correlated.\
O	Gao and Huang [35] considered the travel time τijðtÞ on arc (i, j) at time t as a time-discrete random variable with a finite number of positive and integral support points. 
O	Thomas and White III [87] assumed that the vehicle driver knows instantaneously if the status of one of some “observed” arcs has changed and model the transition of observed arcs from congested to non-congested as discrete-time Markov chains.
O	Azaron and Kianfar [4] associated an environmental variable, indicating the weather conditions, with each node.
O	Nannicini et al. [67] proposed a bidirectional search. Since the arrival time at the destination is unknown, the algorithm starts a backward search from the destination node using lower bounds on arc costs in order to restrict the set of nodes that have to be explored by the forward search. 
O	Barrett et al. [8] considered shortest paths on time-dependent multi-modal transportation networks, where restrictions or preferences on the use of certain modes are modeled by means of regular languages.
O	Recently, Wen et al. [93] proposed two heuristic methods to solve the minimum cost path problem between a pair of nodes with a time-varying road network and a congestion charge.
O	Franceschetti et al. [32] considered a problem in which peak periods reduce the vehicle speed and increase fuel consumption.
O	To each customer node corresponds a service time and a hard time window in which service must start. 
O	Bast et al. [9] modelled these services as arcs of suitably defined time-expanded networks. 
O	Dell'Amico et al. [18] considered a shortest path problem on a non-FIFO network in which waiting at nodes is allowed.
P	Recently, Nielsen et al. [68] devised a solution algorithm for the a priori shortest path problem
P	In the context of production scheduling, we finally mention the work of Stecco et al. [81] who modelled a sequence-dependent and time-dependent scheduling problem (STDSP) as a TDTSP which they solved by a branch-and-cut algorithm.
P	Li et al. [58] developed and compared two heuristics for the TDTSP.
O	Harwood et al. [43] investigated the use of metaheuristics for the TDTSP.
P	Recently, Cordeau et al. [16] developed a branch-and-cut algorithm able to solve instances with up to 40 vertices.
O	Albiach et al. [2] studied the TDTSP with time windows.
P	Arigliano et al. [3] have extended the result presented by Cordeau et al. [16] to the TDTSP with time windows.
P	This problem, also referred to as a time-dependent TSP, has been addressed, among others, by Picard and Queyranne [73], Fox et al. [30], Gouveia and Voß [40], vander Wiel and Sahinidis [91], Bigras et al. [11], and Miranda-Bront et al. [64].
P	Helvig et al. [46] developed exact and approximate algorithms for a time-dependent generalization of the TSP, which they called the Moving-Target TSP, in which a pursuer must intercept in minimum time a set of targets that move with constant velocities.
P	A genetic algorithm for this problem was also proposed by Jiang et al. [51].
O	Montemanni et al. [65] addressed the robust TSP with interval data in which travel times are specified as ranges of possible values.
O	Verbeeck et al. [92] combined an ant colony system with a local search for the Time-Dependent Orienteering Problem. 
P	Ichoua et al. [49] developed a parallel tabu search heuristic.
P	Then, Donati et al. [23] developed a multi ant colony system for the TDVRP.
O	The TDVRP with Time Windows (TDVRPTW) was studied by Jung and Haghani [52], Osvald and Stirn [70], Hashimoto et al. [45], Soler et al. [80], Balseiro et al. [5], Dabia et al. [17] and Harwood [44].
P	Soler et al. [80] introduced a graph transformation as an asymmetric capacitated VRP.
O	Balseiro et al. [5] presented an Ant Colony System (ACS) algorithm hybridized with insertion heuristics for the TDVRPTW.
O	Tas et al. [86] studied a vehicle routing problem with timedependent and stochastic travel times.
P	Dabia et al. [17] presented an arc-based formulation and a branch-and-price algorithm
O	Finally, Zhang et al. [95] studied the TDVRP with simultaneous pickup and delivery.
O	It is worth noting that both Zhang et al. [95] and Figliozzi [26] proposed instances with Δ ¼ 1.
P	Ichoua et al. [49] developed a parallel tabu search heuristic and showed that the time-dependent model provides substantial improvements over a model based on fixed travel times.
P	Fleischmann et al. [27] developed constructive dispatching procedures for a real-time problem with pickups and deliveries with time windows. 
O	Haghani and Jung [41] considered a variant of the previous problem in which vehicles may have different capacities. 
P	Potvin et al. [75] developed reactive dispatching strategies for a dynamic vehicle routing and scheduling problem with time windows.
P	Chen et al. [14] presented a series of mixed integer programming models that account for real-time and time-dependent travel times and real-time demands in a unified framework. 
P	Maden et al. [59] proposed the LANTIME algorithm, a TS heuristic for the TDVRPTW
O	Donati et al. [23] developed an ACS algorithm for a similar problem with the aim of minimizing two hierarchical objectives: the number of tours and the total travel time.
O	Schilde et al. [78] dealt with the time-dependent dynamic dial-a-ride problem for which they adapted four existing metaheuristics.\
O	The proposed approach led to savings in CO2 emissions of about 7%. Jabali et al. [50] studied a TDVRP that considered travel time, fuel and CO2 emissions costs.
O	Finally, Franceschetti et al. [31] dealt with the time-dependent pollution-routing problem in which the authors took into account speed restrictions imposed by traffic congestion. 
O	Lecluyse et al. [57] considered a vehicle routing problem with both time-dependent and stochastic travel times.
O	Tagmouti et al. [83] studied an arc routing problem with capacity constraints and time-dependent service costs, motivated by winter gritting applications.
P	Tagmouti et al. [84] considered a new variant of the capacitated arc routing problem in which a time-dependent piecewise linear service cost is associated with each required arc.
O	Tagmouti et al. [85] tackled the dynamic variant of the capacitated arc routing problem using a Variable Neighborhood Descent heuristic.
O	Black et al. [12] studied a time-dependent prize-collecting arc routing problem that arises whenever one has to choose between a number of full truckload pick-ups and deliveries on a road network where travel times change over the day.