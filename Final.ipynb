{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\M Ahsan/nltk_data'\n    - 'C:\\\\Users\\\\M Ahsan\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\M Ahsan\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\M Ahsan\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\M Ahsan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\M Ahsan/nltk_data'\n    - 'C:\\\\Users\\\\M Ahsan\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\M Ahsan\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\M Ahsan\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\M Ahsan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d0b58365c42d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m# Cleaning data to removing stop words, string punctuations, making tokens, apply lemmatizations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[0mStop_Words_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'me'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'my'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'myself'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'we'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'our'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ours'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ourselves'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'you'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"you're\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"you've\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"you'll\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"you'd\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'your'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'yours'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'yourself'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'yourselves'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'he'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'him'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'his'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'himself'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'she'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"she's\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'her'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'hers'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'herself'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'it'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"it's\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'its'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'itself'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'they'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'them'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'their'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'theirs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'themselves'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'what'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'which'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'who'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'whom'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'this'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'that'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"that'll\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'these'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'those'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'am'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'is'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'are'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'was'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'were'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'be'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'been'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'being'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'have'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'has'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'had'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'having'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'do'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'does'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'did'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'doing'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'an'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'the'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'and'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'but'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'if'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'or'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'because'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'as'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'until'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'while'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'of'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'at'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'by'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'for'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'with'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'about'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'against'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'between'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'into'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'through'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'during'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'before'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'after'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'above'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'below'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'to'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'from'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'up'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'down'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'in'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'out'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'on'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'off'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'over'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'under'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'again'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'further'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'then'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'once'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'here'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'there'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'when'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'where'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'why'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'how'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'all'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'any'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'both'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'each'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'few'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'more'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'most'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'other'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'some'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'such'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'no'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nor'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'not'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'only'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'own'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'so'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'than'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'too'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'very'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m's'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m't'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'can'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'will'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'just'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'don'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'should'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"should've\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'now'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'll'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'o'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m're'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m've'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ain'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'aren'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'couldn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'didn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'doesn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'hadn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'hasn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'haven'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'isn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ma'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mightn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mustn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'needn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shan'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shouldn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wasn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'weren'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'won'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wouldn'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[0mwnl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\M Ahsan/nltk_data'\n    - 'C:\\\\Users\\\\M Ahsan\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\M Ahsan\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\M Ahsan\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\M Ahsan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#libraries for ignore warnings of Deprecation and Future\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "#Library for create Data Frame  \n",
    "import pandas as pd \n",
    "# library for evaluation of metrics\n",
    "from sklearn import metrics\n",
    "#Library for string punctuation\n",
    "import string\n",
    "#library for Tokenize words\n",
    "import re\n",
    "#Library for Natural Language Tool Kit functions\n",
    "import nltk\n",
    "#library for numerical computations\n",
    "import numpy as np\n",
    "#library for Colorfull Graphical representation report\n",
    "import seaborn as sns\n",
    "#library for graps\n",
    "import matplotlib.pyplot as plt\n",
    "#library for Classification Reports lik eprecession, recall, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "#library for accuracy of data\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "#module & function used to split data & training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "# module for converting label P N O to numerical values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#library for Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#library for import Random Forest Classifier from Sklearn module\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#library for import Naive Bayes Classifier from Sklearn module\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#library for import Logistic Regression Classifier from Sklearn module\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#library for import K_Neighbors Classifier from Sklearn module\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#library for import Support Vectore Machine Classifier from Sklearn module\n",
    "from sklearn import svm\n",
    "#library for import Decision Tree Classifier from Sklearn module\n",
    "from sklearn import tree\n",
    "#library for Neural Networks Classifier from Sklearn module\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#library for import Gradient Boosting Classifier from Sklearn module\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#library for checking working Operating Systems paths\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "Data_File = 'Data.tsv'\n",
    "\n",
    "# Pandas Data_Frame\n",
    "\n",
    "Data_Frame = pd.read_table( Data_File , names = ['Data_Class', 'Annotated_Data'])\n",
    "Data_Set = Data_Frame\n",
    "\n",
    "# Conversion P = 1, N = 0, O = 2\n",
    "\n",
    "Data_Set['Bin_Class_Con'] = Data_Set.Data_Class.map({'N':0, 'P':1 , 'O':2})\n",
    "\n",
    "# Cleaning data to removing stop words, string punctuations, making tokens, apply lemmatizations\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "Stop_Words_1 = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn',]\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "def clean_data(a):\n",
    "    text = \"\".join([m for m in a if m not in string.punctuation])\n",
    "    tokens = re.split ('\\W+', a)\n",
    "    text = \" \" .join ([wnl.lemmatize(words) for words in tokens if words not in Stop_Words_1])\n",
    "    return text\n",
    "\n",
    "Data_Set['ML_Anno_Data'] = Data_Set['Annotated_Data'].apply(lambda x: clean_data(x))\n",
    "\n",
    "# Assign objects to colomns\n",
    "\n",
    "X = Data_Set['ML_Anno_Data']\n",
    "y = Data_Set['Bin_Class_Con']\n",
    "y = y.replace(np.nan,0)\n",
    "\n",
    "# Counting of Positive, Negative & Neutrals\n",
    "\n",
    "print (y.value_counts())\n",
    "\n",
    "# Matplotlib Graph Representation\n",
    "\n",
    "sns.countplot(y)\n",
    "\n",
    "\n",
    "\n",
    "# Count Vectorizer\n",
    "\n",
    "Vect = CountVectorizer(ngram_range = (1,1))\n",
    "\n",
    "K_range = list(range(1,2))\n",
    "\n",
    "NB_Mac_Score = []\n",
    "NB_Mic_Score = []\n",
    "RF_Mac_Score = []\n",
    "RF_Mic_Score = []\n",
    "LG_Mac_Score = []\n",
    "LG_Mic_Score = []\n",
    "GB_Mac_Score = []\n",
    "GB_Mic_Score = []\n",
    "NN_Mac_Score = []\n",
    "NN_Mic_Score = []\n",
    "DT_Mac_Score = []\n",
    "DT_Mic_Score = []\n",
    "SVM_Mac_Score = []\n",
    "SVM_Mic_Score = []\n",
    "KN_Mac_Score = []\n",
    "KN_Mic_Score = []\n",
    "\n",
    "for k in K_range:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = k)\n",
    "    X_train_Data = Vect.fit_transform (X_train)\n",
    "    X_test_Data = Vect.transform (X_test)\n",
    "    \n",
    "    #Random Forest Classifier\n",
    "    Rand_for = RandomForestClassifier(n_estimators=40)\n",
    "    Rand_for.fit(X_train_Data,y_train)\n",
    "    Rand_pred = Rand_for.predict(X_test_Data)\n",
    "    RF_Mac_Score.append(metrics.accuracy_score(y_test, Rand_pred))\n",
    "#     RF_Mac_Score.append(metrics.recall_score(y_test, Rand_pred, average = 'macro'))\n",
    "#     RF_Mic_Score.append(metrics.recall_score(y_test, Rand_pred, average = 'micro'))\n",
    "#     RF_Mac_Score.append(metrics.precision_score(y_test, Rand_pred, average = 'macro'))\n",
    "#     RF_Mic_Score.append(metrics.precision_score(y_test, Rand_pred, average = 'micro'))\n",
    "#     RF_Mac_Score.append(metrics.f1_score(y_test, Rand_pred, average = 'macro'))\n",
    "#     RF_Mic_Score.append(metrics.f1_score(y_test, Rand_pred, average = 'micro'))\n",
    "    \n",
    "    \n",
    "    #Logistic Regression Classifier\n",
    "    Log_Reg = LogisticRegression(C=60)\n",
    "    Log_Reg.fit(X_train_Data,y_train)\n",
    "    Log_Reg_Pred = Log_Reg.predict(X_test_Data)\n",
    "    LG_Mac_Score.append(metrics.accuracy_score(y_test, Log_Reg_Pred))\n",
    "#     LG_Mac_Score.append(metrics.recall_score(y_test, Log_Reg_Pred, average = 'macro'))\n",
    "#     LG_Mic_Score.append(metrics.recall_score(y_test, Log_Reg_Pred, average = 'micro'))\n",
    "#     LG_Mac_Score.append(metrics.precision_score(y_test, Log_Reg_Pred, average = 'macro'))\n",
    "#     LG_Mic_Score.append(metrics.precision_score(y_test, Log_Reg_Pred, average = 'micro'))\n",
    "#     LG_Mac_Score.append(metrics.f1_score(y_test, Log_Reg_Pred, average = 'macro'))\n",
    "#     LG_Mic_Score.append(metrics.f1_score(y_test, Log_Reg_Pred, average = 'micro'))\n",
    "    \n",
    "    #Naive Bayes Classifier\n",
    "    Nav_Bay = MultinomialNB()\n",
    "    Nav_Bay.fit(X_train_Data,y_train)\n",
    "    Nav_Bay_Pred = Nav_Bay.predict(X_test_Data)\n",
    "    NB_Mac_Score.append(metrics.accuracy_score(y_test, Nav_Bay_Pred))\n",
    "#     NB_Mac_Score.append(metrics.recall_score(y_test, Nav_Bay_Pred, average = 'macro'))\n",
    "#     NB_Mic_Score.append(metrics.recall_score(y_test, Nav_Bay_Pred, average = 'micro'))\n",
    "#     NB_Mac_Score.append(metrics.precision_score(y_test, Nav_Bay_Pred, average = 'macro'))\n",
    "#     NB_Mic_Score.append(metrics.precision_score(y_test, Nav_Bay_Pred, average = 'micro'))\n",
    "#     NB_Mac_Score.append(metrics.f1_score(y_test, Nav_Bay_Pred, average = 'macro'))\n",
    "#     NB_Mic_Score.append(metrics.f1_score(y_test, Nav_Bay_Pred, average = 'micro'))\n",
    "    \n",
    "    # K_Neighbors Classifier\n",
    "    K_N = KNeighborsClassifier(n_neighbors = 10)\n",
    "    K_N.fit(X_train_Data,y_train)\n",
    "    K_N_Pred = K_N.predict(X_test_Data)\n",
    "    KN_Mac_Score.append(metrics.accuracy_score(y_test, K_N_Pred))\n",
    "#     KN_Mac_Score.append(metrics.recall_score(y_test, K_N_Pred, average = 'macro'))\n",
    "#     KN_Mic_Score.append(metrics.recall_score(y_test, K_N_Pred, average = 'micro'))\n",
    "#     KN_Mac_Score.append(metrics.precision_score(y_test, K_N_Pred, average = 'macro'))\n",
    "#     KN_Mic_Score.append(metrics.precision_score(y_test, K_N_Pred, average = 'micro'))\n",
    "#     KN_Mac_Score.append(metrics.f1_score(y_test, K_N_Pred, average = 'macro'))\n",
    "#     KN_Mic_Score.append(metrics.f1_score(y_test, K_N_Pred, average = 'micro'))\n",
    "    \n",
    "    #Support Vector Machine Classifier\n",
    "    S_VC = svm.SVC(C=225)\n",
    "    S_VC.fit(X_train_Data,y_train)\n",
    "    S_VC_Pred = S_VC.predict(X_test_Data)\n",
    "    SVM_Mac_Score.append(metrics.accuracy_score(y_test, S_VC_Pred))\n",
    "#     SVM_Mac_Score.append(metrics.recall_score(y_test, S_VC_Pred, average = 'macro'))\n",
    "#     SVM_Mic_Score.append(metrics.recall_score(y_test, S_VC_Pred, average = 'micro'))\n",
    "#     SVM_Mac_Score.append(metrics.precision_score(y_test, S_VC_Pred, average = 'macro'))\n",
    "#     SVM_Mic_Score.append(metrics.precision_score(y_test, S_VC_Pred, average = 'micro'))\n",
    "#     SVM_Mac_Score.append(metrics.f1_score(y_test, S_VC_Pred, average = 'macro'))\n",
    "#     SVM_Mic_Score.append(metrics.f1_score(y_test, S_VC_Pred, average = 'micro'))\n",
    "    \n",
    "    #Decision Tree Classifier\n",
    "    Dec_Tree = tree.DecisionTreeClassifier(max_depth=40)\n",
    "    Dec_Tree.fit(X_train_Data,y_train)\n",
    "    Dec_Tree_Pred = Dec_Tree.predict(X_test_Data)\n",
    "    DT_Mac_Score.append(metrics.accuracy_score(y_test, Dec_Tree_Pred))\n",
    "#     DT_Mac_Score.append(metrics.recall_score(y_test, Dec_Tree_Pred, average = 'macro'))\n",
    "#     DT_Mic_Score.append(metrics.recall_score(y_test, Dec_Tree_Pred, average = 'micro'))\n",
    "#     DT_Mac_Score.append(metrics.precision_score(y_test, Dec_Tree_Pred, average = 'macro'))\n",
    "#     DT_Mic_Score.append(metrics.precision_score(y_test, Dec_Tree_Pred, average = 'micro'))\n",
    "#     DT_Mac_Score.append(metrics.f1_score(y_test, Dec_Tree_Pred, average = 'macro'))\n",
    "#     DT_Mic_Score.append(metrics.f1_score(y_test, Dec_Tree_Pred, average = 'micro'))\n",
    "    \n",
    "    #Gradient Boosting Classifier\n",
    "    G_BC = GradientBoostingClassifier(n_estimators = 35)\n",
    "    G_BC.fit(X_train_Data,y_train)\n",
    "    G_BC_Pred = G_BC.predict(X_test_Data)\n",
    "    GB_Mac_Score.append(metrics.accuracy_score(y_test, G_BC_Pred))\n",
    "#     GB_Mac_Score.append(metrics.recall_score(y_test, G_BC_Pred, average = 'macro'))\n",
    "#     GB_Mic_Score.append(metrics.recall_score(y_test, G_BC_Pred, average = 'micro'))\n",
    "#     GB_Mac_Score.append(metrics.precision_score(y_test, G_BC_Pred, average = 'macro'))\n",
    "#     GB_Mic_Score.append(metrics.precision_score(y_test, G_BC_Pred, average = 'micro'))\n",
    "#     GB_Mac_Score.append(metrics.f1_score(y_test, G_BC_Pred, average = 'macro'))\n",
    "#     GB_Mic_Score.append(metrics.f1_score(y_test, G_BC_Pred, average = 'micro'))\n",
    "    \n",
    "    #Neural Networks Classifier\n",
    "    Neu_Net = MLPClassifier()\n",
    "    Neu_Net.fit(X_train_Data,y_train)\n",
    "    Neu_Net_Pred = Neu_Net.predict(X_test_Data)\n",
    "    NN_Mac_Score.append(metrics.accuracy_score(y_test, Neu_Net_Pred))\n",
    "#     NN_Mac_Score.append(metrics.recall_score(y_test, Neu_Net_Pred, average = 'macro'))\n",
    "#     NN_Mic_Score.append(metrics.recall_score(y_test, Neu_Net_Pred, average = 'micro'))\n",
    "#     NN_Mac_Score.append(metrics.precision_score(y_test, Neu_Net_Pred, average = 'macro'))\n",
    "#     NN_Mic_Score.append(metrics.precision_score(y_test, Neu_Net_Pred, average = 'micro'))\n",
    "#     NN_Mac_Score.append(metrics.f1_score(y_test, Neu_Net_Pred, average = 'macro'))\n",
    "#     NN_Mic_Score.append(metrics.f1_score(y_test, Neu_Net_Pred, average = 'micro'))\n",
    "    \n",
    "print('Naive Bayse Macro',np.array(NB_Mac_Score).mean())\n",
    "# print('Naive Bayse Micro',np.array(NB_Mic_Score).mean())\n",
    "print('Support Vector Machine Macro',np.array(SVM_Mac_Score).mean())\n",
    "# print('Support Vector Machine Micro',np.array(SVM_Mic_Score).mean())\n",
    "print('Logistic Regression Macro',np.array(LG_Mac_Score).mean())\n",
    "# print('Logistic Regression Micro',np.array(LG_Mic_Score).mean())\n",
    "print('Decision Tree Macro',np.array(DT_Mac_Score).mean())\n",
    "# print('Decision Tree Micro',np.array(DT_Mic_Score).mean())\n",
    "print('Nearest Neighbour Macro',np.array(KN_Mac_Score).mean())\n",
    "# print('Nearest Neighbour Micro',np.array(KN_Mic_Score).mean())\n",
    "print('Random Forest Macro',np.array(RF_Mac_Score).mean())\n",
    "# print('Random Forest Micro',np.array(RF_Mic_Score).mean())\n",
    "print('Gradient Boosting Macro',np.array(GB_Mac_Score).mean())\n",
    "# print('Gradient Boosting Micro',np.array(GB_Mic_Score).mean())\n",
    "print('Neural Network Macro',np.array(NN_Mac_Score).mean())\n",
    "# print('Neural Network Micro',np.array(NN_Mic_Score).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
